{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj_jHFRS2xOe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor, tensor, LongTensor\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import html\n",
    "from collections import Counter\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80211,
     "status": "ok",
     "timestamp": 1568441310143,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "Z5e_LEjIFb8P",
    "outputId": "5320a62b-5f8e-48ea-baea-002690e9e572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbzGsfrk5Q5m"
   },
   "source": [
    "Inspired by FastAI implementation of AWD LSTM (https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IF2j3Lfi4ucy"
   },
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4r2MdBy4zgP"
   },
   "outputs": [],
   "source": [
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6V2e8f456AVJ"
   },
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSgZ_0wc8ARi"
   },
   "outputs": [],
   "source": [
    "class AWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs = 1\n",
    "        self.emb_sz = emb_sz\n",
    "        self.n_hid = n_hid\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YW4mrofe-DTm"
   },
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXpQdnJE_BgE"
   },
   "outputs": [],
   "source": [
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))\n",
    "\n",
    "\n",
    "def cross_entropy_flat(input, target):\n",
    "    bs,sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n",
    "\n",
    "def accuracy(out, yb): \n",
    "    return (torch.argmax(out, dim=1)==yb).float().mean()\n",
    "\n",
    "def accuracy_flat(input, target):\n",
    "    bs,sl = target.size()\n",
    "    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3JTcQVHAsgB"
   },
   "outputs": [],
   "source": [
    "emb_sz = 300 \n",
    "nh = 300\n",
    "nl =  2\n",
    "vocab = {}\n",
    "vocab['xxxpad'] = 0\n",
    "vocab['test'] = 1\n",
    "tok_pad = 0\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, \n",
    "                           embed_p=0.1, hidden_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5gmTdcXBdWE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJompYNfDk1q"
   },
   "source": [
    "Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-T4xcwgDh33"
   },
   "outputs": [],
   "source": [
    "#TODO: get the pretrained wikitext weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSY2xo63DqVP"
   },
   "source": [
    "Preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBuI1RBWD_fp"
   },
   "outputs": [],
   "source": [
    "base_path = Path('drive/My Drive/datasets')\n",
    "df = pd.read_csv(base_path/'rspct.tsv', sep = '\\t')\n",
    "remove_n=800000\n",
    "drop_indices = np.random.choice(df.index, remove_n, replace=False)\n",
    "df = df.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1568449078322,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "Y1ACMkW1UjvS",
    "outputId": "ecc7cde7-2ab6-4354-f5b2-38a91d7e7e7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6icvfu</td>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Inline Hockey: Where Do I Need To Be? (Positio...</td>\n",
       "      <td>My game is coming on well but one HUGE aspect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6azhj1</td>\n",
       "      <td>rawdenim</td>\n",
       "      <td>Had a custom embroidery job done on my ranch j...</td>\n",
       "      <td>[Album First](http://imgur.com/a/DYdKC)&lt;lb&gt;&lt;lb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7i2bt4</td>\n",
       "      <td>NameThatSong</td>\n",
       "      <td>Can anyone help me find this band?</td>\n",
       "      <td>Thinking of 'who the band sounded like' brings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6zltab</td>\n",
       "      <td>homeless</td>\n",
       "      <td>About to be homeless in LA,CA</td>\n",
       "      <td>My parents have cost me my recent job and are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4zpvar</td>\n",
       "      <td>antidepressants</td>\n",
       "      <td>Sexual performance anxiety</td>\n",
       "      <td>Throwaway here. I've been on sertraline 100mg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8kuivg</td>\n",
       "      <td>sissyhypno</td>\n",
       "      <td>Regrets</td>\n",
       "      <td>So i finally got my first full body waxing don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7cafsz</td>\n",
       "      <td>foreskin_restoration</td>\n",
       "      <td>Huge layer of skin peeled off from glans</td>\n",
       "      <td>WOW! came back home from a hunting trip and ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6ht1be</td>\n",
       "      <td>driving</td>\n",
       "      <td>[UK] Name change before getting full license</td>\n",
       "      <td>My provisional license has a minor spelling er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>55a1gw</td>\n",
       "      <td>Magic</td>\n",
       "      <td>John Bannon Product Advice Please</td>\n",
       "      <td>Hello,&lt;lb&gt;I have beginner level skills. I own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7vfotc</td>\n",
       "      <td>sewing</td>\n",
       "      <td>How do I get in contact with skilled seamstres...</td>\n",
       "      <td>I have a business that makes custom products o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>72i155</td>\n",
       "      <td>JUSTNOMIL</td>\n",
       "      <td>Quick advice needed</td>\n",
       "      <td>So I just wrote out an incredibly long post wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4wwmeq</td>\n",
       "      <td>sleeptrain</td>\n",
       "      <td>Nap training after sleep training</td>\n",
       "      <td>Is it normal to feel like starting NAP trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5k6s8t</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>Weekly Ads Not Sending?</td>\n",
       "      <td>I signed up for weekly ads on GameStop's websi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5jolef</td>\n",
       "      <td>Firefighting</td>\n",
       "      <td>Help writing story about volunteer firefighters</td>\n",
       "      <td>So, I'm a volunteer firefighter myself, and I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>77fo25</td>\n",
       "      <td>riverdale</td>\n",
       "      <td>Did the serpents overhear Hiram ?</td>\n",
       "      <td>I was watching the new episode and saw that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>7eejro</td>\n",
       "      <td>bartenders</td>\n",
       "      <td>First time bartender, certification question.</td>\n",
       "      <td>Hey guys, so I just landed my first bartending...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5vgjb6</td>\n",
       "      <td>musictheory</td>\n",
       "      <td>App or website for specific metronome settings?</td>\n",
       "      <td>I'm looking for a metronome that can go up to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>80f7ox</td>\n",
       "      <td>SexToys</td>\n",
       "      <td>Glass Dildo</td>\n",
       "      <td>So my boyfriend and I own a couple vibrators a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>6cq0d3</td>\n",
       "      <td>Mattress</td>\n",
       "      <td>Just bought a Purple mattress, is there any ne...</td>\n",
       "      <td>I just replaced my existing soft top spring ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8c3n0f</td>\n",
       "      <td>uberdrivers</td>\n",
       "      <td>How am I supposed to improve my rating if I do...</td>\n",
       "      <td>Title says it. I’d like to improve my rating (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6brage</td>\n",
       "      <td>communism101</td>\n",
       "      <td>Term for gradual specialization of tools?</td>\n",
       "      <td>I can't remember if there was a term for this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5dgi9o</td>\n",
       "      <td>windowsphone</td>\n",
       "      <td>is Terraria working on Lumia 640?</td>\n",
       "      <td>i have an iPhone 4s and i wannna switch to Lum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6j08ep</td>\n",
       "      <td>needforspeed</td>\n",
       "      <td>Changing how the cars sounds when you change t...</td>\n",
       "      <td>So I remember seeing BlackPanthaas videos befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>7qv79w</td>\n",
       "      <td>blackmirror</td>\n",
       "      <td>Black Mirror Shut Up And Dance (S3 E3) made me...</td>\n",
       "      <td>What would you do if someone sends you a video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>54yzed</td>\n",
       "      <td>Cubers</td>\n",
       "      <td>Prizes for Comps, starting with the current on...</td>\n",
       "      <td>I am rather excited to announce that, starting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>7edxhd</td>\n",
       "      <td>Warframe</td>\n",
       "      <td>so question about how useful zaw's are.</td>\n",
       "      <td>Are zaws anything to even touch? i get some ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5ypvql</td>\n",
       "      <td>Professors</td>\n",
       "      <td>I'm a librarian and \"instructor\" at a communit...</td>\n",
       "      <td>I never know what title I should use at work. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7iscbn</td>\n",
       "      <td>parrots</td>\n",
       "      <td>Conure just dropped a strange looking feather.</td>\n",
       "      <td>Less than an hour ago, my 21 year old nanday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012860</th>\n",
       "      <td>7hbuw8</td>\n",
       "      <td>TREZOR</td>\n",
       "      <td>Trezor keychain attachment</td>\n",
       "      <td>A few weeks ago I found my trezor on the groun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012861</th>\n",
       "      <td>86wig0</td>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>Usage of どれ</td>\n",
       "      <td>I’m learning Japanese using Genki I, with Ling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012863</th>\n",
       "      <td>7nikka</td>\n",
       "      <td>namenerds</td>\n",
       "      <td>Help me think of middle names</td>\n",
       "      <td>I've been thinking about the name Cassandra if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012866</th>\n",
       "      <td>81gpwa</td>\n",
       "      <td>LineageOS</td>\n",
       "      <td>Problem with Lineage OS + AF Wall+</td>\n",
       "      <td>Dear LineageOS-Community!&lt;lb&gt;&lt;lb&gt;I am very new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012876</th>\n",
       "      <td>57oxod</td>\n",
       "      <td>telescopes</td>\n",
       "      <td>Best scope for 500-700$? Have three in mind, p...</td>\n",
       "      <td>Hi all,&lt;lb&gt;&lt;lb&gt;Much appreciated if anyone can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012878</th>\n",
       "      <td>85ty75</td>\n",
       "      <td>snapchat</td>\n",
       "      <td>19 [M4A] Bored at work</td>\n",
       "      <td>Hello there, typical title, I'm just trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012879</th>\n",
       "      <td>7bw05q</td>\n",
       "      <td>l5r</td>\n",
       "      <td>[discussion]What is your local meta like? What...</td>\n",
       "      <td>When I started playing with locals I noticed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012880</th>\n",
       "      <td>7mvrh6</td>\n",
       "      <td>bulletjournal</td>\n",
       "      <td>Mood Trackers for People with Anxiety?</td>\n",
       "      <td>So, I’ve been on and off on bujo but this year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012884</th>\n",
       "      <td>7s6irs</td>\n",
       "      <td>BDSMcommunity</td>\n",
       "      <td>Severe Depression--Separation from Dom. Need h...</td>\n",
       "      <td>My depression seems to be getting worse over t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012900</th>\n",
       "      <td>71d811</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Weird Meditation Development</td>\n",
       "      <td>Hallo! &lt;lb&gt;&lt;lb&gt;This is my first time posting i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012903</th>\n",
       "      <td>6upjxk</td>\n",
       "      <td>Magic</td>\n",
       "      <td>Noob question, Recommendations for beginner kits.</td>\n",
       "      <td>I am a long time fan of magic, but i am fairly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012905</th>\n",
       "      <td>7x26h5</td>\n",
       "      <td>space</td>\n",
       "      <td>Government Space Agencies =/= Private Space Ag...</td>\n",
       "      <td>I don't get why most scientist support governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012907</th>\n",
       "      <td>7jakt3</td>\n",
       "      <td>mopeio</td>\n",
       "      <td>A challenge...</td>\n",
       "      <td>The challenge is that you have to upgrade only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012910</th>\n",
       "      <td>5myzoq</td>\n",
       "      <td>summonerswar</td>\n",
       "      <td>Sub-stats?</td>\n",
       "      <td>So I've been confused over this for a while an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012917</th>\n",
       "      <td>68kmrd</td>\n",
       "      <td>eroticauthors</td>\n",
       "      <td>3rd story published zero sales not showing ran...</td>\n",
       "      <td>Ok maybe this is a silly question but I publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012919</th>\n",
       "      <td>7103y7</td>\n",
       "      <td>southpark</td>\n",
       "      <td>Is Mr. Garrison just going to be out of the sh...</td>\n",
       "      <td>As far as I know, South Park is going to be st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012923</th>\n",
       "      <td>74rl2f</td>\n",
       "      <td>matlab</td>\n",
       "      <td>Optimizing patch function for multi-faced poly...</td>\n",
       "      <td>Hi, I'm a newbie MATLAB programmer (as in star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012927</th>\n",
       "      <td>73umh7</td>\n",
       "      <td>tableau</td>\n",
       "      <td>Feedback needed for tableau story</td>\n",
       "      <td>Hello, I request feedback for my tableau story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012929</th>\n",
       "      <td>7pqcyq</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>Recently diagnosed, feeling sort of lost.</td>\n",
       "      <td>Okay so, I was diagnosed yesterday and I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012935</th>\n",
       "      <td>6atf1d</td>\n",
       "      <td>bettafish</td>\n",
       "      <td>Help with divided tank...</td>\n",
       "      <td>I have 4 male betta fish all a little more agg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012938</th>\n",
       "      <td>7cm7ia</td>\n",
       "      <td>architecture</td>\n",
       "      <td>GRAND DESIGN: I Want to design a modern Roman/...</td>\n",
       "      <td>I have always loved the old style and this is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012943</th>\n",
       "      <td>8fiiqg</td>\n",
       "      <td>Gloomhaven</td>\n",
       "      <td>[Mild spoilers] executioner battle goal</td>\n",
       "      <td>Hello! I was thinking about the battlegoal and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012945</th>\n",
       "      <td>58vk8e</td>\n",
       "      <td>asexuality</td>\n",
       "      <td>LPT: Hang on in there &amp;amp; be patient. Good t...</td>\n",
       "      <td>I'm a 22 year-old girl &amp;amp; a university stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012946</th>\n",
       "      <td>6z7i2o</td>\n",
       "      <td>Cloud9</td>\n",
       "      <td>Crossing the Gauntlet: 2017 edition</td>\n",
       "      <td>Inspired by the works of /u/sayntclair and /u/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012957</th>\n",
       "      <td>5513um</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>I keep seeing a little girl. Description below.</td>\n",
       "      <td>She has dark black hair? Something like the gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012973</th>\n",
       "      <td>4rj8lk</td>\n",
       "      <td>hiking</td>\n",
       "      <td>Recommend good waterfall/swim holes near the m...</td>\n",
       "      <td>My wife and I have enjoyed great hikes to wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012984</th>\n",
       "      <td>6hv12d</td>\n",
       "      <td>drums</td>\n",
       "      <td>Trying to remove drum wrap - running into issues</td>\n",
       "      <td>So I'm currently using a heat gun to remove th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012988</th>\n",
       "      <td>7dbctm</td>\n",
       "      <td>leopardgeckos</td>\n",
       "      <td>I think my gecko ripped out his nail..</td>\n",
       "      <td>I came back from work today to what looked lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012994</th>\n",
       "      <td>6wnx2j</td>\n",
       "      <td>wildhearthstone</td>\n",
       "      <td>Viability of Combo Renolock vs N'Zoth / Demon ...</td>\n",
       "      <td>With Priest getting unlimited shot guns agains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012999</th>\n",
       "      <td>6ump0y</td>\n",
       "      <td>wine</td>\n",
       "      <td>What is the worse wine you ever had?</td>\n",
       "      <td>My worst wine was at a dinner party. My friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ...                                           selftext\n",
       "3        6ti6re  ...  I know this is a sub for the 'Ring Doorbell' b...\n",
       "4        77sxto  ...  Prime95 (regardless of version) and OCCT both,...\n",
       "7        6icvfu  ...  My game is coming on well but one HUGE aspect ...\n",
       "9        6azhj1  ...  [Album First](http://imgur.com/a/DYdKC)<lb><lb...\n",
       "14       7i2bt4  ...  Thinking of 'who the band sounded like' brings...\n",
       "15       6zltab  ...  My parents have cost me my recent job and are ...\n",
       "16       4zpvar  ...  Throwaway here. I've been on sertraline 100mg ...\n",
       "19       8kuivg  ...  So i finally got my first full body waxing don...\n",
       "30       7cafsz  ...  WOW! came back home from a hunting trip and ha...\n",
       "36       6ht1be  ...  My provisional license has a minor spelling er...\n",
       "39       55a1gw  ...  Hello,<lb>I have beginner level skills. I own ...\n",
       "44       7vfotc  ...  I have a business that makes custom products o...\n",
       "47       72i155  ...  So I just wrote out an incredibly long post wi...\n",
       "51       4wwmeq  ...  Is it normal to feel like starting NAP trainin...\n",
       "53       5k6s8t  ...  I signed up for weekly ads on GameStop's websi...\n",
       "55       5jolef  ...  So, I'm a volunteer firefighter myself, and I ...\n",
       "57       77fo25  ...  I was watching the new episode and saw that th...\n",
       "59       7eejro  ...  Hey guys, so I just landed my first bartending...\n",
       "63       5vgjb6  ...  I'm looking for a metronome that can go up to ...\n",
       "65       80f7ox  ...  So my boyfriend and I own a couple vibrators a...\n",
       "69       6cq0d3  ...  I just replaced my existing soft top spring ma...\n",
       "71       8c3n0f  ...  Title says it. I’d like to improve my rating (...\n",
       "74       6brage  ...  I can't remember if there was a term for this ...\n",
       "75       5dgi9o  ...  i have an iPhone 4s and i wannna switch to Lum...\n",
       "83       6j08ep  ...  So I remember seeing BlackPanthaas videos befo...\n",
       "85       7qv79w  ...  What would you do if someone sends you a video...\n",
       "91       54yzed  ...  I am rather excited to announce that, starting...\n",
       "93       7edxhd  ...  Are zaws anything to even touch? i get some ar...\n",
       "94       5ypvql  ...  I never know what title I should use at work. ...\n",
       "95       7iscbn  ...   Less than an hour ago, my 21 year old nanday ...\n",
       "...         ...  ...                                                ...\n",
       "1012860  7hbuw8  ...  A few weeks ago I found my trezor on the groun...\n",
       "1012861  86wig0  ...  I’m learning Japanese using Genki I, with Ling...\n",
       "1012863  7nikka  ...  I've been thinking about the name Cassandra if...\n",
       "1012866  81gpwa  ...  Dear LineageOS-Community!<lb><lb>I am very new...\n",
       "1012876  57oxod  ...  Hi all,<lb><lb>Much appreciated if anyone can ...\n",
       "1012878  85ty75  ...  Hello there, typical title, I'm just trying to...\n",
       "1012879  7bw05q  ...  When I started playing with locals I noticed t...\n",
       "1012880  7mvrh6  ...  So, I’ve been on and off on bujo but this year...\n",
       "1012884  7s6irs  ...  My depression seems to be getting worse over t...\n",
       "1012900  71d811  ...  Hallo! <lb><lb>This is my first time posting i...\n",
       "1012903  6upjxk  ...  I am a long time fan of magic, but i am fairly...\n",
       "1012905  7x26h5  ...  I don't get why most scientist support governm...\n",
       "1012907  7jakt3  ...  The challenge is that you have to upgrade only...\n",
       "1012910  5myzoq  ...  So I've been confused over this for a while an...\n",
       "1012917  68kmrd  ...  Ok maybe this is a silly question but I publis...\n",
       "1012919  7103y7  ...  As far as I know, South Park is going to be st...\n",
       "1012923  74rl2f  ...  Hi, I'm a newbie MATLAB programmer (as in star...\n",
       "1012927  73umh7  ...  Hello, I request feedback for my tableau story...\n",
       "1012929  7pqcyq  ...  Okay so, I was diagnosed yesterday and I have ...\n",
       "1012935  6atf1d  ...  I have 4 male betta fish all a little more agg...\n",
       "1012938  7cm7ia  ...  I have always loved the old style and this is ...\n",
       "1012943  8fiiqg  ...  Hello! I was thinking about the battlegoal and...\n",
       "1012945  58vk8e  ...  I'm a 22 year-old girl &amp; a university stud...\n",
       "1012946  6z7i2o  ...  Inspired by the works of /u/sayntclair and /u/...\n",
       "1012957  5513um  ...  She has dark black hair? Something like the gr...\n",
       "1012973  4rj8lk  ...  My wife and I have enjoyed great hikes to wate...\n",
       "1012984  6hv12d  ...  So I'm currently using a heat gun to remove th...\n",
       "1012988  7dbctm  ...  I came back from work today to what looked lik...\n",
       "1012994  6wnx2j  ...  With Priest getting unlimited shot guns agains...\n",
       "1012999  6ump0y  ...  My worst wine was at a dinner party. My friend...\n",
       "\n",
       "[213000 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2liHIuPgHaGj"
   },
   "outputs": [],
   "source": [
    "#simple preprocess (lower case, remove weird symbols)\n",
    "UNK = 'xxxunk'\n",
    "BOS = 'xxxbos'\n",
    "EOS = 'xxxeos'\n",
    "def get_corpus(df):\n",
    "    corpus = ''\n",
    "    for i in range(len(df.index)):\n",
    "        corpus += BOS +' '+ df.iloc[i, -1] + ' ' + df.iloc[i, -2] + ' ' + EOS\n",
    "    return corpus\n",
    "\n",
    "def to_lower(t):\n",
    "    return t.lower()\n",
    "        \n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\" \", t)\n",
    "\n",
    "\n",
    "def sub_lb(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*lb\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\" \", t)\n",
    "\n",
    "def sub_nl(t):\n",
    "    t.replace('\\n',' ')\n",
    "    return t\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \" \").replace('\\\\\"', '\"').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "#tokenize\n",
    "def tokenize(corpus, vocab = None):\n",
    "    tokenizer = spacy.blank(\"en\").tokenizer\n",
    "    doc = tokenizer(corpus)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if(token.text.strip() != \"\"):\n",
    "            if(vocab != None and token.text not in vocab):\n",
    "                tokens.append('xxxunk')\n",
    "            else:\n",
    "                tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "#generate vocab(REMEMBER TOKENS(eos, bos, pad, unk))\n",
    "TITLE_START = 'xxxts'\n",
    "TITLE_END = 'xxxte'\n",
    "\n",
    "def generate_vocab(df, min_freq = 2):\n",
    "    all_unique_words_counter = Counter()\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if(i % 100 == 0):\n",
    "            print(\"done \" + str(i) + \"/\" + str(len(df)))\n",
    "        all_unique_words_counter += Counter(row[-1]) + Counter(row[-2])\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "        \n",
    "    vocab = {}\n",
    "    vocab['xxxpad'] = 0\n",
    "    vocab['xxxunk'] = 1\n",
    "    index = 2\n",
    "    print(\"counter len: \" + str(len(all_unique_words_counter)))\n",
    "    i = 0\n",
    "    for w in all_unique_words_counter.keys():\n",
    "        if(i % 100 == 0):\n",
    "            print('done 2nd loop ' + str(i) + '/' + str(len(all_unique_words_counter)))\n",
    "        if(all_unique_words_counter[w] >= min_freq and w.strip() != \"\"):\n",
    "            vocab[w] = index\n",
    "            index += 1\n",
    "        i+=1\n",
    "    return vocab\n",
    "\n",
    "#create language model dataloaders and stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRIID9vMVPEQ"
   },
   "outputs": [],
   "source": [
    "corpus = get_corpus(df)\n",
    "with open(base_path/'corpus.txt', 'w+') as f:\n",
    "    f.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cTg7Bk2V8Vt"
   },
   "outputs": [],
   "source": [
    "corpus = None\n",
    "with open(base_path/'corpus.txt', 'r') as f:\n",
    "    corpus = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1568447922724,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "9OGGJM_qrqfC",
    "outputId": "df6d2b41-2a4e-46c6-bb6e-5ef344626cc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxxbos I know this is a sub for the 'Ring Doorbell' but has anyone used the Floodlight?  I already have the wire and existing bracket for the floodlight on the back of my house, but the problem is tha\""
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1568449214791,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "dlxSVKxE261C",
    "outputId": "734dfd7f-d6d5-453d-ffc9-bd436445818d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6icvfu</td>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Inline Hockey: Where Do I Need To Be? (Positio...</td>\n",
       "      <td>My game is coming on well but one HUGE aspect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6azhj1</td>\n",
       "      <td>rawdenim</td>\n",
       "      <td>Had a custom embroidery job done on my ranch j...</td>\n",
       "      <td>[Album First](http://imgur.com/a/DYdKC)&lt;lb&gt;&lt;lb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7i2bt4</td>\n",
       "      <td>NameThatSong</td>\n",
       "      <td>Can anyone help me find this band?</td>\n",
       "      <td>Thinking of 'who the band sounded like' brings...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ...                                           selftext\n",
       "3   6ti6re  ...  I know this is a sub for the 'Ring Doorbell' b...\n",
       "4   77sxto  ...  Prime95 (regardless of version) and OCCT both,...\n",
       "7   6icvfu  ...  My game is coming on well but one HUGE aspect ...\n",
       "9   6azhj1  ...  [Album First](http://imgur.com/a/DYdKC)<lb><lb...\n",
       "14  7i2bt4  ...  Thinking of 'who the band sounded like' brings...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xh7_85mFXtDx"
   },
   "outputs": [],
   "source": [
    "preprocess_funcs = [sub_br, sub_lb, spec_add_spaces, rm_useless_spaces, fixup_text, to_lower, sub_nl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8317865,
     "status": "error",
     "timestamp": 1568462974427,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "NEhaYqB1X9EM",
    "outputId": "cafafd75-164c-442a-c089-20ed65a24d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0 out of 213000\n",
      "done 100 out of 213000\n",
      "done 200 out of 213000\n",
      "done 300 out of 213000\n",
      "done 400 out of 213000\n",
      "done 500 out of 213000\n",
      "done 600 out of 213000\n",
      "done 700 out of 213000\n",
      "done 800 out of 213000\n",
      "done 900 out of 213000\n",
      "done 1000 out of 213000\n",
      "done 1100 out of 213000\n",
      "done 1200 out of 213000\n",
      "done 1300 out of 213000\n",
      "done 1400 out of 213000\n",
      "done 1500 out of 213000\n",
      "done 1600 out of 213000\n",
      "done 1700 out of 213000\n",
      "done 1800 out of 213000\n",
      "done 1900 out of 213000\n",
      "done 2000 out of 213000\n",
      "done 2100 out of 213000\n",
      "done 2200 out of 213000\n",
      "done 2300 out of 213000\n",
      "done 2400 out of 213000\n",
      "done 2500 out of 213000\n",
      "done 2600 out of 213000\n",
      "done 2700 out of 213000\n",
      "done 2800 out of 213000\n",
      "done 2900 out of 213000\n",
      "done 3000 out of 213000\n",
      "done 3100 out of 213000\n",
      "done 3200 out of 213000\n",
      "done 3300 out of 213000\n",
      "done 3400 out of 213000\n",
      "done 3500 out of 213000\n",
      "done 3600 out of 213000\n",
      "done 3700 out of 213000\n",
      "done 3800 out of 213000\n",
      "done 3900 out of 213000\n",
      "done 4000 out of 213000\n",
      "done 4100 out of 213000\n",
      "done 4200 out of 213000\n",
      "done 4300 out of 213000\n",
      "done 4400 out of 213000\n",
      "done 4500 out of 213000\n",
      "done 4600 out of 213000\n",
      "done 4700 out of 213000\n",
      "done 4800 out of 213000\n",
      "done 4900 out of 213000\n",
      "done 5000 out of 213000\n",
      "done 5100 out of 213000\n",
      "done 5200 out of 213000\n",
      "done 5300 out of 213000\n",
      "done 5400 out of 213000\n",
      "done 5500 out of 213000\n",
      "done 5600 out of 213000\n",
      "done 5700 out of 213000\n",
      "done 5800 out of 213000\n",
      "done 5900 out of 213000\n",
      "done 6000 out of 213000\n",
      "done 6100 out of 213000\n",
      "done 6200 out of 213000\n",
      "done 6300 out of 213000\n",
      "done 6400 out of 213000\n",
      "done 6500 out of 213000\n",
      "done 6600 out of 213000\n",
      "done 6700 out of 213000\n",
      "done 6800 out of 213000\n",
      "done 6900 out of 213000\n",
      "done 7000 out of 213000\n",
      "done 7100 out of 213000\n",
      "done 7200 out of 213000\n",
      "done 7300 out of 213000\n",
      "done 7400 out of 213000\n",
      "done 7500 out of 213000\n",
      "done 7600 out of 213000\n",
      "done 7700 out of 213000\n",
      "done 7800 out of 213000\n",
      "done 7900 out of 213000\n",
      "done 8000 out of 213000\n",
      "done 8100 out of 213000\n",
      "done 8200 out of 213000\n",
      "done 8300 out of 213000\n",
      "done 8400 out of 213000\n",
      "done 8500 out of 213000\n",
      "done 8600 out of 213000\n",
      "done 8700 out of 213000\n",
      "done 8800 out of 213000\n",
      "done 8900 out of 213000\n",
      "done 9000 out of 213000\n",
      "done 9100 out of 213000\n",
      "done 9200 out of 213000\n",
      "done 9300 out of 213000\n",
      "done 9400 out of 213000\n",
      "done 9500 out of 213000\n",
      "done 9600 out of 213000\n",
      "done 9700 out of 213000\n",
      "done 9800 out of 213000\n",
      "done 9900 out of 213000\n",
      "done 10000 out of 213000\n",
      "done 10100 out of 213000\n",
      "done 10200 out of 213000\n",
      "done 10300 out of 213000\n",
      "done 10400 out of 213000\n",
      "done 10500 out of 213000\n",
      "done 10600 out of 213000\n",
      "done 10700 out of 213000\n",
      "done 10800 out of 213000\n",
      "done 10900 out of 213000\n",
      "done 11000 out of 213000\n",
      "done 11100 out of 213000\n",
      "done 11200 out of 213000\n",
      "done 11300 out of 213000\n",
      "done 11400 out of 213000\n",
      "done 11500 out of 213000\n",
      "done 11600 out of 213000\n",
      "done 11700 out of 213000\n",
      "done 11800 out of 213000\n",
      "done 11900 out of 213000\n",
      "done 12000 out of 213000\n",
      "done 12100 out of 213000\n",
      "done 12200 out of 213000\n",
      "done 12300 out of 213000\n",
      "done 12400 out of 213000\n",
      "done 12500 out of 213000\n",
      "done 12600 out of 213000\n",
      "done 12700 out of 213000\n",
      "done 12800 out of 213000\n",
      "done 12900 out of 213000\n",
      "done 13000 out of 213000\n",
      "done 13100 out of 213000\n",
      "done 13200 out of 213000\n",
      "done 13300 out of 213000\n",
      "done 13400 out of 213000\n",
      "done 13500 out of 213000\n",
      "done 13600 out of 213000\n",
      "done 13700 out of 213000\n",
      "done 13800 out of 213000\n",
      "done 13900 out of 213000\n",
      "done 14000 out of 213000\n",
      "done 14100 out of 213000\n",
      "done 14200 out of 213000\n",
      "done 14300 out of 213000\n",
      "done 14400 out of 213000\n",
      "done 14500 out of 213000\n",
      "done 14600 out of 213000\n",
      "done 14700 out of 213000\n",
      "done 14800 out of 213000\n",
      "done 14900 out of 213000\n",
      "done 15000 out of 213000\n",
      "done 15100 out of 213000\n",
      "done 15200 out of 213000\n",
      "done 15300 out of 213000\n",
      "done 15400 out of 213000\n",
      "done 15500 out of 213000\n",
      "done 15600 out of 213000\n",
      "done 15700 out of 213000\n",
      "done 15800 out of 213000\n",
      "done 15900 out of 213000\n",
      "done 16000 out of 213000\n",
      "done 16100 out of 213000\n",
      "done 16200 out of 213000\n",
      "done 16300 out of 213000\n",
      "done 16400 out of 213000\n",
      "done 16500 out of 213000\n",
      "done 16600 out of 213000\n",
      "done 16700 out of 213000\n",
      "done 16800 out of 213000\n",
      "done 16900 out of 213000\n",
      "done 17000 out of 213000\n",
      "done 17100 out of 213000\n",
      "done 17200 out of 213000\n",
      "done 17300 out of 213000\n",
      "done 17400 out of 213000\n",
      "done 17500 out of 213000\n",
      "done 17600 out of 213000\n",
      "done 17700 out of 213000\n",
      "done 17800 out of 213000\n",
      "done 17900 out of 213000\n",
      "done 18000 out of 213000\n",
      "done 18100 out of 213000\n",
      "done 18200 out of 213000\n",
      "done 18300 out of 213000\n",
      "done 18400 out of 213000\n",
      "done 18500 out of 213000\n",
      "done 18600 out of 213000\n",
      "done 18700 out of 213000\n",
      "done 18800 out of 213000\n",
      "done 18900 out of 213000\n",
      "done 19000 out of 213000\n",
      "done 19100 out of 213000\n",
      "done 19200 out of 213000\n",
      "done 19300 out of 213000\n",
      "done 19400 out of 213000\n",
      "done 19500 out of 213000\n",
      "done 19600 out of 213000\n",
      "done 19700 out of 213000\n",
      "done 19800 out of 213000\n",
      "done 19900 out of 213000\n",
      "done 20000 out of 213000\n",
      "done 20100 out of 213000\n",
      "done 20200 out of 213000\n",
      "done 20300 out of 213000\n",
      "done 20400 out of 213000\n",
      "done 20500 out of 213000\n",
      "done 20600 out of 213000\n",
      "done 20700 out of 213000\n",
      "done 20800 out of 213000\n",
      "done 20900 out of 213000\n",
      "done 21000 out of 213000\n",
      "done 21100 out of 213000\n",
      "done 21200 out of 213000\n",
      "done 21300 out of 213000\n",
      "done 21400 out of 213000\n",
      "done 21500 out of 213000\n",
      "done 21600 out of 213000\n",
      "done 21700 out of 213000\n",
      "done 21800 out of 213000\n",
      "done 21900 out of 213000\n",
      "done 22000 out of 213000\n",
      "done 22100 out of 213000\n",
      "done 22200 out of 213000\n",
      "done 22300 out of 213000\n",
      "done 22400 out of 213000\n",
      "done 22500 out of 213000\n",
      "done 22600 out of 213000\n",
      "done 22700 out of 213000\n",
      "done 22800 out of 213000\n",
      "done 22900 out of 213000\n",
      "done 23000 out of 213000\n",
      "done 23100 out of 213000\n",
      "done 23200 out of 213000\n",
      "done 23300 out of 213000\n",
      "done 23400 out of 213000\n",
      "done 23500 out of 213000\n",
      "done 23600 out of 213000\n",
      "done 23700 out of 213000\n",
      "done 23800 out of 213000\n",
      "done 23900 out of 213000\n",
      "done 24000 out of 213000\n",
      "done 24100 out of 213000\n",
      "done 24200 out of 213000\n",
      "done 24300 out of 213000\n",
      "done 24400 out of 213000\n",
      "done 24500 out of 213000\n",
      "done 24600 out of 213000\n",
      "done 24700 out of 213000\n",
      "done 24800 out of 213000\n",
      "done 24900 out of 213000\n",
      "done 25000 out of 213000\n",
      "done 25100 out of 213000\n",
      "done 25200 out of 213000\n",
      "done 25300 out of 213000\n",
      "done 25400 out of 213000\n",
      "done 25500 out of 213000\n",
      "done 25600 out of 213000\n",
      "done 25700 out of 213000\n",
      "done 25800 out of 213000\n",
      "done 25900 out of 213000\n",
      "done 26000 out of 213000\n",
      "done 26100 out of 213000\n",
      "done 26200 out of 213000\n",
      "done 26300 out of 213000\n",
      "done 26400 out of 213000\n",
      "done 26500 out of 213000\n",
      "done 26600 out of 213000\n",
      "done 26700 out of 213000\n",
      "done 26800 out of 213000\n",
      "done 26900 out of 213000\n",
      "done 27000 out of 213000\n",
      "done 27100 out of 213000\n",
      "done 27200 out of 213000\n",
      "done 27300 out of 213000\n",
      "done 27400 out of 213000\n",
      "done 27500 out of 213000\n",
      "done 27600 out of 213000\n",
      "done 27700 out of 213000\n",
      "done 27800 out of 213000\n",
      "done 27900 out of 213000\n",
      "done 28000 out of 213000\n",
      "done 28100 out of 213000\n",
      "done 28200 out of 213000\n",
      "done 28300 out of 213000\n",
      "done 28400 out of 213000\n",
      "done 28500 out of 213000\n",
      "done 28600 out of 213000\n",
      "done 28700 out of 213000\n",
      "done 28800 out of 213000\n",
      "done 28900 out of 213000\n",
      "done 29000 out of 213000\n",
      "done 29100 out of 213000\n",
      "done 29200 out of 213000\n",
      "done 29300 out of 213000\n",
      "done 29400 out of 213000\n",
      "done 29500 out of 213000\n",
      "done 29600 out of 213000\n",
      "done 29700 out of 213000\n",
      "done 29800 out of 213000\n",
      "done 29900 out of 213000\n",
      "done 30000 out of 213000\n",
      "done 30100 out of 213000\n",
      "done 30200 out of 213000\n",
      "done 30300 out of 213000\n",
      "done 30400 out of 213000\n",
      "done 30500 out of 213000\n",
      "done 30600 out of 213000\n",
      "done 30700 out of 213000\n",
      "done 30800 out of 213000\n",
      "done 30900 out of 213000\n",
      "done 31000 out of 213000\n",
      "done 31100 out of 213000\n",
      "done 31200 out of 213000\n",
      "done 31300 out of 213000\n",
      "done 31400 out of 213000\n",
      "done 31500 out of 213000\n",
      "done 31600 out of 213000\n",
      "done 31700 out of 213000\n",
      "done 31800 out of 213000\n",
      "done 31900 out of 213000\n",
      "done 32000 out of 213000\n",
      "done 32100 out of 213000\n",
      "done 32200 out of 213000\n",
      "done 32300 out of 213000\n",
      "done 32400 out of 213000\n",
      "done 32500 out of 213000\n",
      "done 32600 out of 213000\n",
      "done 32700 out of 213000\n",
      "done 32800 out of 213000\n",
      "done 32900 out of 213000\n",
      "done 33000 out of 213000\n",
      "done 33100 out of 213000\n",
      "done 33200 out of 213000\n",
      "done 33300 out of 213000\n",
      "done 33400 out of 213000\n",
      "done 33500 out of 213000\n",
      "done 33600 out of 213000\n",
      "done 33700 out of 213000\n",
      "done 33800 out of 213000\n",
      "done 33900 out of 213000\n",
      "done 34000 out of 213000\n",
      "done 34100 out of 213000\n",
      "done 34200 out of 213000\n",
      "done 34300 out of 213000\n",
      "done 34400 out of 213000\n",
      "done 34500 out of 213000\n",
      "done 34600 out of 213000\n",
      "done 34700 out of 213000\n",
      "done 34800 out of 213000\n",
      "done 34900 out of 213000\n",
      "done 35000 out of 213000\n",
      "done 35100 out of 213000\n",
      "done 35200 out of 213000\n",
      "done 35300 out of 213000\n",
      "done 35400 out of 213000\n",
      "done 35500 out of 213000\n",
      "done 35600 out of 213000\n",
      "done 35700 out of 213000\n",
      "done 35800 out of 213000\n",
      "done 35900 out of 213000\n",
      "done 36000 out of 213000\n",
      "done 36100 out of 213000\n",
      "done 36200 out of 213000\n",
      "done 36300 out of 213000\n",
      "done 36400 out of 213000\n",
      "done 36500 out of 213000\n",
      "done 36600 out of 213000\n",
      "done 36700 out of 213000\n",
      "done 36800 out of 213000\n",
      "done 36900 out of 213000\n",
      "done 37000 out of 213000\n",
      "done 37100 out of 213000\n",
      "done 37200 out of 213000\n",
      "done 37300 out of 213000\n",
      "done 37400 out of 213000\n",
      "done 37500 out of 213000\n",
      "done 37600 out of 213000\n",
      "done 37700 out of 213000\n",
      "done 37800 out of 213000\n",
      "done 37900 out of 213000\n",
      "done 38000 out of 213000\n",
      "done 38100 out of 213000\n",
      "done 38200 out of 213000\n",
      "done 38300 out of 213000\n",
      "done 38400 out of 213000\n",
      "done 38500 out of 213000\n",
      "done 38600 out of 213000\n",
      "done 38700 out of 213000\n",
      "done 38800 out of 213000\n",
      "done 38900 out of 213000\n",
      "done 39000 out of 213000\n",
      "done 39100 out of 213000\n",
      "done 39200 out of 213000\n",
      "done 39300 out of 213000\n",
      "done 39400 out of 213000\n",
      "done 39500 out of 213000\n",
      "done 39600 out of 213000\n",
      "done 39700 out of 213000\n",
      "done 39800 out of 213000\n",
      "done 39900 out of 213000\n",
      "done 40000 out of 213000\n",
      "done 40100 out of 213000\n",
      "done 40200 out of 213000\n",
      "done 40300 out of 213000\n",
      "done 40400 out of 213000\n",
      "done 40500 out of 213000\n",
      "done 40600 out of 213000\n",
      "done 40700 out of 213000\n",
      "done 40800 out of 213000\n",
      "done 40900 out of 213000\n",
      "done 41000 out of 213000\n",
      "done 41100 out of 213000\n",
      "done 41200 out of 213000\n",
      "done 41300 out of 213000\n",
      "done 41400 out of 213000\n",
      "done 41500 out of 213000\n",
      "done 41600 out of 213000\n",
      "done 41700 out of 213000\n",
      "done 41800 out of 213000\n",
      "done 41900 out of 213000\n",
      "done 42000 out of 213000\n",
      "done 42100 out of 213000\n",
      "done 42200 out of 213000\n",
      "done 42300 out of 213000\n",
      "done 42400 out of 213000\n",
      "done 42500 out of 213000\n",
      "done 42600 out of 213000\n",
      "done 42700 out of 213000\n",
      "done 42800 out of 213000\n",
      "done 42900 out of 213000\n",
      "done 43000 out of 213000\n",
      "done 43100 out of 213000\n",
      "done 43200 out of 213000\n",
      "done 43300 out of 213000\n",
      "done 43400 out of 213000\n",
      "done 43500 out of 213000\n",
      "done 43600 out of 213000\n",
      "done 43700 out of 213000\n",
      "done 43800 out of 213000\n",
      "done 43900 out of 213000\n",
      "done 44000 out of 213000\n",
      "done 44100 out of 213000\n",
      "done 44200 out of 213000\n",
      "done 44300 out of 213000\n",
      "done 44400 out of 213000\n",
      "done 44500 out of 213000\n",
      "done 44600 out of 213000\n",
      "done 44700 out of 213000\n",
      "done 44800 out of 213000\n",
      "done 44900 out of 213000\n",
      "done 45000 out of 213000\n",
      "done 45100 out of 213000\n",
      "done 45200 out of 213000\n",
      "done 45300 out of 213000\n",
      "done 45400 out of 213000\n",
      "done 45500 out of 213000\n",
      "done 45600 out of 213000\n",
      "done 45700 out of 213000\n",
      "done 45800 out of 213000\n",
      "done 45900 out of 213000\n",
      "done 46000 out of 213000\n",
      "done 46100 out of 213000\n",
      "done 46200 out of 213000\n",
      "done 46300 out of 213000\n",
      "done 46400 out of 213000\n",
      "done 46500 out of 213000\n",
      "done 46600 out of 213000\n",
      "done 46700 out of 213000\n",
      "done 46800 out of 213000\n",
      "done 46900 out of 213000\n",
      "done 47000 out of 213000\n",
      "done 47100 out of 213000\n",
      "done 47200 out of 213000\n",
      "done 47300 out of 213000\n",
      "done 47400 out of 213000\n",
      "done 47500 out of 213000\n",
      "done 47600 out of 213000\n",
      "done 47700 out of 213000\n",
      "done 47800 out of 213000\n",
      "done 47900 out of 213000\n",
      "done 48000 out of 213000\n",
      "done 48100 out of 213000\n",
      "done 48200 out of 213000\n",
      "done 48300 out of 213000\n",
      "done 48400 out of 213000\n",
      "done 48500 out of 213000\n",
      "done 48600 out of 213000\n",
      "done 48700 out of 213000\n",
      "done 48800 out of 213000\n",
      "done 48900 out of 213000\n",
      "done 49000 out of 213000\n",
      "done 49100 out of 213000\n",
      "done 49200 out of 213000\n",
      "done 49300 out of 213000\n",
      "done 49400 out of 213000\n",
      "done 49500 out of 213000\n",
      "done 49600 out of 213000\n",
      "done 49700 out of 213000\n",
      "done 49800 out of 213000\n",
      "done 49900 out of 213000\n",
      "done 50000 out of 213000\n",
      "done 50100 out of 213000\n",
      "done 50200 out of 213000\n",
      "done 50300 out of 213000\n",
      "done 50400 out of 213000\n",
      "done 50500 out of 213000\n",
      "done 50600 out of 213000\n",
      "done 50700 out of 213000\n",
      "done 50800 out of 213000\n",
      "done 50900 out of 213000\n",
      "done 51000 out of 213000\n",
      "done 51100 out of 213000\n",
      "done 51200 out of 213000\n",
      "done 51300 out of 213000\n",
      "done 51400 out of 213000\n",
      "done 51500 out of 213000\n",
      "done 51600 out of 213000\n",
      "done 51700 out of 213000\n",
      "done 51800 out of 213000\n",
      "done 51900 out of 213000\n",
      "done 52000 out of 213000\n",
      "done 52100 out of 213000\n",
      "done 52200 out of 213000\n",
      "done 52300 out of 213000\n",
      "done 52400 out of 213000\n",
      "done 52500 out of 213000\n",
      "done 52600 out of 213000\n",
      "done 52700 out of 213000\n",
      "done 52800 out of 213000\n",
      "done 52900 out of 213000\n",
      "done 53000 out of 213000\n",
      "done 53100 out of 213000\n",
      "done 53200 out of 213000\n",
      "done 53300 out of 213000\n",
      "done 53400 out of 213000\n",
      "done 53500 out of 213000\n",
      "done 53600 out of 213000\n",
      "done 53700 out of 213000\n",
      "done 53800 out of 213000\n",
      "done 53900 out of 213000\n",
      "done 54000 out of 213000\n",
      "done 54100 out of 213000\n",
      "done 54200 out of 213000\n",
      "done 54300 out of 213000\n",
      "done 54400 out of 213000\n",
      "done 54500 out of 213000\n",
      "done 54600 out of 213000\n",
      "done 54700 out of 213000\n",
      "done 54800 out of 213000\n",
      "done 54900 out of 213000\n",
      "done 55000 out of 213000\n",
      "done 55100 out of 213000\n",
      "done 55200 out of 213000\n",
      "done 55300 out of 213000\n",
      "done 55400 out of 213000\n",
      "done 55500 out of 213000\n",
      "done 55600 out of 213000\n",
      "done 55700 out of 213000\n",
      "done 55800 out of 213000\n",
      "done 55900 out of 213000\n",
      "done 56000 out of 213000\n",
      "done 56100 out of 213000\n",
      "done 56200 out of 213000\n",
      "done 56300 out of 213000\n",
      "done 56400 out of 213000\n",
      "done 56500 out of 213000\n",
      "done 56600 out of 213000\n",
      "done 56700 out of 213000\n",
      "done 56800 out of 213000\n",
      "done 56900 out of 213000\n",
      "done 57000 out of 213000\n",
      "done 57100 out of 213000\n",
      "done 57200 out of 213000\n",
      "done 57300 out of 213000\n",
      "done 57400 out of 213000\n",
      "done 57500 out of 213000\n",
      "done 57600 out of 213000\n",
      "done 57700 out of 213000\n",
      "done 57800 out of 213000\n",
      "done 57900 out of 213000\n",
      "done 58000 out of 213000\n",
      "done 58100 out of 213000\n",
      "done 58200 out of 213000\n",
      "done 58300 out of 213000\n",
      "done 58400 out of 213000\n",
      "done 58500 out of 213000\n",
      "done 58600 out of 213000\n",
      "done 58700 out of 213000\n",
      "done 58800 out of 213000\n",
      "done 58900 out of 213000\n",
      "done 59000 out of 213000\n",
      "done 59100 out of 213000\n",
      "done 59200 out of 213000\n",
      "done 59300 out of 213000\n",
      "done 59400 out of 213000\n",
      "done 59500 out of 213000\n",
      "done 59600 out of 213000\n",
      "done 59700 out of 213000\n",
      "done 59800 out of 213000\n",
      "done 59900 out of 213000\n",
      "done 60000 out of 213000\n",
      "done 60100 out of 213000\n",
      "done 60200 out of 213000\n",
      "done 60300 out of 213000\n",
      "done 60400 out of 213000\n",
      "done 60500 out of 213000\n",
      "done 60600 out of 213000\n",
      "done 60700 out of 213000\n",
      "done 60800 out of 213000\n",
      "done 60900 out of 213000\n",
      "done 61000 out of 213000\n",
      "done 61100 out of 213000\n",
      "done 61200 out of 213000\n",
      "done 61300 out of 213000\n",
      "done 61400 out of 213000\n",
      "done 61500 out of 213000\n",
      "done 61600 out of 213000\n",
      "done 61700 out of 213000\n",
      "done 61800 out of 213000\n",
      "done 61900 out of 213000\n",
      "done 62000 out of 213000\n",
      "done 62100 out of 213000\n",
      "done 62200 out of 213000\n",
      "done 62300 out of 213000\n",
      "done 62400 out of 213000\n",
      "done 62500 out of 213000\n",
      "done 62600 out of 213000\n",
      "done 62700 out of 213000\n",
      "done 62800 out of 213000\n",
      "done 62900 out of 213000\n",
      "done 63000 out of 213000\n",
      "done 63100 out of 213000\n",
      "done 63200 out of 213000\n",
      "done 63300 out of 213000\n",
      "done 63400 out of 213000\n",
      "done 63500 out of 213000\n",
      "done 63600 out of 213000\n",
      "done 63700 out of 213000\n",
      "done 63800 out of 213000\n",
      "done 63900 out of 213000\n",
      "done 64000 out of 213000\n",
      "done 64100 out of 213000\n",
      "done 64200 out of 213000\n",
      "done 64300 out of 213000\n",
      "done 64400 out of 213000\n",
      "done 64500 out of 213000\n",
      "done 64600 out of 213000\n",
      "done 64700 out of 213000\n",
      "done 64800 out of 213000\n",
      "done 64900 out of 213000\n",
      "done 65000 out of 213000\n",
      "done 65100 out of 213000\n",
      "done 65200 out of 213000\n",
      "done 65300 out of 213000\n",
      "done 65400 out of 213000\n",
      "done 65500 out of 213000\n",
      "done 65600 out of 213000\n",
      "done 65700 out of 213000\n",
      "done 65800 out of 213000\n",
      "done 65900 out of 213000\n",
      "done 66000 out of 213000\n",
      "done 66100 out of 213000\n",
      "done 66200 out of 213000\n",
      "done 66300 out of 213000\n",
      "done 66400 out of 213000\n",
      "done 66500 out of 213000\n",
      "done 66600 out of 213000\n",
      "done 66700 out of 213000\n",
      "done 66800 out of 213000\n",
      "done 66900 out of 213000\n",
      "done 67000 out of 213000\n",
      "done 67100 out of 213000\n",
      "done 67200 out of 213000\n",
      "done 67300 out of 213000\n",
      "done 67400 out of 213000\n",
      "done 67500 out of 213000\n",
      "done 67600 out of 213000\n",
      "done 67700 out of 213000\n",
      "done 67800 out of 213000\n",
      "done 67900 out of 213000\n",
      "done 68000 out of 213000\n",
      "done 68100 out of 213000\n",
      "done 68200 out of 213000\n",
      "done 68300 out of 213000\n",
      "done 68400 out of 213000\n",
      "done 68500 out of 213000\n",
      "done 68600 out of 213000\n",
      "done 68700 out of 213000\n",
      "done 68800 out of 213000\n",
      "done 68900 out of 213000\n",
      "done 69000 out of 213000\n",
      "done 69100 out of 213000\n",
      "done 69200 out of 213000\n",
      "done 69300 out of 213000\n",
      "done 69400 out of 213000\n",
      "done 69500 out of 213000\n",
      "done 69600 out of 213000\n",
      "done 69700 out of 213000\n",
      "done 69800 out of 213000\n",
      "done 69900 out of 213000\n",
      "done 70000 out of 213000\n",
      "done 70100 out of 213000\n",
      "done 70200 out of 213000\n",
      "done 70300 out of 213000\n",
      "done 70400 out of 213000\n",
      "done 70500 out of 213000\n",
      "done 70600 out of 213000\n",
      "done 70700 out of 213000\n",
      "done 70800 out of 213000\n",
      "done 70900 out of 213000\n",
      "done 71000 out of 213000\n",
      "done 71100 out of 213000\n",
      "done 71200 out of 213000\n",
      "done 71300 out of 213000\n",
      "done 71400 out of 213000\n",
      "done 71500 out of 213000\n",
      "done 71600 out of 213000\n",
      "done 71700 out of 213000\n",
      "done 71800 out of 213000\n",
      "done 71900 out of 213000\n",
      "done 72000 out of 213000\n",
      "done 72100 out of 213000\n",
      "done 72200 out of 213000\n",
      "done 72300 out of 213000\n",
      "done 72400 out of 213000\n",
      "done 72500 out of 213000\n",
      "done 72600 out of 213000\n",
      "done 72700 out of 213000\n",
      "done 72800 out of 213000\n",
      "done 72900 out of 213000\n",
      "done 73000 out of 213000\n",
      "done 73100 out of 213000\n",
      "done 73200 out of 213000\n",
      "done 73300 out of 213000\n",
      "done 73400 out of 213000\n",
      "done 73500 out of 213000\n",
      "done 73600 out of 213000\n",
      "done 73700 out of 213000\n",
      "done 73800 out of 213000\n",
      "done 73900 out of 213000\n",
      "done 74000 out of 213000\n",
      "done 74100 out of 213000\n",
      "done 74200 out of 213000\n",
      "done 74300 out of 213000\n",
      "done 74400 out of 213000\n",
      "done 74500 out of 213000\n",
      "done 74600 out of 213000\n",
      "done 74700 out of 213000\n",
      "done 74800 out of 213000\n",
      "done 74900 out of 213000\n",
      "done 75000 out of 213000\n",
      "done 75100 out of 213000\n",
      "done 75200 out of 213000\n",
      "done 75300 out of 213000\n",
      "done 75400 out of 213000\n",
      "done 75500 out of 213000\n",
      "done 75600 out of 213000\n",
      "done 75700 out of 213000\n",
      "done 75800 out of 213000\n",
      "done 75900 out of 213000\n",
      "done 76000 out of 213000\n",
      "done 76100 out of 213000\n",
      "done 76200 out of 213000\n",
      "done 76300 out of 213000\n",
      "done 76400 out of 213000\n",
      "done 76500 out of 213000\n",
      "done 76600 out of 213000\n",
      "done 76700 out of 213000\n",
      "done 76800 out of 213000\n",
      "done 76900 out of 213000\n",
      "done 77000 out of 213000\n",
      "done 77100 out of 213000\n",
      "done 77200 out of 213000\n",
      "done 77300 out of 213000\n",
      "done 77400 out of 213000\n",
      "done 77500 out of 213000\n",
      "done 77600 out of 213000\n",
      "done 77700 out of 213000\n",
      "done 77800 out of 213000\n",
      "done 77900 out of 213000\n",
      "done 78000 out of 213000\n",
      "done 78100 out of 213000\n",
      "done 78200 out of 213000\n",
      "done 78300 out of 213000\n",
      "done 78400 out of 213000\n",
      "done 78500 out of 213000\n",
      "done 78600 out of 213000\n",
      "done 78700 out of 213000\n",
      "done 78800 out of 213000\n",
      "done 78900 out of 213000\n",
      "done 79000 out of 213000\n",
      "done 79100 out of 213000\n",
      "done 79200 out of 213000\n",
      "done 79300 out of 213000\n",
      "done 79400 out of 213000\n",
      "done 79500 out of 213000\n",
      "done 79600 out of 213000\n",
      "done 79700 out of 213000\n",
      "done 79800 out of 213000\n",
      "done 79900 out of 213000\n",
      "done 80000 out of 213000\n",
      "done 80100 out of 213000\n",
      "done 80200 out of 213000\n",
      "done 80300 out of 213000\n",
      "done 80400 out of 213000\n",
      "done 80500 out of 213000\n",
      "done 80600 out of 213000\n",
      "done 80700 out of 213000\n",
      "done 80800 out of 213000\n",
      "done 80900 out of 213000\n",
      "done 81000 out of 213000\n",
      "done 81100 out of 213000\n",
      "done 81200 out of 213000\n",
      "done 81300 out of 213000\n",
      "done 81400 out of 213000\n",
      "done 81500 out of 213000\n",
      "done 81600 out of 213000\n",
      "done 81700 out of 213000\n",
      "done 81800 out of 213000\n",
      "done 81900 out of 213000\n",
      "done 82000 out of 213000\n",
      "done 82100 out of 213000\n",
      "done 82200 out of 213000\n",
      "done 82300 out of 213000\n",
      "done 82400 out of 213000\n",
      "done 82500 out of 213000\n",
      "done 82600 out of 213000\n",
      "done 82700 out of 213000\n",
      "done 82800 out of 213000\n",
      "done 82900 out of 213000\n",
      "done 83000 out of 213000\n",
      "done 83100 out of 213000\n",
      "done 83200 out of 213000\n",
      "done 83300 out of 213000\n",
      "done 83400 out of 213000\n",
      "done 83500 out of 213000\n",
      "done 83600 out of 213000\n",
      "done 83700 out of 213000\n",
      "done 83800 out of 213000\n",
      "done 83900 out of 213000\n",
      "done 84000 out of 213000\n",
      "done 84100 out of 213000\n",
      "done 84200 out of 213000\n",
      "done 84300 out of 213000\n",
      "done 84400 out of 213000\n",
      "done 84500 out of 213000\n",
      "done 84600 out of 213000\n",
      "done 84700 out of 213000\n",
      "done 84800 out of 213000\n",
      "done 84900 out of 213000\n",
      "done 85000 out of 213000\n",
      "done 85100 out of 213000\n",
      "done 85200 out of 213000\n",
      "done 85300 out of 213000\n",
      "done 85400 out of 213000\n",
      "done 85500 out of 213000\n",
      "done 85600 out of 213000\n",
      "done 85700 out of 213000\n",
      "done 85800 out of 213000\n",
      "done 85900 out of 213000\n",
      "done 86000 out of 213000\n",
      "done 86100 out of 213000\n",
      "done 86200 out of 213000\n",
      "done 86300 out of 213000\n",
      "done 86400 out of 213000\n",
      "done 86500 out of 213000\n",
      "done 86600 out of 213000\n",
      "done 86700 out of 213000\n",
      "done 86800 out of 213000\n",
      "done 86900 out of 213000\n",
      "done 87000 out of 213000\n",
      "done 87100 out of 213000\n",
      "done 87200 out of 213000\n",
      "done 87300 out of 213000\n",
      "done 87400 out of 213000\n",
      "done 87500 out of 213000\n",
      "done 87600 out of 213000\n",
      "done 87700 out of 213000\n",
      "done 87800 out of 213000\n",
      "done 87900 out of 213000\n",
      "done 88000 out of 213000\n",
      "done 88100 out of 213000\n",
      "done 88200 out of 213000\n",
      "done 88300 out of 213000\n",
      "done 88400 out of 213000\n",
      "done 88500 out of 213000\n",
      "done 88600 out of 213000\n",
      "done 88700 out of 213000\n",
      "done 88800 out of 213000\n",
      "done 88900 out of 213000\n",
      "done 89000 out of 213000\n",
      "done 89100 out of 213000\n",
      "done 89200 out of 213000\n",
      "done 89300 out of 213000\n",
      "done 89400 out of 213000\n",
      "done 89500 out of 213000\n",
      "done 89600 out of 213000\n",
      "done 89700 out of 213000\n",
      "done 89800 out of 213000\n",
      "done 89900 out of 213000\n",
      "done 90000 out of 213000\n",
      "done 90100 out of 213000\n",
      "done 90200 out of 213000\n",
      "done 90300 out of 213000\n",
      "done 90400 out of 213000\n",
      "done 90500 out of 213000\n",
      "done 90600 out of 213000\n",
      "done 90700 out of 213000\n",
      "done 90800 out of 213000\n",
      "done 90900 out of 213000\n",
      "done 91000 out of 213000\n",
      "done 91100 out of 213000\n",
      "done 91200 out of 213000\n",
      "done 91300 out of 213000\n",
      "done 91400 out of 213000\n",
      "done 91500 out of 213000\n",
      "done 91600 out of 213000\n",
      "done 91700 out of 213000\n",
      "done 91800 out of 213000\n",
      "done 91900 out of 213000\n",
      "done 92000 out of 213000\n",
      "done 92100 out of 213000\n",
      "done 92200 out of 213000\n",
      "done 92300 out of 213000\n",
      "done 92400 out of 213000\n",
      "done 92500 out of 213000\n",
      "done 92600 out of 213000\n",
      "done 92700 out of 213000\n",
      "done 92800 out of 213000\n",
      "done 92900 out of 213000\n",
      "done 93000 out of 213000\n",
      "done 93100 out of 213000\n",
      "done 93200 out of 213000\n",
      "done 93300 out of 213000\n",
      "done 93400 out of 213000\n",
      "done 93500 out of 213000\n",
      "done 93600 out of 213000\n",
      "done 93700 out of 213000\n",
      "done 93800 out of 213000\n",
      "done 93900 out of 213000\n",
      "done 94000 out of 213000\n",
      "done 94100 out of 213000\n",
      "done 94200 out of 213000\n",
      "done 94300 out of 213000\n",
      "done 94400 out of 213000\n",
      "done 94500 out of 213000\n",
      "done 94600 out of 213000\n",
      "done 94700 out of 213000\n",
      "done 94800 out of 213000\n",
      "done 94900 out of 213000\n",
      "done 95000 out of 213000\n",
      "done 95100 out of 213000\n",
      "done 95200 out of 213000\n",
      "done 95300 out of 213000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-58535419e0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mselftext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtitle_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mselftext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-1bfacbadcbbb>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(corpus, vocab)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mblank\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mLangClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLangClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, make_doc, max_length, meta, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmake_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[0;34m(cls, nlp)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0msuffix_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix_search\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minfix_finditer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfix_finditer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mtoken_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_match\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.make_fused_token\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "table = []\n",
    "EOS = 'xxxeos'\n",
    "BOS = 'xxxbos'\n",
    "TITLE_START = 'xxxts'\n",
    "TITLE_END = 'xxxte'\n",
    "prog = 0\n",
    "for index, row in df.iterrows():\n",
    "    title = row[-2]\n",
    "    selftext = row[-1]\n",
    "    \n",
    "    if(prog % 100 == 0):\n",
    "        print('done ' + str(prog) + ' out of ' + str(len(df)))\n",
    "  \n",
    "    for f in preprocess_funcs:\n",
    "        title = f(title)\n",
    "        selftext = f(selftext)\n",
    "    \n",
    "    title_tokens = tokenize(title)\n",
    "    selftext_tokens = tokenize(selftext)\n",
    "    \n",
    "    title_tokens = [TITLE_START] + title_tokens + [TITLE_END]\n",
    "    selftext_tokens = [BOS] + selftext_tokens +  [EOS]\n",
    "    \n",
    "    table.append([row[0], row[1], title_tokens, selftext_tokens])\n",
    "    prog +=1 \n",
    "   \n",
    "    \n",
    "\n",
    "good_df = pd.DataFrame(table, columns = ['id', 'subreddit', 'title', 'selftext'])\n",
    "good_df.to_csv(base_path/'processed_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdvMPn7O6iYP"
   },
   "outputs": [],
   "source": [
    "good_df = pd.DataFrame(table, columns = ['id', 'subreddit', 'title', 'selftext'])\n",
    "good_df.to_csv(base_path/'processed_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1568462987346,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "2arj0POmoHms",
    "outputId": "d2162845-9d6b-417f-cd5f-21a209129a59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>[xxxts, not, door, bell, ,, but, floodlight, m...</td>\n",
       "      <td>[xxxbos, i, know, this, is, a, sub, for, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>[xxxts, worried, about, my, 8700k, small, fft,...</td>\n",
       "      <td>[xxxbos, prime95, (, regardless, of, version, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6icvfu</td>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>[xxxts, inline, hockey, :, where, do, i, need,...</td>\n",
       "      <td>[xxxbos, my, game, is, coming, on, well, but, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6azhj1</td>\n",
       "      <td>rawdenim</td>\n",
       "      <td>[xxxts, had, a, custom, embroidery, job, done,...</td>\n",
       "      <td>[xxxbos, [, album, first](http, :, /, /, imgur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7i2bt4</td>\n",
       "      <td>NameThatSong</td>\n",
       "      <td>[xxxts, can, anyone, help, me, find, this, ban...</td>\n",
       "      <td>[xxxbos, thinking, of, ', who, the, band, soun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  ...                                           selftext\n",
       "0  6ti6re  ...  [xxxbos, i, know, this, is, a, sub, for, the, ...\n",
       "1  77sxto  ...  [xxxbos, prime95, (, regardless, of, version, ...\n",
       "2  6icvfu  ...  [xxxbos, my, game, is, coming, on, well, but, ...\n",
       "3  6azhj1  ...  [xxxbos, [, album, first](http, :, /, /, imgur...\n",
       "4  7i2bt4  ...  [xxxbos, thinking, of, ', who, the, band, soun...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfKly-x0mozq"
   },
   "outputs": [],
   "source": [
    "def converter(x):\n",
    "    #convert \"list\" to list\n",
    "    return literal_eval(x)\n",
    "base_path = Path('storage/htn2019')\n",
    "converters={'title': converter, 'selftext': converter}\n",
    "df = pd.read_csv(base_path/'processed_data.csv', converters = converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_classification(df, vocab, category_encoder, pct = 0.1):\n",
    "    index = int(len(df) * pct)\n",
    "    train_df = df.iloc[0:(len(df) - index), :]\n",
    "    valid_df = df.iloc[(len(df) - index):, :]\n",
    "    \n",
    "    train_txt_ds = ClassificationDataset(train_df, vocab, category_encoder)\n",
    "    valid_txt_ds = ClassificationDataset(valid_df, vocab, category_encoder)\n",
    "    \n",
    "    return train_txt_ds, valid_txt_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1568463113606,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "YuqhpEd0rz7x",
    "outputId": "5cbef383-d068-4ef4-f302-4f25b90f5e7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxxbos',\n",
       " 'i',\n",
       " 'know',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sub',\n",
       " 'for',\n",
       " 'the',\n",
       " \"'\",\n",
       " 'ring',\n",
       " 'doorbell',\n",
       " \"'\",\n",
       " 'but',\n",
       " 'has',\n",
       " 'anyone',\n",
       " 'used',\n",
       " 'the',\n",
       " 'floodlight',\n",
       " '?',\n",
       " 'i',\n",
       " 'already',\n",
       " 'have',\n",
       " 'the',\n",
       " 'wire',\n",
       " 'and',\n",
       " 'existing',\n",
       " 'bracket',\n",
       " 'for',\n",
       " 'the',\n",
       " 'floodlight',\n",
       " 'on',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'my',\n",
       " 'house',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'about',\n",
       " '12',\n",
       " 'feet',\n",
       " 'above',\n",
       " 'ground',\n",
       " 'level',\n",
       " '(',\n",
       " '10',\n",
       " 'ft',\n",
       " 'above',\n",
       " 'the',\n",
       " 'deck',\n",
       " ',',\n",
       " '2',\n",
       " 'ft',\n",
       " 'drop',\n",
       " 'from',\n",
       " 'the',\n",
       " 'deck',\n",
       " 'down',\n",
       " 'to',\n",
       " 'the',\n",
       " 'grass',\n",
       " ')',\n",
       " 'is',\n",
       " 'that',\n",
       " 'too',\n",
       " 'high',\n",
       " 'to',\n",
       " 'mount',\n",
       " '?',\n",
       " 'the',\n",
       " 'website',\n",
       " 'says',\n",
       " '9',\n",
       " 'ft',\n",
       " 'is',\n",
       " 'ideal',\n",
       " '.',\n",
       " 'anyone',\n",
       " 'had',\n",
       " 'any',\n",
       " 'problems',\n",
       " 'mounting',\n",
       " 'it',\n",
       " 'higher',\n",
       " 'than',\n",
       " 'that',\n",
       " '?',\n",
       " 'xxxeos']"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KPTT-BQZQUT"
   },
   "outputs": [],
   "source": [
    "with open(base_path/'corpus.txt', 'w+') as f:\n",
    "    f.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1568447955272,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "8Kgj9XNMsAM6",
    "outputId": "da0d0b0a-7f7a-4c8c-f660-1d74d71cb16b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxxbos i know this is a sub for the 'ring doorbell' but has anyone used the floodlight? i already ha\""
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2nkANWKduGe"
   },
   "outputs": [],
   "source": [
    "corpus = None\n",
    "with open(base_path/'corpus.txt', 'r') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kR2efKp9l1MQ"
   },
   "outputs": [],
   "source": [
    "UNK = 'xxxunk'\n",
    "PAD = 'xxxpad'\n",
    "BOS = 'xxxbos'\n",
    "EOS = 'xxxeos'\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS]\n",
    "\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    if max_workers<2: results = list(map(func, enumerate(arr)), total=len(arr))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(ex.map(func, enumerate(arr)), total=len(arr))\n",
    "    if any([o is not None for o in results]): return results\n",
    "\n",
    "class Processor(): \n",
    "    def process(self, items): return items\n",
    "    \n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): \n",
    "        self.chunksize,self.max_workers = chunksize,max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        #self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n",
    "        #self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "\n",
    "    def proc_chunk(self, args):\n",
    "        i,chunk = args\n",
    "        #chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        #docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, items): \n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n",
    "        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "    \n",
    "    def proc1(self, item): return self.proc_chunk([item])[0]\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1071523,
     "status": "ok",
     "timestamp": 1568466457444,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "yHwofWd4i_Vd",
    "outputId": "0bd5a866-6bb3-428c-8470-ae9bf4c6a171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0/95331\n",
      "done 100/95331\n",
      "done 200/95331\n",
      "done 300/95331\n",
      "done 400/95331\n",
      "done 500/95331\n",
      "done 600/95331\n",
      "done 700/95331\n",
      "done 800/95331\n",
      "done 900/95331\n",
      "done 1000/95331\n",
      "done 1100/95331\n",
      "done 1200/95331\n",
      "done 1300/95331\n",
      "done 1400/95331\n",
      "done 1500/95331\n",
      "done 1600/95331\n",
      "done 1700/95331\n",
      "done 1800/95331\n",
      "done 1900/95331\n",
      "done 2000/95331\n",
      "done 2100/95331\n",
      "done 2200/95331\n",
      "done 2300/95331\n",
      "done 2400/95331\n",
      "done 2500/95331\n",
      "done 2600/95331\n",
      "done 2700/95331\n",
      "done 2800/95331\n",
      "done 2900/95331\n",
      "done 3000/95331\n",
      "done 3100/95331\n",
      "done 3200/95331\n",
      "done 3300/95331\n",
      "done 3400/95331\n",
      "done 3500/95331\n",
      "done 3600/95331\n",
      "done 3700/95331\n",
      "done 3800/95331\n",
      "done 3900/95331\n",
      "done 4000/95331\n",
      "done 4100/95331\n",
      "done 4200/95331\n",
      "done 4300/95331\n",
      "done 4400/95331\n",
      "done 4500/95331\n",
      "done 4600/95331\n",
      "done 4700/95331\n",
      "done 4800/95331\n",
      "done 4900/95331\n",
      "done 5000/95331\n",
      "done 5100/95331\n",
      "done 5200/95331\n",
      "done 5300/95331\n",
      "done 5400/95331\n",
      "done 5500/95331\n",
      "done 5600/95331\n",
      "done 5700/95331\n",
      "done 5800/95331\n",
      "done 5900/95331\n",
      "done 6000/95331\n",
      "done 6100/95331\n",
      "done 6200/95331\n",
      "done 6300/95331\n",
      "done 6400/95331\n",
      "done 6500/95331\n",
      "done 6600/95331\n",
      "done 6700/95331\n",
      "done 6800/95331\n",
      "done 6900/95331\n",
      "done 7000/95331\n",
      "done 7100/95331\n",
      "done 7200/95331\n",
      "done 7300/95331\n",
      "done 7400/95331\n",
      "done 7500/95331\n",
      "done 7600/95331\n",
      "done 7700/95331\n",
      "done 7800/95331\n",
      "done 7900/95331\n",
      "done 8000/95331\n",
      "done 8100/95331\n",
      "done 8200/95331\n",
      "done 8300/95331\n",
      "done 8400/95331\n",
      "done 8500/95331\n",
      "done 8600/95331\n",
      "done 8700/95331\n",
      "done 8800/95331\n",
      "done 8900/95331\n",
      "done 9000/95331\n",
      "done 9100/95331\n",
      "done 9200/95331\n",
      "done 9300/95331\n",
      "done 9400/95331\n",
      "done 9500/95331\n",
      "done 9600/95331\n",
      "done 9700/95331\n",
      "done 9800/95331\n",
      "done 9900/95331\n",
      "done 10000/95331\n",
      "done 10100/95331\n",
      "done 10200/95331\n",
      "done 10300/95331\n",
      "done 10400/95331\n",
      "done 10500/95331\n",
      "done 10600/95331\n",
      "done 10700/95331\n",
      "done 10800/95331\n",
      "done 10900/95331\n",
      "done 11000/95331\n",
      "done 11100/95331\n",
      "done 11200/95331\n",
      "done 11300/95331\n",
      "done 11400/95331\n",
      "done 11500/95331\n",
      "done 11600/95331\n",
      "done 11700/95331\n",
      "done 11800/95331\n",
      "done 11900/95331\n",
      "done 12000/95331\n",
      "done 12100/95331\n",
      "done 12200/95331\n",
      "done 12300/95331\n",
      "done 12400/95331\n",
      "done 12500/95331\n",
      "done 12600/95331\n",
      "done 12700/95331\n",
      "done 12800/95331\n",
      "done 12900/95331\n",
      "done 13000/95331\n",
      "done 13100/95331\n",
      "done 13200/95331\n",
      "done 13300/95331\n",
      "done 13400/95331\n",
      "done 13500/95331\n",
      "done 13600/95331\n",
      "done 13700/95331\n",
      "done 13800/95331\n",
      "done 13900/95331\n",
      "done 14000/95331\n",
      "done 14100/95331\n",
      "done 14200/95331\n",
      "done 14300/95331\n",
      "done 14400/95331\n",
      "done 14500/95331\n",
      "done 14600/95331\n",
      "done 14700/95331\n",
      "done 14800/95331\n",
      "done 14900/95331\n",
      "done 15000/95331\n",
      "done 15100/95331\n",
      "done 15200/95331\n",
      "done 15300/95331\n",
      "done 15400/95331\n",
      "done 15500/95331\n",
      "done 15600/95331\n",
      "done 15700/95331\n",
      "done 15800/95331\n",
      "done 15900/95331\n",
      "done 16000/95331\n",
      "done 16100/95331\n",
      "done 16200/95331\n",
      "done 16300/95331\n",
      "done 16400/95331\n",
      "done 16500/95331\n",
      "done 16600/95331\n",
      "done 16700/95331\n",
      "done 16800/95331\n",
      "done 16900/95331\n",
      "done 17000/95331\n",
      "done 17100/95331\n",
      "done 17200/95331\n",
      "done 17300/95331\n",
      "done 17400/95331\n",
      "done 17500/95331\n",
      "done 17600/95331\n",
      "done 17700/95331\n",
      "done 17800/95331\n",
      "done 17900/95331\n",
      "done 18000/95331\n",
      "done 18100/95331\n",
      "done 18200/95331\n",
      "done 18300/95331\n",
      "done 18400/95331\n",
      "done 18500/95331\n",
      "done 18600/95331\n",
      "done 18700/95331\n",
      "done 18800/95331\n",
      "done 18900/95331\n",
      "done 19000/95331\n",
      "done 19100/95331\n",
      "done 19200/95331\n",
      "done 19300/95331\n",
      "done 19400/95331\n",
      "done 19500/95331\n",
      "done 19600/95331\n",
      "done 19700/95331\n",
      "done 19800/95331\n",
      "done 19900/95331\n",
      "done 20000/95331\n",
      "done 20100/95331\n",
      "done 20200/95331\n",
      "done 20300/95331\n",
      "done 20400/95331\n",
      "done 20500/95331\n",
      "done 20600/95331\n",
      "done 20700/95331\n",
      "done 20800/95331\n",
      "done 20900/95331\n",
      "done 21000/95331\n",
      "done 21100/95331\n",
      "done 21200/95331\n",
      "done 21300/95331\n",
      "done 21400/95331\n",
      "done 21500/95331\n",
      "done 21600/95331\n",
      "done 21700/95331\n",
      "done 21800/95331\n",
      "done 21900/95331\n",
      "done 22000/95331\n",
      "done 22100/95331\n",
      "done 22200/95331\n",
      "done 22300/95331\n",
      "done 22400/95331\n",
      "done 22500/95331\n",
      "done 22600/95331\n",
      "done 22700/95331\n",
      "done 22800/95331\n",
      "done 22900/95331\n",
      "done 23000/95331\n",
      "done 23100/95331\n",
      "done 23200/95331\n",
      "done 23300/95331\n",
      "done 23400/95331\n",
      "done 23500/95331\n",
      "done 23600/95331\n",
      "done 23700/95331\n",
      "done 23800/95331\n",
      "done 23900/95331\n",
      "done 24000/95331\n",
      "done 24100/95331\n",
      "done 24200/95331\n",
      "done 24300/95331\n",
      "done 24400/95331\n",
      "done 24500/95331\n",
      "done 24600/95331\n",
      "done 24700/95331\n",
      "done 24800/95331\n",
      "done 24900/95331\n",
      "done 25000/95331\n",
      "done 25100/95331\n",
      "done 25200/95331\n",
      "done 25300/95331\n",
      "done 25400/95331\n",
      "done 25500/95331\n",
      "done 25600/95331\n",
      "done 25700/95331\n",
      "done 25800/95331\n",
      "done 25900/95331\n",
      "done 26000/95331\n",
      "done 26100/95331\n",
      "done 26200/95331\n",
      "done 26300/95331\n",
      "done 26400/95331\n",
      "done 26500/95331\n",
      "done 26600/95331\n",
      "done 26700/95331\n",
      "done 26800/95331\n",
      "done 26900/95331\n",
      "done 27000/95331\n",
      "done 27100/95331\n",
      "done 27200/95331\n",
      "done 27300/95331\n",
      "done 27400/95331\n",
      "done 27500/95331\n",
      "done 27600/95331\n",
      "done 27700/95331\n",
      "done 27800/95331\n",
      "done 27900/95331\n",
      "done 28000/95331\n",
      "done 28100/95331\n",
      "done 28200/95331\n",
      "done 28300/95331\n",
      "done 28400/95331\n",
      "done 28500/95331\n",
      "done 28600/95331\n",
      "done 28700/95331\n",
      "done 28800/95331\n",
      "done 28900/95331\n",
      "done 29000/95331\n",
      "done 29100/95331\n",
      "done 29200/95331\n",
      "done 29300/95331\n",
      "done 29400/95331\n",
      "done 29500/95331\n",
      "done 29600/95331\n",
      "done 29700/95331\n",
      "done 29800/95331\n",
      "done 29900/95331\n",
      "done 30000/95331\n",
      "done 30100/95331\n",
      "done 30200/95331\n",
      "done 30300/95331\n",
      "done 30400/95331\n",
      "done 30500/95331\n",
      "done 30600/95331\n",
      "done 30700/95331\n",
      "done 30800/95331\n",
      "done 30900/95331\n",
      "done 31000/95331\n",
      "done 31100/95331\n",
      "done 31200/95331\n",
      "done 31300/95331\n",
      "done 31400/95331\n",
      "done 31500/95331\n",
      "done 31600/95331\n",
      "done 31700/95331\n",
      "done 31800/95331\n",
      "done 31900/95331\n",
      "done 32000/95331\n",
      "done 32100/95331\n",
      "done 32200/95331\n",
      "done 32300/95331\n",
      "done 32400/95331\n",
      "done 32500/95331\n",
      "done 32600/95331\n",
      "done 32700/95331\n",
      "done 32800/95331\n",
      "done 32900/95331\n",
      "done 33000/95331\n",
      "done 33100/95331\n",
      "done 33200/95331\n",
      "done 33300/95331\n",
      "done 33400/95331\n",
      "done 33500/95331\n",
      "done 33600/95331\n",
      "done 33700/95331\n",
      "done 33800/95331\n",
      "done 33900/95331\n",
      "done 34000/95331\n",
      "done 34100/95331\n",
      "done 34200/95331\n",
      "done 34300/95331\n",
      "done 34400/95331\n",
      "done 34500/95331\n",
      "done 34600/95331\n",
      "done 34700/95331\n",
      "done 34800/95331\n",
      "done 34900/95331\n",
      "done 35000/95331\n",
      "done 35100/95331\n",
      "done 35200/95331\n",
      "done 35300/95331\n",
      "done 35400/95331\n",
      "done 35500/95331\n",
      "done 35600/95331\n",
      "done 35700/95331\n",
      "done 35800/95331\n",
      "done 35900/95331\n",
      "done 36000/95331\n",
      "done 36100/95331\n",
      "done 36200/95331\n",
      "done 36300/95331\n",
      "done 36400/95331\n",
      "done 36500/95331\n",
      "done 36600/95331\n",
      "done 36700/95331\n",
      "done 36800/95331\n",
      "done 36900/95331\n",
      "done 37000/95331\n",
      "done 37100/95331\n",
      "done 37200/95331\n",
      "done 37300/95331\n",
      "done 37400/95331\n",
      "done 37500/95331\n",
      "done 37600/95331\n",
      "done 37700/95331\n",
      "done 37800/95331\n",
      "done 37900/95331\n",
      "done 38000/95331\n",
      "done 38100/95331\n",
      "done 38200/95331\n",
      "done 38300/95331\n",
      "done 38400/95331\n",
      "done 38500/95331\n",
      "done 38600/95331\n",
      "done 38700/95331\n",
      "done 38800/95331\n",
      "done 38900/95331\n",
      "done 39000/95331\n",
      "done 39100/95331\n",
      "done 39200/95331\n",
      "done 39300/95331\n",
      "done 39400/95331\n",
      "done 39500/95331\n",
      "done 39600/95331\n",
      "done 39700/95331\n",
      "done 39800/95331\n",
      "done 39900/95331\n",
      "done 40000/95331\n",
      "done 40100/95331\n",
      "done 40200/95331\n",
      "done 40300/95331\n",
      "done 40400/95331\n",
      "done 40500/95331\n",
      "done 40600/95331\n",
      "done 40700/95331\n",
      "done 40800/95331\n",
      "done 40900/95331\n",
      "done 41000/95331\n",
      "done 41100/95331\n",
      "done 41200/95331\n",
      "done 41300/95331\n",
      "done 41400/95331\n",
      "done 41500/95331\n",
      "done 41600/95331\n",
      "done 41700/95331\n",
      "done 41800/95331\n",
      "done 41900/95331\n",
      "done 42000/95331\n",
      "done 42100/95331\n",
      "done 42200/95331\n",
      "done 42300/95331\n",
      "done 42400/95331\n",
      "done 42500/95331\n",
      "done 42600/95331\n",
      "done 42700/95331\n",
      "done 42800/95331\n",
      "done 42900/95331\n",
      "done 43000/95331\n",
      "done 43100/95331\n",
      "done 43200/95331\n",
      "done 43300/95331\n",
      "done 43400/95331\n",
      "done 43500/95331\n",
      "done 43600/95331\n",
      "done 43700/95331\n",
      "done 43800/95331\n",
      "done 43900/95331\n",
      "done 44000/95331\n",
      "done 44100/95331\n",
      "done 44200/95331\n",
      "done 44300/95331\n",
      "done 44400/95331\n",
      "done 44500/95331\n",
      "done 44600/95331\n",
      "done 44700/95331\n",
      "done 44800/95331\n",
      "done 44900/95331\n",
      "done 45000/95331\n",
      "done 45100/95331\n",
      "done 45200/95331\n",
      "done 45300/95331\n",
      "done 45400/95331\n",
      "done 45500/95331\n",
      "done 45600/95331\n",
      "done 45700/95331\n",
      "done 45800/95331\n",
      "done 45900/95331\n",
      "done 46000/95331\n",
      "done 46100/95331\n",
      "done 46200/95331\n",
      "done 46300/95331\n",
      "done 46400/95331\n",
      "done 46500/95331\n",
      "done 46600/95331\n",
      "done 46700/95331\n",
      "done 46800/95331\n",
      "done 46900/95331\n",
      "done 47000/95331\n",
      "done 47100/95331\n",
      "done 47200/95331\n",
      "done 47300/95331\n",
      "done 47400/95331\n",
      "done 47500/95331\n",
      "done 47600/95331\n",
      "done 47700/95331\n",
      "done 47800/95331\n",
      "done 47900/95331\n",
      "done 48000/95331\n",
      "done 48100/95331\n",
      "done 48200/95331\n",
      "done 48300/95331\n",
      "done 48400/95331\n",
      "done 48500/95331\n",
      "done 48600/95331\n",
      "done 48700/95331\n",
      "done 48800/95331\n",
      "done 48900/95331\n",
      "done 49000/95331\n",
      "done 49100/95331\n",
      "done 49200/95331\n",
      "done 49300/95331\n",
      "done 49400/95331\n",
      "done 49500/95331\n",
      "done 49600/95331\n",
      "done 49700/95331\n",
      "done 49800/95331\n",
      "done 49900/95331\n",
      "done 50000/95331\n",
      "done 50100/95331\n",
      "done 50200/95331\n",
      "done 50300/95331\n",
      "done 50400/95331\n",
      "done 50500/95331\n",
      "done 50600/95331\n",
      "done 50700/95331\n",
      "done 50800/95331\n",
      "done 50900/95331\n",
      "done 51000/95331\n",
      "done 51100/95331\n",
      "done 51200/95331\n",
      "done 51300/95331\n",
      "done 51400/95331\n",
      "done 51500/95331\n",
      "done 51600/95331\n",
      "done 51700/95331\n",
      "done 51800/95331\n",
      "done 51900/95331\n",
      "done 52000/95331\n",
      "done 52100/95331\n",
      "done 52200/95331\n",
      "done 52300/95331\n",
      "done 52400/95331\n",
      "done 52500/95331\n",
      "done 52600/95331\n",
      "done 52700/95331\n",
      "done 52800/95331\n",
      "done 52900/95331\n",
      "done 53000/95331\n",
      "done 53100/95331\n",
      "done 53200/95331\n",
      "done 53300/95331\n",
      "done 53400/95331\n",
      "done 53500/95331\n",
      "done 53600/95331\n",
      "done 53700/95331\n",
      "done 53800/95331\n",
      "done 53900/95331\n",
      "done 54000/95331\n",
      "done 54100/95331\n",
      "done 54200/95331\n",
      "done 54300/95331\n",
      "done 54400/95331\n",
      "done 54500/95331\n",
      "done 54600/95331\n",
      "done 54700/95331\n",
      "done 54800/95331\n",
      "done 54900/95331\n",
      "done 55000/95331\n",
      "done 55100/95331\n",
      "done 55200/95331\n",
      "done 55300/95331\n",
      "done 55400/95331\n",
      "done 55500/95331\n",
      "done 55600/95331\n",
      "done 55700/95331\n",
      "done 55800/95331\n",
      "done 55900/95331\n",
      "done 56000/95331\n",
      "done 56100/95331\n",
      "done 56200/95331\n",
      "done 56300/95331\n",
      "done 56400/95331\n",
      "done 56500/95331\n",
      "done 56600/95331\n",
      "done 56700/95331\n",
      "done 56800/95331\n",
      "done 56900/95331\n",
      "done 57000/95331\n",
      "done 57100/95331\n",
      "done 57200/95331\n",
      "done 57300/95331\n",
      "done 57400/95331\n",
      "done 57500/95331\n",
      "done 57600/95331\n",
      "done 57700/95331\n",
      "done 57800/95331\n",
      "done 57900/95331\n",
      "done 58000/95331\n",
      "done 58100/95331\n",
      "done 58200/95331\n",
      "done 58300/95331\n",
      "done 58400/95331\n",
      "done 58500/95331\n",
      "done 58600/95331\n",
      "done 58700/95331\n",
      "done 58800/95331\n",
      "done 58900/95331\n",
      "done 59000/95331\n",
      "done 59100/95331\n",
      "done 59200/95331\n",
      "done 59300/95331\n",
      "done 59400/95331\n",
      "done 59500/95331\n",
      "done 59600/95331\n",
      "done 59700/95331\n",
      "done 59800/95331\n",
      "done 59900/95331\n",
      "done 60000/95331\n",
      "done 60100/95331\n",
      "done 60200/95331\n",
      "done 60300/95331\n",
      "done 60400/95331\n",
      "done 60500/95331\n",
      "done 60600/95331\n",
      "done 60700/95331\n",
      "done 60800/95331\n",
      "done 60900/95331\n",
      "done 61000/95331\n",
      "done 61100/95331\n",
      "done 61200/95331\n",
      "done 61300/95331\n",
      "done 61400/95331\n",
      "done 61500/95331\n",
      "done 61600/95331\n",
      "done 61700/95331\n",
      "done 61800/95331\n",
      "done 61900/95331\n",
      "done 62000/95331\n",
      "done 62100/95331\n",
      "done 62200/95331\n",
      "done 62300/95331\n",
      "done 62400/95331\n",
      "done 62500/95331\n",
      "done 62600/95331\n",
      "done 62700/95331\n",
      "done 62800/95331\n",
      "done 62900/95331\n",
      "done 63000/95331\n",
      "done 63100/95331\n",
      "done 63200/95331\n",
      "done 63300/95331\n",
      "done 63400/95331\n",
      "done 63500/95331\n",
      "done 63600/95331\n",
      "done 63700/95331\n",
      "done 63800/95331\n",
      "done 63900/95331\n",
      "done 64000/95331\n",
      "done 64100/95331\n",
      "done 64200/95331\n",
      "done 64300/95331\n",
      "done 64400/95331\n",
      "done 64500/95331\n",
      "done 64600/95331\n",
      "done 64700/95331\n",
      "done 64800/95331\n",
      "done 64900/95331\n",
      "done 65000/95331\n",
      "done 65100/95331\n",
      "done 65200/95331\n",
      "done 65300/95331\n",
      "done 65400/95331\n",
      "done 65500/95331\n",
      "done 65600/95331\n",
      "done 65700/95331\n",
      "done 65800/95331\n",
      "done 65900/95331\n",
      "done 66000/95331\n",
      "done 66100/95331\n",
      "done 66200/95331\n",
      "done 66300/95331\n",
      "done 66400/95331\n",
      "done 66500/95331\n",
      "done 66600/95331\n",
      "done 66700/95331\n",
      "done 66800/95331\n",
      "done 66900/95331\n",
      "done 67000/95331\n",
      "done 67100/95331\n",
      "done 67200/95331\n",
      "done 67300/95331\n",
      "done 67400/95331\n",
      "done 67500/95331\n",
      "done 67600/95331\n",
      "done 67700/95331\n",
      "done 67800/95331\n",
      "done 67900/95331\n",
      "done 68000/95331\n",
      "done 68100/95331\n",
      "done 68200/95331\n",
      "done 68300/95331\n",
      "done 68400/95331\n",
      "done 68500/95331\n",
      "done 68600/95331\n",
      "done 68700/95331\n",
      "done 68800/95331\n",
      "done 68900/95331\n",
      "done 69000/95331\n",
      "done 69100/95331\n",
      "done 69200/95331\n",
      "done 69300/95331\n",
      "done 69400/95331\n",
      "done 69500/95331\n",
      "done 69600/95331\n",
      "done 69700/95331\n",
      "done 69800/95331\n",
      "done 69900/95331\n",
      "done 70000/95331\n",
      "done 70100/95331\n",
      "done 70200/95331\n",
      "done 70300/95331\n",
      "done 70400/95331\n",
      "done 70500/95331\n",
      "done 70600/95331\n",
      "done 70700/95331\n",
      "done 70800/95331\n",
      "done 70900/95331\n",
      "done 71000/95331\n",
      "done 71100/95331\n",
      "done 71200/95331\n",
      "done 71300/95331\n",
      "done 71400/95331\n",
      "done 71500/95331\n",
      "done 71600/95331\n",
      "done 71700/95331\n",
      "done 71800/95331\n",
      "done 71900/95331\n",
      "done 72000/95331\n",
      "done 72100/95331\n",
      "done 72200/95331\n",
      "done 72300/95331\n",
      "done 72400/95331\n",
      "done 72500/95331\n",
      "done 72600/95331\n",
      "done 72700/95331\n",
      "done 72800/95331\n",
      "done 72900/95331\n",
      "done 73000/95331\n",
      "done 73100/95331\n",
      "done 73200/95331\n",
      "done 73300/95331\n",
      "done 73400/95331\n",
      "done 73500/95331\n",
      "done 73600/95331\n",
      "done 73700/95331\n",
      "done 73800/95331\n",
      "done 73900/95331\n",
      "done 74000/95331\n",
      "done 74100/95331\n",
      "done 74200/95331\n",
      "done 74300/95331\n",
      "done 74400/95331\n",
      "done 74500/95331\n",
      "done 74600/95331\n",
      "done 74700/95331\n",
      "done 74800/95331\n",
      "done 74900/95331\n",
      "done 75000/95331\n",
      "done 75100/95331\n",
      "done 75200/95331\n",
      "done 75300/95331\n",
      "done 75400/95331\n",
      "done 75500/95331\n",
      "done 75600/95331\n",
      "done 75700/95331\n",
      "done 75800/95331\n",
      "done 75900/95331\n",
      "done 76000/95331\n",
      "done 76100/95331\n",
      "done 76200/95331\n",
      "done 76300/95331\n",
      "done 76400/95331\n",
      "done 76500/95331\n",
      "done 76600/95331\n",
      "done 76700/95331\n",
      "done 76800/95331\n",
      "done 76900/95331\n",
      "done 77000/95331\n",
      "done 77100/95331\n",
      "done 77200/95331\n",
      "done 77300/95331\n",
      "done 77400/95331\n",
      "done 77500/95331\n",
      "done 77600/95331\n",
      "done 77700/95331\n",
      "done 77800/95331\n",
      "done 77900/95331\n",
      "done 78000/95331\n",
      "done 78100/95331\n",
      "done 78200/95331\n",
      "done 78300/95331\n",
      "done 78400/95331\n",
      "done 78500/95331\n",
      "done 78600/95331\n",
      "done 78700/95331\n",
      "done 78800/95331\n",
      "done 78900/95331\n",
      "done 79000/95331\n",
      "done 79100/95331\n",
      "done 79200/95331\n",
      "done 79300/95331\n",
      "done 79400/95331\n",
      "done 79500/95331\n",
      "done 79600/95331\n",
      "done 79700/95331\n",
      "done 79800/95331\n",
      "done 79900/95331\n",
      "done 80000/95331\n",
      "done 80100/95331\n",
      "done 80200/95331\n",
      "done 80300/95331\n",
      "done 80400/95331\n",
      "done 80500/95331\n",
      "done 80600/95331\n",
      "done 80700/95331\n",
      "done 80800/95331\n",
      "done 80900/95331\n",
      "done 81000/95331\n",
      "done 81100/95331\n",
      "done 81200/95331\n",
      "done 81300/95331\n",
      "done 81400/95331\n",
      "done 81500/95331\n",
      "done 81600/95331\n",
      "done 81700/95331\n",
      "done 81800/95331\n",
      "done 81900/95331\n",
      "done 82000/95331\n",
      "done 82100/95331\n",
      "done 82200/95331\n",
      "done 82300/95331\n",
      "done 82400/95331\n",
      "done 82500/95331\n",
      "done 82600/95331\n",
      "done 82700/95331\n",
      "done 82800/95331\n",
      "done 82900/95331\n",
      "done 83000/95331\n",
      "done 83100/95331\n",
      "done 83200/95331\n",
      "done 83300/95331\n",
      "done 83400/95331\n",
      "done 83500/95331\n",
      "done 83600/95331\n",
      "done 83700/95331\n",
      "done 83800/95331\n",
      "done 83900/95331\n",
      "done 84000/95331\n",
      "done 84100/95331\n",
      "done 84200/95331\n",
      "done 84300/95331\n",
      "done 84400/95331\n",
      "done 84500/95331\n",
      "done 84600/95331\n",
      "done 84700/95331\n",
      "done 84800/95331\n",
      "done 84900/95331\n",
      "done 85000/95331\n",
      "done 85100/95331\n",
      "done 85200/95331\n",
      "done 85300/95331\n",
      "done 85400/95331\n",
      "done 85500/95331\n",
      "done 85600/95331\n",
      "done 85700/95331\n",
      "done 85800/95331\n",
      "done 85900/95331\n",
      "done 86000/95331\n",
      "done 86100/95331\n",
      "done 86200/95331\n",
      "done 86300/95331\n",
      "done 86400/95331\n",
      "done 86500/95331\n",
      "done 86600/95331\n",
      "done 86700/95331\n",
      "done 86800/95331\n",
      "done 86900/95331\n",
      "done 87000/95331\n",
      "done 87100/95331\n",
      "done 87200/95331\n",
      "done 87300/95331\n",
      "done 87400/95331\n",
      "done 87500/95331\n",
      "done 87600/95331\n",
      "done 87700/95331\n",
      "done 87800/95331\n",
      "done 87900/95331\n",
      "done 88000/95331\n",
      "done 88100/95331\n",
      "done 88200/95331\n",
      "done 88300/95331\n",
      "done 88400/95331\n",
      "done 88500/95331\n",
      "done 88600/95331\n",
      "done 88700/95331\n",
      "done 88800/95331\n",
      "done 88900/95331\n",
      "done 89000/95331\n",
      "done 89100/95331\n",
      "done 89200/95331\n",
      "done 89300/95331\n",
      "done 89400/95331\n",
      "done 89500/95331\n",
      "done 89600/95331\n",
      "done 89700/95331\n",
      "done 89800/95331\n",
      "done 89900/95331\n",
      "done 90000/95331\n",
      "done 90100/95331\n",
      "done 90200/95331\n",
      "done 90300/95331\n",
      "done 90400/95331\n",
      "done 90500/95331\n",
      "done 90600/95331\n",
      "done 90700/95331\n",
      "done 90800/95331\n",
      "done 90900/95331\n",
      "done 91000/95331\n",
      "done 91100/95331\n",
      "done 91200/95331\n",
      "done 91300/95331\n",
      "done 91400/95331\n",
      "done 91500/95331\n",
      "done 91600/95331\n",
      "done 91700/95331\n",
      "done 91800/95331\n",
      "done 91900/95331\n",
      "done 92000/95331\n",
      "done 92100/95331\n",
      "done 92200/95331\n",
      "done 92300/95331\n",
      "done 92400/95331\n",
      "done 92500/95331\n",
      "done 92600/95331\n",
      "done 92700/95331\n",
      "done 92800/95331\n",
      "done 92900/95331\n",
      "done 93000/95331\n",
      "done 93100/95331\n",
      "done 93200/95331\n",
      "done 93300/95331\n",
      "done 93400/95331\n",
      "done 93500/95331\n",
      "done 93600/95331\n",
      "done 93700/95331\n",
      "done 93800/95331\n",
      "done 93900/95331\n",
      "done 94000/95331\n",
      "done 94100/95331\n",
      "done 94200/95331\n",
      "done 94300/95331\n",
      "done 94400/95331\n",
      "done 94500/95331\n",
      "done 94600/95331\n",
      "done 94700/95331\n",
      "done 94800/95331\n",
      "done 94900/95331\n",
      "done 95000/95331\n",
      "done 95100/95331\n",
      "done 95200/95331\n",
      "done 95300/95331\n",
      "counter len: 254647\n",
      "done 2nd loop 0/254647\n",
      "done 2nd loop 100/254647\n",
      "done 2nd loop 200/254647\n",
      "done 2nd loop 300/254647\n",
      "done 2nd loop 400/254647\n",
      "done 2nd loop 500/254647\n",
      "done 2nd loop 600/254647\n",
      "done 2nd loop 700/254647\n",
      "done 2nd loop 800/254647\n",
      "done 2nd loop 900/254647\n",
      "done 2nd loop 1000/254647\n",
      "done 2nd loop 1100/254647\n",
      "done 2nd loop 1200/254647\n",
      "done 2nd loop 1300/254647\n",
      "done 2nd loop 1400/254647\n",
      "done 2nd loop 1500/254647\n",
      "done 2nd loop 1600/254647\n",
      "done 2nd loop 1700/254647\n",
      "done 2nd loop 1800/254647\n",
      "done 2nd loop 1900/254647\n",
      "done 2nd loop 2000/254647\n",
      "done 2nd loop 2100/254647\n",
      "done 2nd loop 2200/254647\n",
      "done 2nd loop 2300/254647\n",
      "done 2nd loop 2400/254647\n",
      "done 2nd loop 2500/254647\n",
      "done 2nd loop 2600/254647\n",
      "done 2nd loop 2700/254647\n",
      "done 2nd loop 2800/254647\n",
      "done 2nd loop 2900/254647\n",
      "done 2nd loop 3000/254647\n",
      "done 2nd loop 3100/254647\n",
      "done 2nd loop 3200/254647\n",
      "done 2nd loop 3300/254647\n",
      "done 2nd loop 3400/254647\n",
      "done 2nd loop 3500/254647\n",
      "done 2nd loop 3600/254647\n",
      "done 2nd loop 3700/254647\n",
      "done 2nd loop 3800/254647\n",
      "done 2nd loop 3900/254647\n",
      "done 2nd loop 4000/254647\n",
      "done 2nd loop 4100/254647\n",
      "done 2nd loop 4200/254647\n",
      "done 2nd loop 4300/254647\n",
      "done 2nd loop 4400/254647\n",
      "done 2nd loop 4500/254647\n",
      "done 2nd loop 4600/254647\n",
      "done 2nd loop 4700/254647\n",
      "done 2nd loop 4800/254647\n",
      "done 2nd loop 4900/254647\n",
      "done 2nd loop 5000/254647\n",
      "done 2nd loop 5100/254647\n",
      "done 2nd loop 5200/254647\n",
      "done 2nd loop 5300/254647\n",
      "done 2nd loop 5400/254647\n",
      "done 2nd loop 5500/254647\n",
      "done 2nd loop 5600/254647\n",
      "done 2nd loop 5700/254647\n",
      "done 2nd loop 5800/254647\n",
      "done 2nd loop 5900/254647\n",
      "done 2nd loop 6000/254647\n",
      "done 2nd loop 6100/254647\n",
      "done 2nd loop 6200/254647\n",
      "done 2nd loop 6300/254647\n",
      "done 2nd loop 6400/254647\n",
      "done 2nd loop 6500/254647\n",
      "done 2nd loop 6600/254647\n",
      "done 2nd loop 6700/254647\n",
      "done 2nd loop 6800/254647\n",
      "done 2nd loop 6900/254647\n",
      "done 2nd loop 7000/254647\n",
      "done 2nd loop 7100/254647\n",
      "done 2nd loop 7200/254647\n",
      "done 2nd loop 7300/254647\n",
      "done 2nd loop 7400/254647\n",
      "done 2nd loop 7500/254647\n",
      "done 2nd loop 7600/254647\n",
      "done 2nd loop 7700/254647\n",
      "done 2nd loop 7800/254647\n",
      "done 2nd loop 7900/254647\n",
      "done 2nd loop 8000/254647\n",
      "done 2nd loop 8100/254647\n",
      "done 2nd loop 8200/254647\n",
      "done 2nd loop 8300/254647\n",
      "done 2nd loop 8400/254647\n",
      "done 2nd loop 8500/254647\n",
      "done 2nd loop 8600/254647\n",
      "done 2nd loop 8700/254647\n",
      "done 2nd loop 8800/254647\n",
      "done 2nd loop 8900/254647\n",
      "done 2nd loop 9000/254647\n",
      "done 2nd loop 9100/254647\n",
      "done 2nd loop 9200/254647\n",
      "done 2nd loop 9300/254647\n",
      "done 2nd loop 9400/254647\n",
      "done 2nd loop 9500/254647\n",
      "done 2nd loop 9600/254647\n",
      "done 2nd loop 9700/254647\n",
      "done 2nd loop 9800/254647\n",
      "done 2nd loop 9900/254647\n",
      "done 2nd loop 10000/254647\n",
      "done 2nd loop 10100/254647\n",
      "done 2nd loop 10200/254647\n",
      "done 2nd loop 10300/254647\n",
      "done 2nd loop 10400/254647\n",
      "done 2nd loop 10500/254647\n",
      "done 2nd loop 10600/254647\n",
      "done 2nd loop 10700/254647\n",
      "done 2nd loop 10800/254647\n",
      "done 2nd loop 10900/254647\n",
      "done 2nd loop 11000/254647\n",
      "done 2nd loop 11100/254647\n",
      "done 2nd loop 11200/254647\n",
      "done 2nd loop 11300/254647\n",
      "done 2nd loop 11400/254647\n",
      "done 2nd loop 11500/254647\n",
      "done 2nd loop 11600/254647\n",
      "done 2nd loop 11700/254647\n",
      "done 2nd loop 11800/254647\n",
      "done 2nd loop 11900/254647\n",
      "done 2nd loop 12000/254647\n",
      "done 2nd loop 12100/254647\n",
      "done 2nd loop 12200/254647\n",
      "done 2nd loop 12300/254647\n",
      "done 2nd loop 12400/254647\n",
      "done 2nd loop 12500/254647\n",
      "done 2nd loop 12600/254647\n",
      "done 2nd loop 12700/254647\n",
      "done 2nd loop 12800/254647\n",
      "done 2nd loop 12900/254647\n",
      "done 2nd loop 13000/254647\n",
      "done 2nd loop 13100/254647\n",
      "done 2nd loop 13200/254647\n",
      "done 2nd loop 13300/254647\n",
      "done 2nd loop 13400/254647\n",
      "done 2nd loop 13500/254647\n",
      "done 2nd loop 13600/254647\n",
      "done 2nd loop 13700/254647\n",
      "done 2nd loop 13800/254647\n",
      "done 2nd loop 13900/254647\n",
      "done 2nd loop 14000/254647\n",
      "done 2nd loop 14100/254647\n",
      "done 2nd loop 14200/254647\n",
      "done 2nd loop 14300/254647\n",
      "done 2nd loop 14400/254647\n",
      "done 2nd loop 14500/254647\n",
      "done 2nd loop 14600/254647\n",
      "done 2nd loop 14700/254647\n",
      "done 2nd loop 14800/254647\n",
      "done 2nd loop 14900/254647\n",
      "done 2nd loop 15000/254647\n",
      "done 2nd loop 15100/254647\n",
      "done 2nd loop 15200/254647\n",
      "done 2nd loop 15300/254647\n",
      "done 2nd loop 15400/254647\n",
      "done 2nd loop 15500/254647\n",
      "done 2nd loop 15600/254647\n",
      "done 2nd loop 15700/254647\n",
      "done 2nd loop 15800/254647\n",
      "done 2nd loop 15900/254647\n",
      "done 2nd loop 16000/254647\n",
      "done 2nd loop 16100/254647\n",
      "done 2nd loop 16200/254647\n",
      "done 2nd loop 16300/254647\n",
      "done 2nd loop 16400/254647\n",
      "done 2nd loop 16500/254647\n",
      "done 2nd loop 16600/254647\n",
      "done 2nd loop 16700/254647\n",
      "done 2nd loop 16800/254647\n",
      "done 2nd loop 16900/254647\n",
      "done 2nd loop 17000/254647\n",
      "done 2nd loop 17100/254647\n",
      "done 2nd loop 17200/254647\n",
      "done 2nd loop 17300/254647\n",
      "done 2nd loop 17400/254647\n",
      "done 2nd loop 17500/254647\n",
      "done 2nd loop 17600/254647\n",
      "done 2nd loop 17700/254647\n",
      "done 2nd loop 17800/254647\n",
      "done 2nd loop 17900/254647\n",
      "done 2nd loop 18000/254647\n",
      "done 2nd loop 18100/254647\n",
      "done 2nd loop 18200/254647\n",
      "done 2nd loop 18300/254647\n",
      "done 2nd loop 18400/254647\n",
      "done 2nd loop 18500/254647\n",
      "done 2nd loop 18600/254647\n",
      "done 2nd loop 18700/254647\n",
      "done 2nd loop 18800/254647\n",
      "done 2nd loop 18900/254647\n",
      "done 2nd loop 19000/254647\n",
      "done 2nd loop 19100/254647\n",
      "done 2nd loop 19200/254647\n",
      "done 2nd loop 19300/254647\n",
      "done 2nd loop 19400/254647\n",
      "done 2nd loop 19500/254647\n",
      "done 2nd loop 19600/254647\n",
      "done 2nd loop 19700/254647\n",
      "done 2nd loop 19800/254647\n",
      "done 2nd loop 19900/254647\n",
      "done 2nd loop 20000/254647\n",
      "done 2nd loop 20100/254647\n",
      "done 2nd loop 20200/254647\n",
      "done 2nd loop 20300/254647\n",
      "done 2nd loop 20400/254647\n",
      "done 2nd loop 20500/254647\n",
      "done 2nd loop 20600/254647\n",
      "done 2nd loop 20700/254647\n",
      "done 2nd loop 20800/254647\n",
      "done 2nd loop 20900/254647\n",
      "done 2nd loop 21000/254647\n",
      "done 2nd loop 21100/254647\n",
      "done 2nd loop 21200/254647\n",
      "done 2nd loop 21300/254647\n",
      "done 2nd loop 21400/254647\n",
      "done 2nd loop 21500/254647\n",
      "done 2nd loop 21600/254647\n",
      "done 2nd loop 21700/254647\n",
      "done 2nd loop 21800/254647\n",
      "done 2nd loop 21900/254647\n",
      "done 2nd loop 22000/254647\n",
      "done 2nd loop 22100/254647\n",
      "done 2nd loop 22200/254647\n",
      "done 2nd loop 22300/254647\n",
      "done 2nd loop 22400/254647\n",
      "done 2nd loop 22500/254647\n",
      "done 2nd loop 22600/254647\n",
      "done 2nd loop 22700/254647\n",
      "done 2nd loop 22800/254647\n",
      "done 2nd loop 22900/254647\n",
      "done 2nd loop 23000/254647\n",
      "done 2nd loop 23100/254647\n",
      "done 2nd loop 23200/254647\n",
      "done 2nd loop 23300/254647\n",
      "done 2nd loop 23400/254647\n",
      "done 2nd loop 23500/254647\n",
      "done 2nd loop 23600/254647\n",
      "done 2nd loop 23700/254647\n",
      "done 2nd loop 23800/254647\n",
      "done 2nd loop 23900/254647\n",
      "done 2nd loop 24000/254647\n",
      "done 2nd loop 24100/254647\n",
      "done 2nd loop 24200/254647\n",
      "done 2nd loop 24300/254647\n",
      "done 2nd loop 24400/254647\n",
      "done 2nd loop 24500/254647\n",
      "done 2nd loop 24600/254647\n",
      "done 2nd loop 24700/254647\n",
      "done 2nd loop 24800/254647\n",
      "done 2nd loop 24900/254647\n",
      "done 2nd loop 25000/254647\n",
      "done 2nd loop 25100/254647\n",
      "done 2nd loop 25200/254647\n",
      "done 2nd loop 25300/254647\n",
      "done 2nd loop 25400/254647\n",
      "done 2nd loop 25500/254647\n",
      "done 2nd loop 25600/254647\n",
      "done 2nd loop 25700/254647\n",
      "done 2nd loop 25800/254647\n",
      "done 2nd loop 25900/254647\n",
      "done 2nd loop 26000/254647\n",
      "done 2nd loop 26100/254647\n",
      "done 2nd loop 26200/254647\n",
      "done 2nd loop 26300/254647\n",
      "done 2nd loop 26400/254647\n",
      "done 2nd loop 26500/254647\n",
      "done 2nd loop 26600/254647\n",
      "done 2nd loop 26700/254647\n",
      "done 2nd loop 26800/254647\n",
      "done 2nd loop 26900/254647\n",
      "done 2nd loop 27000/254647\n",
      "done 2nd loop 27100/254647\n",
      "done 2nd loop 27200/254647\n",
      "done 2nd loop 27300/254647\n",
      "done 2nd loop 27400/254647\n",
      "done 2nd loop 27500/254647\n",
      "done 2nd loop 27600/254647\n",
      "done 2nd loop 27700/254647\n",
      "done 2nd loop 27800/254647\n",
      "done 2nd loop 27900/254647\n",
      "done 2nd loop 28000/254647\n",
      "done 2nd loop 28100/254647\n",
      "done 2nd loop 28200/254647\n",
      "done 2nd loop 28300/254647\n",
      "done 2nd loop 28400/254647\n",
      "done 2nd loop 28500/254647\n",
      "done 2nd loop 28600/254647\n",
      "done 2nd loop 28700/254647\n",
      "done 2nd loop 28800/254647\n",
      "done 2nd loop 28900/254647\n",
      "done 2nd loop 29000/254647\n",
      "done 2nd loop 29100/254647\n",
      "done 2nd loop 29200/254647\n",
      "done 2nd loop 29300/254647\n",
      "done 2nd loop 29400/254647\n",
      "done 2nd loop 29500/254647\n",
      "done 2nd loop 29600/254647\n",
      "done 2nd loop 29700/254647\n",
      "done 2nd loop 29800/254647\n",
      "done 2nd loop 29900/254647\n",
      "done 2nd loop 30000/254647\n",
      "done 2nd loop 30100/254647\n",
      "done 2nd loop 30200/254647\n",
      "done 2nd loop 30300/254647\n",
      "done 2nd loop 30400/254647\n",
      "done 2nd loop 30500/254647\n",
      "done 2nd loop 30600/254647\n",
      "done 2nd loop 30700/254647\n",
      "done 2nd loop 30800/254647\n",
      "done 2nd loop 30900/254647\n",
      "done 2nd loop 31000/254647\n",
      "done 2nd loop 31100/254647\n",
      "done 2nd loop 31200/254647\n",
      "done 2nd loop 31300/254647\n",
      "done 2nd loop 31400/254647\n",
      "done 2nd loop 31500/254647\n",
      "done 2nd loop 31600/254647\n",
      "done 2nd loop 31700/254647\n",
      "done 2nd loop 31800/254647\n",
      "done 2nd loop 31900/254647\n",
      "done 2nd loop 32000/254647\n",
      "done 2nd loop 32100/254647\n",
      "done 2nd loop 32200/254647\n",
      "done 2nd loop 32300/254647\n",
      "done 2nd loop 32400/254647\n",
      "done 2nd loop 32500/254647\n",
      "done 2nd loop 32600/254647\n",
      "done 2nd loop 32700/254647\n",
      "done 2nd loop 32800/254647\n",
      "done 2nd loop 32900/254647\n",
      "done 2nd loop 33000/254647\n",
      "done 2nd loop 33100/254647\n",
      "done 2nd loop 33200/254647\n",
      "done 2nd loop 33300/254647\n",
      "done 2nd loop 33400/254647\n",
      "done 2nd loop 33500/254647\n",
      "done 2nd loop 33600/254647\n",
      "done 2nd loop 33700/254647\n",
      "done 2nd loop 33800/254647\n",
      "done 2nd loop 33900/254647\n",
      "done 2nd loop 34000/254647\n",
      "done 2nd loop 34100/254647\n",
      "done 2nd loop 34200/254647\n",
      "done 2nd loop 34300/254647\n",
      "done 2nd loop 34400/254647\n",
      "done 2nd loop 34500/254647\n",
      "done 2nd loop 34600/254647\n",
      "done 2nd loop 34700/254647\n",
      "done 2nd loop 34800/254647\n",
      "done 2nd loop 34900/254647\n",
      "done 2nd loop 35000/254647\n",
      "done 2nd loop 35100/254647\n",
      "done 2nd loop 35200/254647\n",
      "done 2nd loop 35300/254647\n",
      "done 2nd loop 35400/254647\n",
      "done 2nd loop 35500/254647\n",
      "done 2nd loop 35600/254647\n",
      "done 2nd loop 35700/254647\n",
      "done 2nd loop 35800/254647\n",
      "done 2nd loop 35900/254647\n",
      "done 2nd loop 36000/254647\n",
      "done 2nd loop 36100/254647\n",
      "done 2nd loop 36200/254647\n",
      "done 2nd loop 36300/254647\n",
      "done 2nd loop 36400/254647\n",
      "done 2nd loop 36500/254647\n",
      "done 2nd loop 36600/254647\n",
      "done 2nd loop 36700/254647\n",
      "done 2nd loop 36800/254647\n",
      "done 2nd loop 36900/254647\n",
      "done 2nd loop 37000/254647\n",
      "done 2nd loop 37100/254647\n",
      "done 2nd loop 37200/254647\n",
      "done 2nd loop 37300/254647\n",
      "done 2nd loop 37400/254647\n",
      "done 2nd loop 37500/254647\n",
      "done 2nd loop 37600/254647\n",
      "done 2nd loop 37700/254647\n",
      "done 2nd loop 37800/254647\n",
      "done 2nd loop 37900/254647\n",
      "done 2nd loop 38000/254647\n",
      "done 2nd loop 38100/254647\n",
      "done 2nd loop 38200/254647\n",
      "done 2nd loop 38300/254647\n",
      "done 2nd loop 38400/254647\n",
      "done 2nd loop 38500/254647\n",
      "done 2nd loop 38600/254647\n",
      "done 2nd loop 38700/254647\n",
      "done 2nd loop 38800/254647\n",
      "done 2nd loop 38900/254647\n",
      "done 2nd loop 39000/254647\n",
      "done 2nd loop 39100/254647\n",
      "done 2nd loop 39200/254647\n",
      "done 2nd loop 39300/254647\n",
      "done 2nd loop 39400/254647\n",
      "done 2nd loop 39500/254647\n",
      "done 2nd loop 39600/254647\n",
      "done 2nd loop 39700/254647\n",
      "done 2nd loop 39800/254647\n",
      "done 2nd loop 39900/254647\n",
      "done 2nd loop 40000/254647\n",
      "done 2nd loop 40100/254647\n",
      "done 2nd loop 40200/254647\n",
      "done 2nd loop 40300/254647\n",
      "done 2nd loop 40400/254647\n",
      "done 2nd loop 40500/254647\n",
      "done 2nd loop 40600/254647\n",
      "done 2nd loop 40700/254647\n",
      "done 2nd loop 40800/254647\n",
      "done 2nd loop 40900/254647\n",
      "done 2nd loop 41000/254647\n",
      "done 2nd loop 41100/254647\n",
      "done 2nd loop 41200/254647\n",
      "done 2nd loop 41300/254647\n",
      "done 2nd loop 41400/254647\n",
      "done 2nd loop 41500/254647\n",
      "done 2nd loop 41600/254647\n",
      "done 2nd loop 41700/254647\n",
      "done 2nd loop 41800/254647\n",
      "done 2nd loop 41900/254647\n",
      "done 2nd loop 42000/254647\n",
      "done 2nd loop 42100/254647\n",
      "done 2nd loop 42200/254647\n",
      "done 2nd loop 42300/254647\n",
      "done 2nd loop 42400/254647\n",
      "done 2nd loop 42500/254647\n",
      "done 2nd loop 42600/254647\n",
      "done 2nd loop 42700/254647\n",
      "done 2nd loop 42800/254647\n",
      "done 2nd loop 42900/254647\n",
      "done 2nd loop 43000/254647\n",
      "done 2nd loop 43100/254647\n",
      "done 2nd loop 43200/254647\n",
      "done 2nd loop 43300/254647\n",
      "done 2nd loop 43400/254647\n",
      "done 2nd loop 43500/254647\n",
      "done 2nd loop 43600/254647\n",
      "done 2nd loop 43700/254647\n",
      "done 2nd loop 43800/254647\n",
      "done 2nd loop 43900/254647\n",
      "done 2nd loop 44000/254647\n",
      "done 2nd loop 44100/254647\n",
      "done 2nd loop 44200/254647\n",
      "done 2nd loop 44300/254647\n",
      "done 2nd loop 44400/254647\n",
      "done 2nd loop 44500/254647\n",
      "done 2nd loop 44600/254647\n",
      "done 2nd loop 44700/254647\n",
      "done 2nd loop 44800/254647\n",
      "done 2nd loop 44900/254647\n",
      "done 2nd loop 45000/254647\n",
      "done 2nd loop 45100/254647\n",
      "done 2nd loop 45200/254647\n",
      "done 2nd loop 45300/254647\n",
      "done 2nd loop 45400/254647\n",
      "done 2nd loop 45500/254647\n",
      "done 2nd loop 45600/254647\n",
      "done 2nd loop 45700/254647\n",
      "done 2nd loop 45800/254647\n",
      "done 2nd loop 45900/254647\n",
      "done 2nd loop 46000/254647\n",
      "done 2nd loop 46100/254647\n",
      "done 2nd loop 46200/254647\n",
      "done 2nd loop 46300/254647\n",
      "done 2nd loop 46400/254647\n",
      "done 2nd loop 46500/254647\n",
      "done 2nd loop 46600/254647\n",
      "done 2nd loop 46700/254647\n",
      "done 2nd loop 46800/254647\n",
      "done 2nd loop 46900/254647\n",
      "done 2nd loop 47000/254647\n",
      "done 2nd loop 47100/254647\n",
      "done 2nd loop 47200/254647\n",
      "done 2nd loop 47300/254647\n",
      "done 2nd loop 47400/254647\n",
      "done 2nd loop 47500/254647\n",
      "done 2nd loop 47600/254647\n",
      "done 2nd loop 47700/254647\n",
      "done 2nd loop 47800/254647\n",
      "done 2nd loop 47900/254647\n",
      "done 2nd loop 48000/254647\n",
      "done 2nd loop 48100/254647\n",
      "done 2nd loop 48200/254647\n",
      "done 2nd loop 48300/254647\n",
      "done 2nd loop 48400/254647\n",
      "done 2nd loop 48500/254647\n",
      "done 2nd loop 48600/254647\n",
      "done 2nd loop 48700/254647\n",
      "done 2nd loop 48800/254647\n",
      "done 2nd loop 48900/254647\n",
      "done 2nd loop 49000/254647\n",
      "done 2nd loop 49100/254647\n",
      "done 2nd loop 49200/254647\n",
      "done 2nd loop 49300/254647\n",
      "done 2nd loop 49400/254647\n",
      "done 2nd loop 49500/254647\n",
      "done 2nd loop 49600/254647\n",
      "done 2nd loop 49700/254647\n",
      "done 2nd loop 49800/254647\n",
      "done 2nd loop 49900/254647\n",
      "done 2nd loop 50000/254647\n",
      "done 2nd loop 50100/254647\n",
      "done 2nd loop 50200/254647\n",
      "done 2nd loop 50300/254647\n",
      "done 2nd loop 50400/254647\n",
      "done 2nd loop 50500/254647\n",
      "done 2nd loop 50600/254647\n",
      "done 2nd loop 50700/254647\n",
      "done 2nd loop 50800/254647\n",
      "done 2nd loop 50900/254647\n",
      "done 2nd loop 51000/254647\n",
      "done 2nd loop 51100/254647\n",
      "done 2nd loop 51200/254647\n",
      "done 2nd loop 51300/254647\n",
      "done 2nd loop 51400/254647\n",
      "done 2nd loop 51500/254647\n",
      "done 2nd loop 51600/254647\n",
      "done 2nd loop 51700/254647\n",
      "done 2nd loop 51800/254647\n",
      "done 2nd loop 51900/254647\n",
      "done 2nd loop 52000/254647\n",
      "done 2nd loop 52100/254647\n",
      "done 2nd loop 52200/254647\n",
      "done 2nd loop 52300/254647\n",
      "done 2nd loop 52400/254647\n",
      "done 2nd loop 52500/254647\n",
      "done 2nd loop 52600/254647\n",
      "done 2nd loop 52700/254647\n",
      "done 2nd loop 52800/254647\n",
      "done 2nd loop 52900/254647\n",
      "done 2nd loop 53000/254647\n",
      "done 2nd loop 53100/254647\n",
      "done 2nd loop 53200/254647\n",
      "done 2nd loop 53300/254647\n",
      "done 2nd loop 53400/254647\n",
      "done 2nd loop 53500/254647\n",
      "done 2nd loop 53600/254647\n",
      "done 2nd loop 53700/254647\n",
      "done 2nd loop 53800/254647\n",
      "done 2nd loop 53900/254647\n",
      "done 2nd loop 54000/254647\n",
      "done 2nd loop 54100/254647\n",
      "done 2nd loop 54200/254647\n",
      "done 2nd loop 54300/254647\n",
      "done 2nd loop 54400/254647\n",
      "done 2nd loop 54500/254647\n",
      "done 2nd loop 54600/254647\n",
      "done 2nd loop 54700/254647\n",
      "done 2nd loop 54800/254647\n",
      "done 2nd loop 54900/254647\n",
      "done 2nd loop 55000/254647\n",
      "done 2nd loop 55100/254647\n",
      "done 2nd loop 55200/254647\n",
      "done 2nd loop 55300/254647\n",
      "done 2nd loop 55400/254647\n",
      "done 2nd loop 55500/254647\n",
      "done 2nd loop 55600/254647\n",
      "done 2nd loop 55700/254647\n",
      "done 2nd loop 55800/254647\n",
      "done 2nd loop 55900/254647\n",
      "done 2nd loop 56000/254647\n",
      "done 2nd loop 56100/254647\n",
      "done 2nd loop 56200/254647\n",
      "done 2nd loop 56300/254647\n",
      "done 2nd loop 56400/254647\n",
      "done 2nd loop 56500/254647\n",
      "done 2nd loop 56600/254647\n",
      "done 2nd loop 56700/254647\n",
      "done 2nd loop 56800/254647\n",
      "done 2nd loop 56900/254647\n",
      "done 2nd loop 57000/254647\n",
      "done 2nd loop 57100/254647\n",
      "done 2nd loop 57200/254647\n",
      "done 2nd loop 57300/254647\n",
      "done 2nd loop 57400/254647\n",
      "done 2nd loop 57500/254647\n",
      "done 2nd loop 57600/254647\n",
      "done 2nd loop 57700/254647\n",
      "done 2nd loop 57800/254647\n",
      "done 2nd loop 57900/254647\n",
      "done 2nd loop 58000/254647\n",
      "done 2nd loop 58100/254647\n",
      "done 2nd loop 58200/254647\n",
      "done 2nd loop 58300/254647\n",
      "done 2nd loop 58400/254647\n",
      "done 2nd loop 58500/254647\n",
      "done 2nd loop 58600/254647\n",
      "done 2nd loop 58700/254647\n",
      "done 2nd loop 58800/254647\n",
      "done 2nd loop 58900/254647\n",
      "done 2nd loop 59000/254647\n",
      "done 2nd loop 59100/254647\n",
      "done 2nd loop 59200/254647\n",
      "done 2nd loop 59300/254647\n",
      "done 2nd loop 59400/254647\n",
      "done 2nd loop 59500/254647\n",
      "done 2nd loop 59600/254647\n",
      "done 2nd loop 59700/254647\n",
      "done 2nd loop 59800/254647\n",
      "done 2nd loop 59900/254647\n",
      "done 2nd loop 60000/254647\n",
      "done 2nd loop 60100/254647\n",
      "done 2nd loop 60200/254647\n",
      "done 2nd loop 60300/254647\n",
      "done 2nd loop 60400/254647\n",
      "done 2nd loop 60500/254647\n",
      "done 2nd loop 60600/254647\n",
      "done 2nd loop 60700/254647\n",
      "done 2nd loop 60800/254647\n",
      "done 2nd loop 60900/254647\n",
      "done 2nd loop 61000/254647\n",
      "done 2nd loop 61100/254647\n",
      "done 2nd loop 61200/254647\n",
      "done 2nd loop 61300/254647\n",
      "done 2nd loop 61400/254647\n",
      "done 2nd loop 61500/254647\n",
      "done 2nd loop 61600/254647\n",
      "done 2nd loop 61700/254647\n",
      "done 2nd loop 61800/254647\n",
      "done 2nd loop 61900/254647\n",
      "done 2nd loop 62000/254647\n",
      "done 2nd loop 62100/254647\n",
      "done 2nd loop 62200/254647\n",
      "done 2nd loop 62300/254647\n",
      "done 2nd loop 62400/254647\n",
      "done 2nd loop 62500/254647\n",
      "done 2nd loop 62600/254647\n",
      "done 2nd loop 62700/254647\n",
      "done 2nd loop 62800/254647\n",
      "done 2nd loop 62900/254647\n",
      "done 2nd loop 63000/254647\n",
      "done 2nd loop 63100/254647\n",
      "done 2nd loop 63200/254647\n",
      "done 2nd loop 63300/254647\n",
      "done 2nd loop 63400/254647\n",
      "done 2nd loop 63500/254647\n",
      "done 2nd loop 63600/254647\n",
      "done 2nd loop 63700/254647\n",
      "done 2nd loop 63800/254647\n",
      "done 2nd loop 63900/254647\n",
      "done 2nd loop 64000/254647\n",
      "done 2nd loop 64100/254647\n",
      "done 2nd loop 64200/254647\n",
      "done 2nd loop 64300/254647\n",
      "done 2nd loop 64400/254647\n",
      "done 2nd loop 64500/254647\n",
      "done 2nd loop 64600/254647\n",
      "done 2nd loop 64700/254647\n",
      "done 2nd loop 64800/254647\n",
      "done 2nd loop 64900/254647\n",
      "done 2nd loop 65000/254647\n",
      "done 2nd loop 65100/254647\n",
      "done 2nd loop 65200/254647\n",
      "done 2nd loop 65300/254647\n",
      "done 2nd loop 65400/254647\n",
      "done 2nd loop 65500/254647\n",
      "done 2nd loop 65600/254647\n",
      "done 2nd loop 65700/254647\n",
      "done 2nd loop 65800/254647\n",
      "done 2nd loop 65900/254647\n",
      "done 2nd loop 66000/254647\n",
      "done 2nd loop 66100/254647\n",
      "done 2nd loop 66200/254647\n",
      "done 2nd loop 66300/254647\n",
      "done 2nd loop 66400/254647\n",
      "done 2nd loop 66500/254647\n",
      "done 2nd loop 66600/254647\n",
      "done 2nd loop 66700/254647\n",
      "done 2nd loop 66800/254647\n",
      "done 2nd loop 66900/254647\n",
      "done 2nd loop 67000/254647\n",
      "done 2nd loop 67100/254647\n",
      "done 2nd loop 67200/254647\n",
      "done 2nd loop 67300/254647\n",
      "done 2nd loop 67400/254647\n",
      "done 2nd loop 67500/254647\n",
      "done 2nd loop 67600/254647\n",
      "done 2nd loop 67700/254647\n",
      "done 2nd loop 67800/254647\n",
      "done 2nd loop 67900/254647\n",
      "done 2nd loop 68000/254647\n",
      "done 2nd loop 68100/254647\n",
      "done 2nd loop 68200/254647\n",
      "done 2nd loop 68300/254647\n",
      "done 2nd loop 68400/254647\n",
      "done 2nd loop 68500/254647\n",
      "done 2nd loop 68600/254647\n",
      "done 2nd loop 68700/254647\n",
      "done 2nd loop 68800/254647\n",
      "done 2nd loop 68900/254647\n",
      "done 2nd loop 69000/254647\n",
      "done 2nd loop 69100/254647\n",
      "done 2nd loop 69200/254647\n",
      "done 2nd loop 69300/254647\n",
      "done 2nd loop 69400/254647\n",
      "done 2nd loop 69500/254647\n",
      "done 2nd loop 69600/254647\n",
      "done 2nd loop 69700/254647\n",
      "done 2nd loop 69800/254647\n",
      "done 2nd loop 69900/254647\n",
      "done 2nd loop 70000/254647\n",
      "done 2nd loop 70100/254647\n",
      "done 2nd loop 70200/254647\n",
      "done 2nd loop 70300/254647\n",
      "done 2nd loop 70400/254647\n",
      "done 2nd loop 70500/254647\n",
      "done 2nd loop 70600/254647\n",
      "done 2nd loop 70700/254647\n",
      "done 2nd loop 70800/254647\n",
      "done 2nd loop 70900/254647\n",
      "done 2nd loop 71000/254647\n",
      "done 2nd loop 71100/254647\n",
      "done 2nd loop 71200/254647\n",
      "done 2nd loop 71300/254647\n",
      "done 2nd loop 71400/254647\n",
      "done 2nd loop 71500/254647\n",
      "done 2nd loop 71600/254647\n",
      "done 2nd loop 71700/254647\n",
      "done 2nd loop 71800/254647\n",
      "done 2nd loop 71900/254647\n",
      "done 2nd loop 72000/254647\n",
      "done 2nd loop 72100/254647\n",
      "done 2nd loop 72200/254647\n",
      "done 2nd loop 72300/254647\n",
      "done 2nd loop 72400/254647\n",
      "done 2nd loop 72500/254647\n",
      "done 2nd loop 72600/254647\n",
      "done 2nd loop 72700/254647\n",
      "done 2nd loop 72800/254647\n",
      "done 2nd loop 72900/254647\n",
      "done 2nd loop 73000/254647\n",
      "done 2nd loop 73100/254647\n",
      "done 2nd loop 73200/254647\n",
      "done 2nd loop 73300/254647\n",
      "done 2nd loop 73400/254647\n",
      "done 2nd loop 73500/254647\n",
      "done 2nd loop 73600/254647\n",
      "done 2nd loop 73700/254647\n",
      "done 2nd loop 73800/254647\n",
      "done 2nd loop 73900/254647\n",
      "done 2nd loop 74000/254647\n",
      "done 2nd loop 74100/254647\n",
      "done 2nd loop 74200/254647\n",
      "done 2nd loop 74300/254647\n",
      "done 2nd loop 74400/254647\n",
      "done 2nd loop 74500/254647\n",
      "done 2nd loop 74600/254647\n",
      "done 2nd loop 74700/254647\n",
      "done 2nd loop 74800/254647\n",
      "done 2nd loop 74900/254647\n",
      "done 2nd loop 75000/254647\n",
      "done 2nd loop 75100/254647\n",
      "done 2nd loop 75200/254647\n",
      "done 2nd loop 75300/254647\n",
      "done 2nd loop 75400/254647\n",
      "done 2nd loop 75500/254647\n",
      "done 2nd loop 75600/254647\n",
      "done 2nd loop 75700/254647\n",
      "done 2nd loop 75800/254647\n",
      "done 2nd loop 75900/254647\n",
      "done 2nd loop 76000/254647\n",
      "done 2nd loop 76100/254647\n",
      "done 2nd loop 76200/254647\n",
      "done 2nd loop 76300/254647\n",
      "done 2nd loop 76400/254647\n",
      "done 2nd loop 76500/254647\n",
      "done 2nd loop 76600/254647\n",
      "done 2nd loop 76700/254647\n",
      "done 2nd loop 76800/254647\n",
      "done 2nd loop 76900/254647\n",
      "done 2nd loop 77000/254647\n",
      "done 2nd loop 77100/254647\n",
      "done 2nd loop 77200/254647\n",
      "done 2nd loop 77300/254647\n",
      "done 2nd loop 77400/254647\n",
      "done 2nd loop 77500/254647\n",
      "done 2nd loop 77600/254647\n",
      "done 2nd loop 77700/254647\n",
      "done 2nd loop 77800/254647\n",
      "done 2nd loop 77900/254647\n",
      "done 2nd loop 78000/254647\n",
      "done 2nd loop 78100/254647\n",
      "done 2nd loop 78200/254647\n",
      "done 2nd loop 78300/254647\n",
      "done 2nd loop 78400/254647\n",
      "done 2nd loop 78500/254647\n",
      "done 2nd loop 78600/254647\n",
      "done 2nd loop 78700/254647\n",
      "done 2nd loop 78800/254647\n",
      "done 2nd loop 78900/254647\n",
      "done 2nd loop 79000/254647\n",
      "done 2nd loop 79100/254647\n",
      "done 2nd loop 79200/254647\n",
      "done 2nd loop 79300/254647\n",
      "done 2nd loop 79400/254647\n",
      "done 2nd loop 79500/254647\n",
      "done 2nd loop 79600/254647\n",
      "done 2nd loop 79700/254647\n",
      "done 2nd loop 79800/254647\n",
      "done 2nd loop 79900/254647\n",
      "done 2nd loop 80000/254647\n",
      "done 2nd loop 80100/254647\n",
      "done 2nd loop 80200/254647\n",
      "done 2nd loop 80300/254647\n",
      "done 2nd loop 80400/254647\n",
      "done 2nd loop 80500/254647\n",
      "done 2nd loop 80600/254647\n",
      "done 2nd loop 80700/254647\n",
      "done 2nd loop 80800/254647\n",
      "done 2nd loop 80900/254647\n",
      "done 2nd loop 81000/254647\n",
      "done 2nd loop 81100/254647\n",
      "done 2nd loop 81200/254647\n",
      "done 2nd loop 81300/254647\n",
      "done 2nd loop 81400/254647\n",
      "done 2nd loop 81500/254647\n",
      "done 2nd loop 81600/254647\n",
      "done 2nd loop 81700/254647\n",
      "done 2nd loop 81800/254647\n",
      "done 2nd loop 81900/254647\n",
      "done 2nd loop 82000/254647\n",
      "done 2nd loop 82100/254647\n",
      "done 2nd loop 82200/254647\n",
      "done 2nd loop 82300/254647\n",
      "done 2nd loop 82400/254647\n",
      "done 2nd loop 82500/254647\n",
      "done 2nd loop 82600/254647\n",
      "done 2nd loop 82700/254647\n",
      "done 2nd loop 82800/254647\n",
      "done 2nd loop 82900/254647\n",
      "done 2nd loop 83000/254647\n",
      "done 2nd loop 83100/254647\n",
      "done 2nd loop 83200/254647\n",
      "done 2nd loop 83300/254647\n",
      "done 2nd loop 83400/254647\n",
      "done 2nd loop 83500/254647\n",
      "done 2nd loop 83600/254647\n",
      "done 2nd loop 83700/254647\n",
      "done 2nd loop 83800/254647\n",
      "done 2nd loop 83900/254647\n",
      "done 2nd loop 84000/254647\n",
      "done 2nd loop 84100/254647\n",
      "done 2nd loop 84200/254647\n",
      "done 2nd loop 84300/254647\n",
      "done 2nd loop 84400/254647\n",
      "done 2nd loop 84500/254647\n",
      "done 2nd loop 84600/254647\n",
      "done 2nd loop 84700/254647\n",
      "done 2nd loop 84800/254647\n",
      "done 2nd loop 84900/254647\n",
      "done 2nd loop 85000/254647\n",
      "done 2nd loop 85100/254647\n",
      "done 2nd loop 85200/254647\n",
      "done 2nd loop 85300/254647\n",
      "done 2nd loop 85400/254647\n",
      "done 2nd loop 85500/254647\n",
      "done 2nd loop 85600/254647\n",
      "done 2nd loop 85700/254647\n",
      "done 2nd loop 85800/254647\n",
      "done 2nd loop 85900/254647\n",
      "done 2nd loop 86000/254647\n",
      "done 2nd loop 86100/254647\n",
      "done 2nd loop 86200/254647\n",
      "done 2nd loop 86300/254647\n",
      "done 2nd loop 86400/254647\n",
      "done 2nd loop 86500/254647\n",
      "done 2nd loop 86600/254647\n",
      "done 2nd loop 86700/254647\n",
      "done 2nd loop 86800/254647\n",
      "done 2nd loop 86900/254647\n",
      "done 2nd loop 87000/254647\n",
      "done 2nd loop 87100/254647\n",
      "done 2nd loop 87200/254647\n",
      "done 2nd loop 87300/254647\n",
      "done 2nd loop 87400/254647\n",
      "done 2nd loop 87500/254647\n",
      "done 2nd loop 87600/254647\n",
      "done 2nd loop 87700/254647\n",
      "done 2nd loop 87800/254647\n",
      "done 2nd loop 87900/254647\n",
      "done 2nd loop 88000/254647\n",
      "done 2nd loop 88100/254647\n",
      "done 2nd loop 88200/254647\n",
      "done 2nd loop 88300/254647\n",
      "done 2nd loop 88400/254647\n",
      "done 2nd loop 88500/254647\n",
      "done 2nd loop 88600/254647\n",
      "done 2nd loop 88700/254647\n",
      "done 2nd loop 88800/254647\n",
      "done 2nd loop 88900/254647\n",
      "done 2nd loop 89000/254647\n",
      "done 2nd loop 89100/254647\n",
      "done 2nd loop 89200/254647\n",
      "done 2nd loop 89300/254647\n",
      "done 2nd loop 89400/254647\n",
      "done 2nd loop 89500/254647\n",
      "done 2nd loop 89600/254647\n",
      "done 2nd loop 89700/254647\n",
      "done 2nd loop 89800/254647\n",
      "done 2nd loop 89900/254647\n",
      "done 2nd loop 90000/254647\n",
      "done 2nd loop 90100/254647\n",
      "done 2nd loop 90200/254647\n",
      "done 2nd loop 90300/254647\n",
      "done 2nd loop 90400/254647\n",
      "done 2nd loop 90500/254647\n",
      "done 2nd loop 90600/254647\n",
      "done 2nd loop 90700/254647\n",
      "done 2nd loop 90800/254647\n",
      "done 2nd loop 90900/254647\n",
      "done 2nd loop 91000/254647\n",
      "done 2nd loop 91100/254647\n",
      "done 2nd loop 91200/254647\n",
      "done 2nd loop 91300/254647\n",
      "done 2nd loop 91400/254647\n",
      "done 2nd loop 91500/254647\n",
      "done 2nd loop 91600/254647\n",
      "done 2nd loop 91700/254647\n",
      "done 2nd loop 91800/254647\n",
      "done 2nd loop 91900/254647\n",
      "done 2nd loop 92000/254647\n",
      "done 2nd loop 92100/254647\n",
      "done 2nd loop 92200/254647\n",
      "done 2nd loop 92300/254647\n",
      "done 2nd loop 92400/254647\n",
      "done 2nd loop 92500/254647\n",
      "done 2nd loop 92600/254647\n",
      "done 2nd loop 92700/254647\n",
      "done 2nd loop 92800/254647\n",
      "done 2nd loop 92900/254647\n",
      "done 2nd loop 93000/254647\n",
      "done 2nd loop 93100/254647\n",
      "done 2nd loop 93200/254647\n",
      "done 2nd loop 93300/254647\n",
      "done 2nd loop 93400/254647\n",
      "done 2nd loop 93500/254647\n",
      "done 2nd loop 93600/254647\n",
      "done 2nd loop 93700/254647\n",
      "done 2nd loop 93800/254647\n",
      "done 2nd loop 93900/254647\n",
      "done 2nd loop 94000/254647\n",
      "done 2nd loop 94100/254647\n",
      "done 2nd loop 94200/254647\n",
      "done 2nd loop 94300/254647\n",
      "done 2nd loop 94400/254647\n",
      "done 2nd loop 94500/254647\n",
      "done 2nd loop 94600/254647\n",
      "done 2nd loop 94700/254647\n",
      "done 2nd loop 94800/254647\n",
      "done 2nd loop 94900/254647\n",
      "done 2nd loop 95000/254647\n",
      "done 2nd loop 95100/254647\n",
      "done 2nd loop 95200/254647\n",
      "done 2nd loop 95300/254647\n",
      "done 2nd loop 95400/254647\n",
      "done 2nd loop 95500/254647\n",
      "done 2nd loop 95600/254647\n",
      "done 2nd loop 95700/254647\n",
      "done 2nd loop 95800/254647\n",
      "done 2nd loop 95900/254647\n",
      "done 2nd loop 96000/254647\n",
      "done 2nd loop 96100/254647\n",
      "done 2nd loop 96200/254647\n",
      "done 2nd loop 96300/254647\n",
      "done 2nd loop 96400/254647\n",
      "done 2nd loop 96500/254647\n",
      "done 2nd loop 96600/254647\n",
      "done 2nd loop 96700/254647\n",
      "done 2nd loop 96800/254647\n",
      "done 2nd loop 96900/254647\n",
      "done 2nd loop 97000/254647\n",
      "done 2nd loop 97100/254647\n",
      "done 2nd loop 97200/254647\n",
      "done 2nd loop 97300/254647\n",
      "done 2nd loop 97400/254647\n",
      "done 2nd loop 97500/254647\n",
      "done 2nd loop 97600/254647\n",
      "done 2nd loop 97700/254647\n",
      "done 2nd loop 97800/254647\n",
      "done 2nd loop 97900/254647\n",
      "done 2nd loop 98000/254647\n",
      "done 2nd loop 98100/254647\n",
      "done 2nd loop 98200/254647\n",
      "done 2nd loop 98300/254647\n",
      "done 2nd loop 98400/254647\n",
      "done 2nd loop 98500/254647\n",
      "done 2nd loop 98600/254647\n",
      "done 2nd loop 98700/254647\n",
      "done 2nd loop 98800/254647\n",
      "done 2nd loop 98900/254647\n",
      "done 2nd loop 99000/254647\n",
      "done 2nd loop 99100/254647\n",
      "done 2nd loop 99200/254647\n",
      "done 2nd loop 99300/254647\n",
      "done 2nd loop 99400/254647\n",
      "done 2nd loop 99500/254647\n",
      "done 2nd loop 99600/254647\n",
      "done 2nd loop 99700/254647\n",
      "done 2nd loop 99800/254647\n",
      "done 2nd loop 99900/254647\n",
      "done 2nd loop 100000/254647\n",
      "done 2nd loop 100100/254647\n",
      "done 2nd loop 100200/254647\n",
      "done 2nd loop 100300/254647\n",
      "done 2nd loop 100400/254647\n",
      "done 2nd loop 100500/254647\n",
      "done 2nd loop 100600/254647\n",
      "done 2nd loop 100700/254647\n",
      "done 2nd loop 100800/254647\n",
      "done 2nd loop 100900/254647\n",
      "done 2nd loop 101000/254647\n",
      "done 2nd loop 101100/254647\n",
      "done 2nd loop 101200/254647\n",
      "done 2nd loop 101300/254647\n",
      "done 2nd loop 101400/254647\n",
      "done 2nd loop 101500/254647\n",
      "done 2nd loop 101600/254647\n",
      "done 2nd loop 101700/254647\n",
      "done 2nd loop 101800/254647\n",
      "done 2nd loop 101900/254647\n",
      "done 2nd loop 102000/254647\n",
      "done 2nd loop 102100/254647\n",
      "done 2nd loop 102200/254647\n",
      "done 2nd loop 102300/254647\n",
      "done 2nd loop 102400/254647\n",
      "done 2nd loop 102500/254647\n",
      "done 2nd loop 102600/254647\n",
      "done 2nd loop 102700/254647\n",
      "done 2nd loop 102800/254647\n",
      "done 2nd loop 102900/254647\n",
      "done 2nd loop 103000/254647\n",
      "done 2nd loop 103100/254647\n",
      "done 2nd loop 103200/254647\n",
      "done 2nd loop 103300/254647\n",
      "done 2nd loop 103400/254647\n",
      "done 2nd loop 103500/254647\n",
      "done 2nd loop 103600/254647\n",
      "done 2nd loop 103700/254647\n",
      "done 2nd loop 103800/254647\n",
      "done 2nd loop 103900/254647\n",
      "done 2nd loop 104000/254647\n",
      "done 2nd loop 104100/254647\n",
      "done 2nd loop 104200/254647\n",
      "done 2nd loop 104300/254647\n",
      "done 2nd loop 104400/254647\n",
      "done 2nd loop 104500/254647\n",
      "done 2nd loop 104600/254647\n",
      "done 2nd loop 104700/254647\n",
      "done 2nd loop 104800/254647\n",
      "done 2nd loop 104900/254647\n",
      "done 2nd loop 105000/254647\n",
      "done 2nd loop 105100/254647\n",
      "done 2nd loop 105200/254647\n",
      "done 2nd loop 105300/254647\n",
      "done 2nd loop 105400/254647\n",
      "done 2nd loop 105500/254647\n",
      "done 2nd loop 105600/254647\n",
      "done 2nd loop 105700/254647\n",
      "done 2nd loop 105800/254647\n",
      "done 2nd loop 105900/254647\n",
      "done 2nd loop 106000/254647\n",
      "done 2nd loop 106100/254647\n",
      "done 2nd loop 106200/254647\n",
      "done 2nd loop 106300/254647\n",
      "done 2nd loop 106400/254647\n",
      "done 2nd loop 106500/254647\n",
      "done 2nd loop 106600/254647\n",
      "done 2nd loop 106700/254647\n",
      "done 2nd loop 106800/254647\n",
      "done 2nd loop 106900/254647\n",
      "done 2nd loop 107000/254647\n",
      "done 2nd loop 107100/254647\n",
      "done 2nd loop 107200/254647\n",
      "done 2nd loop 107300/254647\n",
      "done 2nd loop 107400/254647\n",
      "done 2nd loop 107500/254647\n",
      "done 2nd loop 107600/254647\n",
      "done 2nd loop 107700/254647\n",
      "done 2nd loop 107800/254647\n",
      "done 2nd loop 107900/254647\n",
      "done 2nd loop 108000/254647\n",
      "done 2nd loop 108100/254647\n",
      "done 2nd loop 108200/254647\n",
      "done 2nd loop 108300/254647\n",
      "done 2nd loop 108400/254647\n",
      "done 2nd loop 108500/254647\n",
      "done 2nd loop 108600/254647\n",
      "done 2nd loop 108700/254647\n",
      "done 2nd loop 108800/254647\n",
      "done 2nd loop 108900/254647\n",
      "done 2nd loop 109000/254647\n",
      "done 2nd loop 109100/254647\n",
      "done 2nd loop 109200/254647\n",
      "done 2nd loop 109300/254647\n",
      "done 2nd loop 109400/254647\n",
      "done 2nd loop 109500/254647\n",
      "done 2nd loop 109600/254647\n",
      "done 2nd loop 109700/254647\n",
      "done 2nd loop 109800/254647\n",
      "done 2nd loop 109900/254647\n",
      "done 2nd loop 110000/254647\n",
      "done 2nd loop 110100/254647\n",
      "done 2nd loop 110200/254647\n",
      "done 2nd loop 110300/254647\n",
      "done 2nd loop 110400/254647\n",
      "done 2nd loop 110500/254647\n",
      "done 2nd loop 110600/254647\n",
      "done 2nd loop 110700/254647\n",
      "done 2nd loop 110800/254647\n",
      "done 2nd loop 110900/254647\n",
      "done 2nd loop 111000/254647\n",
      "done 2nd loop 111100/254647\n",
      "done 2nd loop 111200/254647\n",
      "done 2nd loop 111300/254647\n",
      "done 2nd loop 111400/254647\n",
      "done 2nd loop 111500/254647\n",
      "done 2nd loop 111600/254647\n",
      "done 2nd loop 111700/254647\n",
      "done 2nd loop 111800/254647\n",
      "done 2nd loop 111900/254647\n",
      "done 2nd loop 112000/254647\n",
      "done 2nd loop 112100/254647\n",
      "done 2nd loop 112200/254647\n",
      "done 2nd loop 112300/254647\n",
      "done 2nd loop 112400/254647\n",
      "done 2nd loop 112500/254647\n",
      "done 2nd loop 112600/254647\n",
      "done 2nd loop 112700/254647\n",
      "done 2nd loop 112800/254647\n",
      "done 2nd loop 112900/254647\n",
      "done 2nd loop 113000/254647\n",
      "done 2nd loop 113100/254647\n",
      "done 2nd loop 113200/254647\n",
      "done 2nd loop 113300/254647\n",
      "done 2nd loop 113400/254647\n",
      "done 2nd loop 113500/254647\n",
      "done 2nd loop 113600/254647\n",
      "done 2nd loop 113700/254647\n",
      "done 2nd loop 113800/254647\n",
      "done 2nd loop 113900/254647\n",
      "done 2nd loop 114000/254647\n",
      "done 2nd loop 114100/254647\n",
      "done 2nd loop 114200/254647\n",
      "done 2nd loop 114300/254647\n",
      "done 2nd loop 114400/254647\n",
      "done 2nd loop 114500/254647\n",
      "done 2nd loop 114600/254647\n",
      "done 2nd loop 114700/254647\n",
      "done 2nd loop 114800/254647\n",
      "done 2nd loop 114900/254647\n",
      "done 2nd loop 115000/254647\n",
      "done 2nd loop 115100/254647\n",
      "done 2nd loop 115200/254647\n",
      "done 2nd loop 115300/254647\n",
      "done 2nd loop 115400/254647\n",
      "done 2nd loop 115500/254647\n",
      "done 2nd loop 115600/254647\n",
      "done 2nd loop 115700/254647\n",
      "done 2nd loop 115800/254647\n",
      "done 2nd loop 115900/254647\n",
      "done 2nd loop 116000/254647\n",
      "done 2nd loop 116100/254647\n",
      "done 2nd loop 116200/254647\n",
      "done 2nd loop 116300/254647\n",
      "done 2nd loop 116400/254647\n",
      "done 2nd loop 116500/254647\n",
      "done 2nd loop 116600/254647\n",
      "done 2nd loop 116700/254647\n",
      "done 2nd loop 116800/254647\n",
      "done 2nd loop 116900/254647\n",
      "done 2nd loop 117000/254647\n",
      "done 2nd loop 117100/254647\n",
      "done 2nd loop 117200/254647\n",
      "done 2nd loop 117300/254647\n",
      "done 2nd loop 117400/254647\n",
      "done 2nd loop 117500/254647\n",
      "done 2nd loop 117600/254647\n",
      "done 2nd loop 117700/254647\n",
      "done 2nd loop 117800/254647\n",
      "done 2nd loop 117900/254647\n",
      "done 2nd loop 118000/254647\n",
      "done 2nd loop 118100/254647\n",
      "done 2nd loop 118200/254647\n",
      "done 2nd loop 118300/254647\n",
      "done 2nd loop 118400/254647\n",
      "done 2nd loop 118500/254647\n",
      "done 2nd loop 118600/254647\n",
      "done 2nd loop 118700/254647\n",
      "done 2nd loop 118800/254647\n",
      "done 2nd loop 118900/254647\n",
      "done 2nd loop 119000/254647\n",
      "done 2nd loop 119100/254647\n",
      "done 2nd loop 119200/254647\n",
      "done 2nd loop 119300/254647\n",
      "done 2nd loop 119400/254647\n",
      "done 2nd loop 119500/254647\n",
      "done 2nd loop 119600/254647\n",
      "done 2nd loop 119700/254647\n",
      "done 2nd loop 119800/254647\n",
      "done 2nd loop 119900/254647\n",
      "done 2nd loop 120000/254647\n",
      "done 2nd loop 120100/254647\n",
      "done 2nd loop 120200/254647\n",
      "done 2nd loop 120300/254647\n",
      "done 2nd loop 120400/254647\n",
      "done 2nd loop 120500/254647\n",
      "done 2nd loop 120600/254647\n",
      "done 2nd loop 120700/254647\n",
      "done 2nd loop 120800/254647\n",
      "done 2nd loop 120900/254647\n",
      "done 2nd loop 121000/254647\n",
      "done 2nd loop 121100/254647\n",
      "done 2nd loop 121200/254647\n",
      "done 2nd loop 121300/254647\n",
      "done 2nd loop 121400/254647\n",
      "done 2nd loop 121500/254647\n",
      "done 2nd loop 121600/254647\n",
      "done 2nd loop 121700/254647\n",
      "done 2nd loop 121800/254647\n",
      "done 2nd loop 121900/254647\n",
      "done 2nd loop 122000/254647\n",
      "done 2nd loop 122100/254647\n",
      "done 2nd loop 122200/254647\n",
      "done 2nd loop 122300/254647\n",
      "done 2nd loop 122400/254647\n",
      "done 2nd loop 122500/254647\n",
      "done 2nd loop 122600/254647\n",
      "done 2nd loop 122700/254647\n",
      "done 2nd loop 122800/254647\n",
      "done 2nd loop 122900/254647\n",
      "done 2nd loop 123000/254647\n",
      "done 2nd loop 123100/254647\n",
      "done 2nd loop 123200/254647\n",
      "done 2nd loop 123300/254647\n",
      "done 2nd loop 123400/254647\n",
      "done 2nd loop 123500/254647\n",
      "done 2nd loop 123600/254647\n",
      "done 2nd loop 123700/254647\n",
      "done 2nd loop 123800/254647\n",
      "done 2nd loop 123900/254647\n",
      "done 2nd loop 124000/254647\n",
      "done 2nd loop 124100/254647\n",
      "done 2nd loop 124200/254647\n",
      "done 2nd loop 124300/254647\n",
      "done 2nd loop 124400/254647\n",
      "done 2nd loop 124500/254647\n",
      "done 2nd loop 124600/254647\n",
      "done 2nd loop 124700/254647\n",
      "done 2nd loop 124800/254647\n",
      "done 2nd loop 124900/254647\n",
      "done 2nd loop 125000/254647\n",
      "done 2nd loop 125100/254647\n",
      "done 2nd loop 125200/254647\n",
      "done 2nd loop 125300/254647\n",
      "done 2nd loop 125400/254647\n",
      "done 2nd loop 125500/254647\n",
      "done 2nd loop 125600/254647\n",
      "done 2nd loop 125700/254647\n",
      "done 2nd loop 125800/254647\n",
      "done 2nd loop 125900/254647\n",
      "done 2nd loop 126000/254647\n",
      "done 2nd loop 126100/254647\n",
      "done 2nd loop 126200/254647\n",
      "done 2nd loop 126300/254647\n",
      "done 2nd loop 126400/254647\n",
      "done 2nd loop 126500/254647\n",
      "done 2nd loop 126600/254647\n",
      "done 2nd loop 126700/254647\n",
      "done 2nd loop 126800/254647\n",
      "done 2nd loop 126900/254647\n",
      "done 2nd loop 127000/254647\n",
      "done 2nd loop 127100/254647\n",
      "done 2nd loop 127200/254647\n",
      "done 2nd loop 127300/254647\n",
      "done 2nd loop 127400/254647\n",
      "done 2nd loop 127500/254647\n",
      "done 2nd loop 127600/254647\n",
      "done 2nd loop 127700/254647\n",
      "done 2nd loop 127800/254647\n",
      "done 2nd loop 127900/254647\n",
      "done 2nd loop 128000/254647\n",
      "done 2nd loop 128100/254647\n",
      "done 2nd loop 128200/254647\n",
      "done 2nd loop 128300/254647\n",
      "done 2nd loop 128400/254647\n",
      "done 2nd loop 128500/254647\n",
      "done 2nd loop 128600/254647\n",
      "done 2nd loop 128700/254647\n",
      "done 2nd loop 128800/254647\n",
      "done 2nd loop 128900/254647\n",
      "done 2nd loop 129000/254647\n",
      "done 2nd loop 129100/254647\n",
      "done 2nd loop 129200/254647\n",
      "done 2nd loop 129300/254647\n",
      "done 2nd loop 129400/254647\n",
      "done 2nd loop 129500/254647\n",
      "done 2nd loop 129600/254647\n",
      "done 2nd loop 129700/254647\n",
      "done 2nd loop 129800/254647\n",
      "done 2nd loop 129900/254647\n",
      "done 2nd loop 130000/254647\n",
      "done 2nd loop 130100/254647\n",
      "done 2nd loop 130200/254647\n",
      "done 2nd loop 130300/254647\n",
      "done 2nd loop 130400/254647\n",
      "done 2nd loop 130500/254647\n",
      "done 2nd loop 130600/254647\n",
      "done 2nd loop 130700/254647\n",
      "done 2nd loop 130800/254647\n",
      "done 2nd loop 130900/254647\n",
      "done 2nd loop 131000/254647\n",
      "done 2nd loop 131100/254647\n",
      "done 2nd loop 131200/254647\n",
      "done 2nd loop 131300/254647\n",
      "done 2nd loop 131400/254647\n",
      "done 2nd loop 131500/254647\n",
      "done 2nd loop 131600/254647\n",
      "done 2nd loop 131700/254647\n",
      "done 2nd loop 131800/254647\n",
      "done 2nd loop 131900/254647\n",
      "done 2nd loop 132000/254647\n",
      "done 2nd loop 132100/254647\n",
      "done 2nd loop 132200/254647\n",
      "done 2nd loop 132300/254647\n",
      "done 2nd loop 132400/254647\n",
      "done 2nd loop 132500/254647\n",
      "done 2nd loop 132600/254647\n",
      "done 2nd loop 132700/254647\n",
      "done 2nd loop 132800/254647\n",
      "done 2nd loop 132900/254647\n",
      "done 2nd loop 133000/254647\n",
      "done 2nd loop 133100/254647\n",
      "done 2nd loop 133200/254647\n",
      "done 2nd loop 133300/254647\n",
      "done 2nd loop 133400/254647\n",
      "done 2nd loop 133500/254647\n",
      "done 2nd loop 133600/254647\n",
      "done 2nd loop 133700/254647\n",
      "done 2nd loop 133800/254647\n",
      "done 2nd loop 133900/254647\n",
      "done 2nd loop 134000/254647\n",
      "done 2nd loop 134100/254647\n",
      "done 2nd loop 134200/254647\n",
      "done 2nd loop 134300/254647\n",
      "done 2nd loop 134400/254647\n",
      "done 2nd loop 134500/254647\n",
      "done 2nd loop 134600/254647\n",
      "done 2nd loop 134700/254647\n",
      "done 2nd loop 134800/254647\n",
      "done 2nd loop 134900/254647\n",
      "done 2nd loop 135000/254647\n",
      "done 2nd loop 135100/254647\n",
      "done 2nd loop 135200/254647\n",
      "done 2nd loop 135300/254647\n",
      "done 2nd loop 135400/254647\n",
      "done 2nd loop 135500/254647\n",
      "done 2nd loop 135600/254647\n",
      "done 2nd loop 135700/254647\n",
      "done 2nd loop 135800/254647\n",
      "done 2nd loop 135900/254647\n",
      "done 2nd loop 136000/254647\n",
      "done 2nd loop 136100/254647\n",
      "done 2nd loop 136200/254647\n",
      "done 2nd loop 136300/254647\n",
      "done 2nd loop 136400/254647\n",
      "done 2nd loop 136500/254647\n",
      "done 2nd loop 136600/254647\n",
      "done 2nd loop 136700/254647\n",
      "done 2nd loop 136800/254647\n",
      "done 2nd loop 136900/254647\n",
      "done 2nd loop 137000/254647\n",
      "done 2nd loop 137100/254647\n",
      "done 2nd loop 137200/254647\n",
      "done 2nd loop 137300/254647\n",
      "done 2nd loop 137400/254647\n",
      "done 2nd loop 137500/254647\n",
      "done 2nd loop 137600/254647\n",
      "done 2nd loop 137700/254647\n",
      "done 2nd loop 137800/254647\n",
      "done 2nd loop 137900/254647\n",
      "done 2nd loop 138000/254647\n",
      "done 2nd loop 138100/254647\n",
      "done 2nd loop 138200/254647\n",
      "done 2nd loop 138300/254647\n",
      "done 2nd loop 138400/254647\n",
      "done 2nd loop 138500/254647\n",
      "done 2nd loop 138600/254647\n",
      "done 2nd loop 138700/254647\n",
      "done 2nd loop 138800/254647\n",
      "done 2nd loop 138900/254647\n",
      "done 2nd loop 139000/254647\n",
      "done 2nd loop 139100/254647\n",
      "done 2nd loop 139200/254647\n",
      "done 2nd loop 139300/254647\n",
      "done 2nd loop 139400/254647\n",
      "done 2nd loop 139500/254647\n",
      "done 2nd loop 139600/254647\n",
      "done 2nd loop 139700/254647\n",
      "done 2nd loop 139800/254647\n",
      "done 2nd loop 139900/254647\n",
      "done 2nd loop 140000/254647\n",
      "done 2nd loop 140100/254647\n",
      "done 2nd loop 140200/254647\n",
      "done 2nd loop 140300/254647\n",
      "done 2nd loop 140400/254647\n",
      "done 2nd loop 140500/254647\n",
      "done 2nd loop 140600/254647\n",
      "done 2nd loop 140700/254647\n",
      "done 2nd loop 140800/254647\n",
      "done 2nd loop 140900/254647\n",
      "done 2nd loop 141000/254647\n",
      "done 2nd loop 141100/254647\n",
      "done 2nd loop 141200/254647\n",
      "done 2nd loop 141300/254647\n",
      "done 2nd loop 141400/254647\n",
      "done 2nd loop 141500/254647\n",
      "done 2nd loop 141600/254647\n",
      "done 2nd loop 141700/254647\n",
      "done 2nd loop 141800/254647\n",
      "done 2nd loop 141900/254647\n",
      "done 2nd loop 142000/254647\n",
      "done 2nd loop 142100/254647\n",
      "done 2nd loop 142200/254647\n",
      "done 2nd loop 142300/254647\n",
      "done 2nd loop 142400/254647\n",
      "done 2nd loop 142500/254647\n",
      "done 2nd loop 142600/254647\n",
      "done 2nd loop 142700/254647\n",
      "done 2nd loop 142800/254647\n",
      "done 2nd loop 142900/254647\n",
      "done 2nd loop 143000/254647\n",
      "done 2nd loop 143100/254647\n",
      "done 2nd loop 143200/254647\n",
      "done 2nd loop 143300/254647\n",
      "done 2nd loop 143400/254647\n",
      "done 2nd loop 143500/254647\n",
      "done 2nd loop 143600/254647\n",
      "done 2nd loop 143700/254647\n",
      "done 2nd loop 143800/254647\n",
      "done 2nd loop 143900/254647\n",
      "done 2nd loop 144000/254647\n",
      "done 2nd loop 144100/254647\n",
      "done 2nd loop 144200/254647\n",
      "done 2nd loop 144300/254647\n",
      "done 2nd loop 144400/254647\n",
      "done 2nd loop 144500/254647\n",
      "done 2nd loop 144600/254647\n",
      "done 2nd loop 144700/254647\n",
      "done 2nd loop 144800/254647\n",
      "done 2nd loop 144900/254647\n",
      "done 2nd loop 145000/254647\n",
      "done 2nd loop 145100/254647\n",
      "done 2nd loop 145200/254647\n",
      "done 2nd loop 145300/254647\n",
      "done 2nd loop 145400/254647\n",
      "done 2nd loop 145500/254647\n",
      "done 2nd loop 145600/254647\n",
      "done 2nd loop 145700/254647\n",
      "done 2nd loop 145800/254647\n",
      "done 2nd loop 145900/254647\n",
      "done 2nd loop 146000/254647\n",
      "done 2nd loop 146100/254647\n",
      "done 2nd loop 146200/254647\n",
      "done 2nd loop 146300/254647\n",
      "done 2nd loop 146400/254647\n",
      "done 2nd loop 146500/254647\n",
      "done 2nd loop 146600/254647\n",
      "done 2nd loop 146700/254647\n",
      "done 2nd loop 146800/254647\n",
      "done 2nd loop 146900/254647\n",
      "done 2nd loop 147000/254647\n",
      "done 2nd loop 147100/254647\n",
      "done 2nd loop 147200/254647\n",
      "done 2nd loop 147300/254647\n",
      "done 2nd loop 147400/254647\n",
      "done 2nd loop 147500/254647\n",
      "done 2nd loop 147600/254647\n",
      "done 2nd loop 147700/254647\n",
      "done 2nd loop 147800/254647\n",
      "done 2nd loop 147900/254647\n",
      "done 2nd loop 148000/254647\n",
      "done 2nd loop 148100/254647\n",
      "done 2nd loop 148200/254647\n",
      "done 2nd loop 148300/254647\n",
      "done 2nd loop 148400/254647\n",
      "done 2nd loop 148500/254647\n",
      "done 2nd loop 148600/254647\n",
      "done 2nd loop 148700/254647\n",
      "done 2nd loop 148800/254647\n",
      "done 2nd loop 148900/254647\n",
      "done 2nd loop 149000/254647\n",
      "done 2nd loop 149100/254647\n",
      "done 2nd loop 149200/254647\n",
      "done 2nd loop 149300/254647\n",
      "done 2nd loop 149400/254647\n",
      "done 2nd loop 149500/254647\n",
      "done 2nd loop 149600/254647\n",
      "done 2nd loop 149700/254647\n",
      "done 2nd loop 149800/254647\n",
      "done 2nd loop 149900/254647\n",
      "done 2nd loop 150000/254647\n",
      "done 2nd loop 150100/254647\n",
      "done 2nd loop 150200/254647\n",
      "done 2nd loop 150300/254647\n",
      "done 2nd loop 150400/254647\n",
      "done 2nd loop 150500/254647\n",
      "done 2nd loop 150600/254647\n",
      "done 2nd loop 150700/254647\n",
      "done 2nd loop 150800/254647\n",
      "done 2nd loop 150900/254647\n",
      "done 2nd loop 151000/254647\n",
      "done 2nd loop 151100/254647\n",
      "done 2nd loop 151200/254647\n",
      "done 2nd loop 151300/254647\n",
      "done 2nd loop 151400/254647\n",
      "done 2nd loop 151500/254647\n",
      "done 2nd loop 151600/254647\n",
      "done 2nd loop 151700/254647\n",
      "done 2nd loop 151800/254647\n",
      "done 2nd loop 151900/254647\n",
      "done 2nd loop 152000/254647\n",
      "done 2nd loop 152100/254647\n",
      "done 2nd loop 152200/254647\n",
      "done 2nd loop 152300/254647\n",
      "done 2nd loop 152400/254647\n",
      "done 2nd loop 152500/254647\n",
      "done 2nd loop 152600/254647\n",
      "done 2nd loop 152700/254647\n",
      "done 2nd loop 152800/254647\n",
      "done 2nd loop 152900/254647\n",
      "done 2nd loop 153000/254647\n",
      "done 2nd loop 153100/254647\n",
      "done 2nd loop 153200/254647\n",
      "done 2nd loop 153300/254647\n",
      "done 2nd loop 153400/254647\n",
      "done 2nd loop 153500/254647\n",
      "done 2nd loop 153600/254647\n",
      "done 2nd loop 153700/254647\n",
      "done 2nd loop 153800/254647\n",
      "done 2nd loop 153900/254647\n",
      "done 2nd loop 154000/254647\n",
      "done 2nd loop 154100/254647\n",
      "done 2nd loop 154200/254647\n",
      "done 2nd loop 154300/254647\n",
      "done 2nd loop 154400/254647\n",
      "done 2nd loop 154500/254647\n",
      "done 2nd loop 154600/254647\n",
      "done 2nd loop 154700/254647\n",
      "done 2nd loop 154800/254647\n",
      "done 2nd loop 154900/254647\n",
      "done 2nd loop 155000/254647\n",
      "done 2nd loop 155100/254647\n",
      "done 2nd loop 155200/254647\n",
      "done 2nd loop 155300/254647\n",
      "done 2nd loop 155400/254647\n",
      "done 2nd loop 155500/254647\n",
      "done 2nd loop 155600/254647\n",
      "done 2nd loop 155700/254647\n",
      "done 2nd loop 155800/254647\n",
      "done 2nd loop 155900/254647\n",
      "done 2nd loop 156000/254647\n",
      "done 2nd loop 156100/254647\n",
      "done 2nd loop 156200/254647\n",
      "done 2nd loop 156300/254647\n",
      "done 2nd loop 156400/254647\n",
      "done 2nd loop 156500/254647\n",
      "done 2nd loop 156600/254647\n",
      "done 2nd loop 156700/254647\n",
      "done 2nd loop 156800/254647\n",
      "done 2nd loop 156900/254647\n",
      "done 2nd loop 157000/254647\n",
      "done 2nd loop 157100/254647\n",
      "done 2nd loop 157200/254647\n",
      "done 2nd loop 157300/254647\n",
      "done 2nd loop 157400/254647\n",
      "done 2nd loop 157500/254647\n",
      "done 2nd loop 157600/254647\n",
      "done 2nd loop 157700/254647\n",
      "done 2nd loop 157800/254647\n",
      "done 2nd loop 157900/254647\n",
      "done 2nd loop 158000/254647\n",
      "done 2nd loop 158100/254647\n",
      "done 2nd loop 158200/254647\n",
      "done 2nd loop 158300/254647\n",
      "done 2nd loop 158400/254647\n",
      "done 2nd loop 158500/254647\n",
      "done 2nd loop 158600/254647\n",
      "done 2nd loop 158700/254647\n",
      "done 2nd loop 158800/254647\n",
      "done 2nd loop 158900/254647\n",
      "done 2nd loop 159000/254647\n",
      "done 2nd loop 159100/254647\n",
      "done 2nd loop 159200/254647\n",
      "done 2nd loop 159300/254647\n",
      "done 2nd loop 159400/254647\n",
      "done 2nd loop 159500/254647\n",
      "done 2nd loop 159600/254647\n",
      "done 2nd loop 159700/254647\n",
      "done 2nd loop 159800/254647\n",
      "done 2nd loop 159900/254647\n",
      "done 2nd loop 160000/254647\n",
      "done 2nd loop 160100/254647\n",
      "done 2nd loop 160200/254647\n",
      "done 2nd loop 160300/254647\n",
      "done 2nd loop 160400/254647\n",
      "done 2nd loop 160500/254647\n",
      "done 2nd loop 160600/254647\n",
      "done 2nd loop 160700/254647\n",
      "done 2nd loop 160800/254647\n",
      "done 2nd loop 160900/254647\n",
      "done 2nd loop 161000/254647\n",
      "done 2nd loop 161100/254647\n",
      "done 2nd loop 161200/254647\n",
      "done 2nd loop 161300/254647\n",
      "done 2nd loop 161400/254647\n",
      "done 2nd loop 161500/254647\n",
      "done 2nd loop 161600/254647\n",
      "done 2nd loop 161700/254647\n",
      "done 2nd loop 161800/254647\n",
      "done 2nd loop 161900/254647\n",
      "done 2nd loop 162000/254647\n",
      "done 2nd loop 162100/254647\n",
      "done 2nd loop 162200/254647\n",
      "done 2nd loop 162300/254647\n",
      "done 2nd loop 162400/254647\n",
      "done 2nd loop 162500/254647\n",
      "done 2nd loop 162600/254647\n",
      "done 2nd loop 162700/254647\n",
      "done 2nd loop 162800/254647\n",
      "done 2nd loop 162900/254647\n",
      "done 2nd loop 163000/254647\n",
      "done 2nd loop 163100/254647\n",
      "done 2nd loop 163200/254647\n",
      "done 2nd loop 163300/254647\n",
      "done 2nd loop 163400/254647\n",
      "done 2nd loop 163500/254647\n",
      "done 2nd loop 163600/254647\n",
      "done 2nd loop 163700/254647\n",
      "done 2nd loop 163800/254647\n",
      "done 2nd loop 163900/254647\n",
      "done 2nd loop 164000/254647\n",
      "done 2nd loop 164100/254647\n",
      "done 2nd loop 164200/254647\n",
      "done 2nd loop 164300/254647\n",
      "done 2nd loop 164400/254647\n",
      "done 2nd loop 164500/254647\n",
      "done 2nd loop 164600/254647\n",
      "done 2nd loop 164700/254647\n",
      "done 2nd loop 164800/254647\n",
      "done 2nd loop 164900/254647\n",
      "done 2nd loop 165000/254647\n",
      "done 2nd loop 165100/254647\n",
      "done 2nd loop 165200/254647\n",
      "done 2nd loop 165300/254647\n",
      "done 2nd loop 165400/254647\n",
      "done 2nd loop 165500/254647\n",
      "done 2nd loop 165600/254647\n",
      "done 2nd loop 165700/254647\n",
      "done 2nd loop 165800/254647\n",
      "done 2nd loop 165900/254647\n",
      "done 2nd loop 166000/254647\n",
      "done 2nd loop 166100/254647\n",
      "done 2nd loop 166200/254647\n",
      "done 2nd loop 166300/254647\n",
      "done 2nd loop 166400/254647\n",
      "done 2nd loop 166500/254647\n",
      "done 2nd loop 166600/254647\n",
      "done 2nd loop 166700/254647\n",
      "done 2nd loop 166800/254647\n",
      "done 2nd loop 166900/254647\n",
      "done 2nd loop 167000/254647\n",
      "done 2nd loop 167100/254647\n",
      "done 2nd loop 167200/254647\n",
      "done 2nd loop 167300/254647\n",
      "done 2nd loop 167400/254647\n",
      "done 2nd loop 167500/254647\n",
      "done 2nd loop 167600/254647\n",
      "done 2nd loop 167700/254647\n",
      "done 2nd loop 167800/254647\n",
      "done 2nd loop 167900/254647\n",
      "done 2nd loop 168000/254647\n",
      "done 2nd loop 168100/254647\n",
      "done 2nd loop 168200/254647\n",
      "done 2nd loop 168300/254647\n",
      "done 2nd loop 168400/254647\n",
      "done 2nd loop 168500/254647\n",
      "done 2nd loop 168600/254647\n",
      "done 2nd loop 168700/254647\n",
      "done 2nd loop 168800/254647\n",
      "done 2nd loop 168900/254647\n",
      "done 2nd loop 169000/254647\n",
      "done 2nd loop 169100/254647\n",
      "done 2nd loop 169200/254647\n",
      "done 2nd loop 169300/254647\n",
      "done 2nd loop 169400/254647\n",
      "done 2nd loop 169500/254647\n",
      "done 2nd loop 169600/254647\n",
      "done 2nd loop 169700/254647\n",
      "done 2nd loop 169800/254647\n",
      "done 2nd loop 169900/254647\n",
      "done 2nd loop 170000/254647\n",
      "done 2nd loop 170100/254647\n",
      "done 2nd loop 170200/254647\n",
      "done 2nd loop 170300/254647\n",
      "done 2nd loop 170400/254647\n",
      "done 2nd loop 170500/254647\n",
      "done 2nd loop 170600/254647\n",
      "done 2nd loop 170700/254647\n",
      "done 2nd loop 170800/254647\n",
      "done 2nd loop 170900/254647\n",
      "done 2nd loop 171000/254647\n",
      "done 2nd loop 171100/254647\n",
      "done 2nd loop 171200/254647\n",
      "done 2nd loop 171300/254647\n",
      "done 2nd loop 171400/254647\n",
      "done 2nd loop 171500/254647\n",
      "done 2nd loop 171600/254647\n",
      "done 2nd loop 171700/254647\n",
      "done 2nd loop 171800/254647\n",
      "done 2nd loop 171900/254647\n",
      "done 2nd loop 172000/254647\n",
      "done 2nd loop 172100/254647\n",
      "done 2nd loop 172200/254647\n",
      "done 2nd loop 172300/254647\n",
      "done 2nd loop 172400/254647\n",
      "done 2nd loop 172500/254647\n",
      "done 2nd loop 172600/254647\n",
      "done 2nd loop 172700/254647\n",
      "done 2nd loop 172800/254647\n",
      "done 2nd loop 172900/254647\n",
      "done 2nd loop 173000/254647\n",
      "done 2nd loop 173100/254647\n",
      "done 2nd loop 173200/254647\n",
      "done 2nd loop 173300/254647\n",
      "done 2nd loop 173400/254647\n",
      "done 2nd loop 173500/254647\n",
      "done 2nd loop 173600/254647\n",
      "done 2nd loop 173700/254647\n",
      "done 2nd loop 173800/254647\n",
      "done 2nd loop 173900/254647\n",
      "done 2nd loop 174000/254647\n",
      "done 2nd loop 174100/254647\n",
      "done 2nd loop 174200/254647\n",
      "done 2nd loop 174300/254647\n",
      "done 2nd loop 174400/254647\n",
      "done 2nd loop 174500/254647\n",
      "done 2nd loop 174600/254647\n",
      "done 2nd loop 174700/254647\n",
      "done 2nd loop 174800/254647\n",
      "done 2nd loop 174900/254647\n",
      "done 2nd loop 175000/254647\n",
      "done 2nd loop 175100/254647\n",
      "done 2nd loop 175200/254647\n",
      "done 2nd loop 175300/254647\n",
      "done 2nd loop 175400/254647\n",
      "done 2nd loop 175500/254647\n",
      "done 2nd loop 175600/254647\n",
      "done 2nd loop 175700/254647\n",
      "done 2nd loop 175800/254647\n",
      "done 2nd loop 175900/254647\n",
      "done 2nd loop 176000/254647\n",
      "done 2nd loop 176100/254647\n",
      "done 2nd loop 176200/254647\n",
      "done 2nd loop 176300/254647\n",
      "done 2nd loop 176400/254647\n",
      "done 2nd loop 176500/254647\n",
      "done 2nd loop 176600/254647\n",
      "done 2nd loop 176700/254647\n",
      "done 2nd loop 176800/254647\n",
      "done 2nd loop 176900/254647\n",
      "done 2nd loop 177000/254647\n",
      "done 2nd loop 177100/254647\n",
      "done 2nd loop 177200/254647\n",
      "done 2nd loop 177300/254647\n",
      "done 2nd loop 177400/254647\n",
      "done 2nd loop 177500/254647\n",
      "done 2nd loop 177600/254647\n",
      "done 2nd loop 177700/254647\n",
      "done 2nd loop 177800/254647\n",
      "done 2nd loop 177900/254647\n",
      "done 2nd loop 178000/254647\n",
      "done 2nd loop 178100/254647\n",
      "done 2nd loop 178200/254647\n",
      "done 2nd loop 178300/254647\n",
      "done 2nd loop 178400/254647\n",
      "done 2nd loop 178500/254647\n",
      "done 2nd loop 178600/254647\n",
      "done 2nd loop 178700/254647\n",
      "done 2nd loop 178800/254647\n",
      "done 2nd loop 178900/254647\n",
      "done 2nd loop 179000/254647\n",
      "done 2nd loop 179100/254647\n",
      "done 2nd loop 179200/254647\n",
      "done 2nd loop 179300/254647\n",
      "done 2nd loop 179400/254647\n",
      "done 2nd loop 179500/254647\n",
      "done 2nd loop 179600/254647\n",
      "done 2nd loop 179700/254647\n",
      "done 2nd loop 179800/254647\n",
      "done 2nd loop 179900/254647\n",
      "done 2nd loop 180000/254647\n",
      "done 2nd loop 180100/254647\n",
      "done 2nd loop 180200/254647\n",
      "done 2nd loop 180300/254647\n",
      "done 2nd loop 180400/254647\n",
      "done 2nd loop 180500/254647\n",
      "done 2nd loop 180600/254647\n",
      "done 2nd loop 180700/254647\n",
      "done 2nd loop 180800/254647\n",
      "done 2nd loop 180900/254647\n",
      "done 2nd loop 181000/254647\n",
      "done 2nd loop 181100/254647\n",
      "done 2nd loop 181200/254647\n",
      "done 2nd loop 181300/254647\n",
      "done 2nd loop 181400/254647\n",
      "done 2nd loop 181500/254647\n",
      "done 2nd loop 181600/254647\n",
      "done 2nd loop 181700/254647\n",
      "done 2nd loop 181800/254647\n",
      "done 2nd loop 181900/254647\n",
      "done 2nd loop 182000/254647\n",
      "done 2nd loop 182100/254647\n",
      "done 2nd loop 182200/254647\n",
      "done 2nd loop 182300/254647\n",
      "done 2nd loop 182400/254647\n",
      "done 2nd loop 182500/254647\n",
      "done 2nd loop 182600/254647\n",
      "done 2nd loop 182700/254647\n",
      "done 2nd loop 182800/254647\n",
      "done 2nd loop 182900/254647\n",
      "done 2nd loop 183000/254647\n",
      "done 2nd loop 183100/254647\n",
      "done 2nd loop 183200/254647\n",
      "done 2nd loop 183300/254647\n",
      "done 2nd loop 183400/254647\n",
      "done 2nd loop 183500/254647\n",
      "done 2nd loop 183600/254647\n",
      "done 2nd loop 183700/254647\n",
      "done 2nd loop 183800/254647\n",
      "done 2nd loop 183900/254647\n",
      "done 2nd loop 184000/254647\n",
      "done 2nd loop 184100/254647\n",
      "done 2nd loop 184200/254647\n",
      "done 2nd loop 184300/254647\n",
      "done 2nd loop 184400/254647\n",
      "done 2nd loop 184500/254647\n",
      "done 2nd loop 184600/254647\n",
      "done 2nd loop 184700/254647\n",
      "done 2nd loop 184800/254647\n",
      "done 2nd loop 184900/254647\n",
      "done 2nd loop 185000/254647\n",
      "done 2nd loop 185100/254647\n",
      "done 2nd loop 185200/254647\n",
      "done 2nd loop 185300/254647\n",
      "done 2nd loop 185400/254647\n",
      "done 2nd loop 185500/254647\n",
      "done 2nd loop 185600/254647\n",
      "done 2nd loop 185700/254647\n",
      "done 2nd loop 185800/254647\n",
      "done 2nd loop 185900/254647\n",
      "done 2nd loop 186000/254647\n",
      "done 2nd loop 186100/254647\n",
      "done 2nd loop 186200/254647\n",
      "done 2nd loop 186300/254647\n",
      "done 2nd loop 186400/254647\n",
      "done 2nd loop 186500/254647\n",
      "done 2nd loop 186600/254647\n",
      "done 2nd loop 186700/254647\n",
      "done 2nd loop 186800/254647\n",
      "done 2nd loop 186900/254647\n",
      "done 2nd loop 187000/254647\n",
      "done 2nd loop 187100/254647\n",
      "done 2nd loop 187200/254647\n",
      "done 2nd loop 187300/254647\n",
      "done 2nd loop 187400/254647\n",
      "done 2nd loop 187500/254647\n",
      "done 2nd loop 187600/254647\n",
      "done 2nd loop 187700/254647\n",
      "done 2nd loop 187800/254647\n",
      "done 2nd loop 187900/254647\n",
      "done 2nd loop 188000/254647\n",
      "done 2nd loop 188100/254647\n",
      "done 2nd loop 188200/254647\n",
      "done 2nd loop 188300/254647\n",
      "done 2nd loop 188400/254647\n",
      "done 2nd loop 188500/254647\n",
      "done 2nd loop 188600/254647\n",
      "done 2nd loop 188700/254647\n",
      "done 2nd loop 188800/254647\n",
      "done 2nd loop 188900/254647\n",
      "done 2nd loop 189000/254647\n",
      "done 2nd loop 189100/254647\n",
      "done 2nd loop 189200/254647\n",
      "done 2nd loop 189300/254647\n",
      "done 2nd loop 189400/254647\n",
      "done 2nd loop 189500/254647\n",
      "done 2nd loop 189600/254647\n",
      "done 2nd loop 189700/254647\n",
      "done 2nd loop 189800/254647\n",
      "done 2nd loop 189900/254647\n",
      "done 2nd loop 190000/254647\n",
      "done 2nd loop 190100/254647\n",
      "done 2nd loop 190200/254647\n",
      "done 2nd loop 190300/254647\n",
      "done 2nd loop 190400/254647\n",
      "done 2nd loop 190500/254647\n",
      "done 2nd loop 190600/254647\n",
      "done 2nd loop 190700/254647\n",
      "done 2nd loop 190800/254647\n",
      "done 2nd loop 190900/254647\n",
      "done 2nd loop 191000/254647\n",
      "done 2nd loop 191100/254647\n",
      "done 2nd loop 191200/254647\n",
      "done 2nd loop 191300/254647\n",
      "done 2nd loop 191400/254647\n",
      "done 2nd loop 191500/254647\n",
      "done 2nd loop 191600/254647\n",
      "done 2nd loop 191700/254647\n",
      "done 2nd loop 191800/254647\n",
      "done 2nd loop 191900/254647\n",
      "done 2nd loop 192000/254647\n",
      "done 2nd loop 192100/254647\n",
      "done 2nd loop 192200/254647\n",
      "done 2nd loop 192300/254647\n",
      "done 2nd loop 192400/254647\n",
      "done 2nd loop 192500/254647\n",
      "done 2nd loop 192600/254647\n",
      "done 2nd loop 192700/254647\n",
      "done 2nd loop 192800/254647\n",
      "done 2nd loop 192900/254647\n",
      "done 2nd loop 193000/254647\n",
      "done 2nd loop 193100/254647\n",
      "done 2nd loop 193200/254647\n",
      "done 2nd loop 193300/254647\n",
      "done 2nd loop 193400/254647\n",
      "done 2nd loop 193500/254647\n",
      "done 2nd loop 193600/254647\n",
      "done 2nd loop 193700/254647\n",
      "done 2nd loop 193800/254647\n",
      "done 2nd loop 193900/254647\n",
      "done 2nd loop 194000/254647\n",
      "done 2nd loop 194100/254647\n",
      "done 2nd loop 194200/254647\n",
      "done 2nd loop 194300/254647\n",
      "done 2nd loop 194400/254647\n",
      "done 2nd loop 194500/254647\n",
      "done 2nd loop 194600/254647\n",
      "done 2nd loop 194700/254647\n",
      "done 2nd loop 194800/254647\n",
      "done 2nd loop 194900/254647\n",
      "done 2nd loop 195000/254647\n",
      "done 2nd loop 195100/254647\n",
      "done 2nd loop 195200/254647\n",
      "done 2nd loop 195300/254647\n",
      "done 2nd loop 195400/254647\n",
      "done 2nd loop 195500/254647\n",
      "done 2nd loop 195600/254647\n",
      "done 2nd loop 195700/254647\n",
      "done 2nd loop 195800/254647\n",
      "done 2nd loop 195900/254647\n",
      "done 2nd loop 196000/254647\n",
      "done 2nd loop 196100/254647\n",
      "done 2nd loop 196200/254647\n",
      "done 2nd loop 196300/254647\n",
      "done 2nd loop 196400/254647\n",
      "done 2nd loop 196500/254647\n",
      "done 2nd loop 196600/254647\n",
      "done 2nd loop 196700/254647\n",
      "done 2nd loop 196800/254647\n",
      "done 2nd loop 196900/254647\n",
      "done 2nd loop 197000/254647\n",
      "done 2nd loop 197100/254647\n",
      "done 2nd loop 197200/254647\n",
      "done 2nd loop 197300/254647\n",
      "done 2nd loop 197400/254647\n",
      "done 2nd loop 197500/254647\n",
      "done 2nd loop 197600/254647\n",
      "done 2nd loop 197700/254647\n",
      "done 2nd loop 197800/254647\n",
      "done 2nd loop 197900/254647\n",
      "done 2nd loop 198000/254647\n",
      "done 2nd loop 198100/254647\n",
      "done 2nd loop 198200/254647\n",
      "done 2nd loop 198300/254647\n",
      "done 2nd loop 198400/254647\n",
      "done 2nd loop 198500/254647\n",
      "done 2nd loop 198600/254647\n",
      "done 2nd loop 198700/254647\n",
      "done 2nd loop 198800/254647\n",
      "done 2nd loop 198900/254647\n",
      "done 2nd loop 199000/254647\n",
      "done 2nd loop 199100/254647\n",
      "done 2nd loop 199200/254647\n",
      "done 2nd loop 199300/254647\n",
      "done 2nd loop 199400/254647\n",
      "done 2nd loop 199500/254647\n",
      "done 2nd loop 199600/254647\n",
      "done 2nd loop 199700/254647\n",
      "done 2nd loop 199800/254647\n",
      "done 2nd loop 199900/254647\n",
      "done 2nd loop 200000/254647\n",
      "done 2nd loop 200100/254647\n",
      "done 2nd loop 200200/254647\n",
      "done 2nd loop 200300/254647\n",
      "done 2nd loop 200400/254647\n",
      "done 2nd loop 200500/254647\n",
      "done 2nd loop 200600/254647\n",
      "done 2nd loop 200700/254647\n",
      "done 2nd loop 200800/254647\n",
      "done 2nd loop 200900/254647\n",
      "done 2nd loop 201000/254647\n",
      "done 2nd loop 201100/254647\n",
      "done 2nd loop 201200/254647\n",
      "done 2nd loop 201300/254647\n",
      "done 2nd loop 201400/254647\n",
      "done 2nd loop 201500/254647\n",
      "done 2nd loop 201600/254647\n",
      "done 2nd loop 201700/254647\n",
      "done 2nd loop 201800/254647\n",
      "done 2nd loop 201900/254647\n",
      "done 2nd loop 202000/254647\n",
      "done 2nd loop 202100/254647\n",
      "done 2nd loop 202200/254647\n",
      "done 2nd loop 202300/254647\n",
      "done 2nd loop 202400/254647\n",
      "done 2nd loop 202500/254647\n",
      "done 2nd loop 202600/254647\n",
      "done 2nd loop 202700/254647\n",
      "done 2nd loop 202800/254647\n",
      "done 2nd loop 202900/254647\n",
      "done 2nd loop 203000/254647\n",
      "done 2nd loop 203100/254647\n",
      "done 2nd loop 203200/254647\n",
      "done 2nd loop 203300/254647\n",
      "done 2nd loop 203400/254647\n",
      "done 2nd loop 203500/254647\n",
      "done 2nd loop 203600/254647\n",
      "done 2nd loop 203700/254647\n",
      "done 2nd loop 203800/254647\n",
      "done 2nd loop 203900/254647\n",
      "done 2nd loop 204000/254647\n",
      "done 2nd loop 204100/254647\n",
      "done 2nd loop 204200/254647\n",
      "done 2nd loop 204300/254647\n",
      "done 2nd loop 204400/254647\n",
      "done 2nd loop 204500/254647\n",
      "done 2nd loop 204600/254647\n",
      "done 2nd loop 204700/254647\n",
      "done 2nd loop 204800/254647\n",
      "done 2nd loop 204900/254647\n",
      "done 2nd loop 205000/254647\n",
      "done 2nd loop 205100/254647\n",
      "done 2nd loop 205200/254647\n",
      "done 2nd loop 205300/254647\n",
      "done 2nd loop 205400/254647\n",
      "done 2nd loop 205500/254647\n",
      "done 2nd loop 205600/254647\n",
      "done 2nd loop 205700/254647\n",
      "done 2nd loop 205800/254647\n",
      "done 2nd loop 205900/254647\n",
      "done 2nd loop 206000/254647\n",
      "done 2nd loop 206100/254647\n",
      "done 2nd loop 206200/254647\n",
      "done 2nd loop 206300/254647\n",
      "done 2nd loop 206400/254647\n",
      "done 2nd loop 206500/254647\n",
      "done 2nd loop 206600/254647\n",
      "done 2nd loop 206700/254647\n",
      "done 2nd loop 206800/254647\n",
      "done 2nd loop 206900/254647\n",
      "done 2nd loop 207000/254647\n",
      "done 2nd loop 207100/254647\n",
      "done 2nd loop 207200/254647\n",
      "done 2nd loop 207300/254647\n",
      "done 2nd loop 207400/254647\n",
      "done 2nd loop 207500/254647\n",
      "done 2nd loop 207600/254647\n",
      "done 2nd loop 207700/254647\n",
      "done 2nd loop 207800/254647\n",
      "done 2nd loop 207900/254647\n",
      "done 2nd loop 208000/254647\n",
      "done 2nd loop 208100/254647\n",
      "done 2nd loop 208200/254647\n",
      "done 2nd loop 208300/254647\n",
      "done 2nd loop 208400/254647\n",
      "done 2nd loop 208500/254647\n",
      "done 2nd loop 208600/254647\n",
      "done 2nd loop 208700/254647\n",
      "done 2nd loop 208800/254647\n",
      "done 2nd loop 208900/254647\n",
      "done 2nd loop 209000/254647\n",
      "done 2nd loop 209100/254647\n",
      "done 2nd loop 209200/254647\n",
      "done 2nd loop 209300/254647\n",
      "done 2nd loop 209400/254647\n",
      "done 2nd loop 209500/254647\n",
      "done 2nd loop 209600/254647\n",
      "done 2nd loop 209700/254647\n",
      "done 2nd loop 209800/254647\n",
      "done 2nd loop 209900/254647\n",
      "done 2nd loop 210000/254647\n",
      "done 2nd loop 210100/254647\n",
      "done 2nd loop 210200/254647\n",
      "done 2nd loop 210300/254647\n",
      "done 2nd loop 210400/254647\n",
      "done 2nd loop 210500/254647\n",
      "done 2nd loop 210600/254647\n",
      "done 2nd loop 210700/254647\n",
      "done 2nd loop 210800/254647\n",
      "done 2nd loop 210900/254647\n",
      "done 2nd loop 211000/254647\n",
      "done 2nd loop 211100/254647\n",
      "done 2nd loop 211200/254647\n",
      "done 2nd loop 211300/254647\n",
      "done 2nd loop 211400/254647\n",
      "done 2nd loop 211500/254647\n",
      "done 2nd loop 211600/254647\n",
      "done 2nd loop 211700/254647\n",
      "done 2nd loop 211800/254647\n",
      "done 2nd loop 211900/254647\n",
      "done 2nd loop 212000/254647\n",
      "done 2nd loop 212100/254647\n",
      "done 2nd loop 212200/254647\n",
      "done 2nd loop 212300/254647\n",
      "done 2nd loop 212400/254647\n",
      "done 2nd loop 212500/254647\n",
      "done 2nd loop 212600/254647\n",
      "done 2nd loop 212700/254647\n",
      "done 2nd loop 212800/254647\n",
      "done 2nd loop 212900/254647\n",
      "done 2nd loop 213000/254647\n",
      "done 2nd loop 213100/254647\n",
      "done 2nd loop 213200/254647\n",
      "done 2nd loop 213300/254647\n",
      "done 2nd loop 213400/254647\n",
      "done 2nd loop 213500/254647\n",
      "done 2nd loop 213600/254647\n",
      "done 2nd loop 213700/254647\n",
      "done 2nd loop 213800/254647\n",
      "done 2nd loop 213900/254647\n",
      "done 2nd loop 214000/254647\n",
      "done 2nd loop 214100/254647\n",
      "done 2nd loop 214200/254647\n",
      "done 2nd loop 214300/254647\n",
      "done 2nd loop 214400/254647\n",
      "done 2nd loop 214500/254647\n",
      "done 2nd loop 214600/254647\n",
      "done 2nd loop 214700/254647\n",
      "done 2nd loop 214800/254647\n",
      "done 2nd loop 214900/254647\n",
      "done 2nd loop 215000/254647\n",
      "done 2nd loop 215100/254647\n",
      "done 2nd loop 215200/254647\n",
      "done 2nd loop 215300/254647\n",
      "done 2nd loop 215400/254647\n",
      "done 2nd loop 215500/254647\n",
      "done 2nd loop 215600/254647\n",
      "done 2nd loop 215700/254647\n",
      "done 2nd loop 215800/254647\n",
      "done 2nd loop 215900/254647\n",
      "done 2nd loop 216000/254647\n",
      "done 2nd loop 216100/254647\n",
      "done 2nd loop 216200/254647\n",
      "done 2nd loop 216300/254647\n",
      "done 2nd loop 216400/254647\n",
      "done 2nd loop 216500/254647\n",
      "done 2nd loop 216600/254647\n",
      "done 2nd loop 216700/254647\n",
      "done 2nd loop 216800/254647\n",
      "done 2nd loop 216900/254647\n",
      "done 2nd loop 217000/254647\n",
      "done 2nd loop 217100/254647\n",
      "done 2nd loop 217200/254647\n",
      "done 2nd loop 217300/254647\n",
      "done 2nd loop 217400/254647\n",
      "done 2nd loop 217500/254647\n",
      "done 2nd loop 217600/254647\n",
      "done 2nd loop 217700/254647\n",
      "done 2nd loop 217800/254647\n",
      "done 2nd loop 217900/254647\n",
      "done 2nd loop 218000/254647\n",
      "done 2nd loop 218100/254647\n",
      "done 2nd loop 218200/254647\n",
      "done 2nd loop 218300/254647\n",
      "done 2nd loop 218400/254647\n",
      "done 2nd loop 218500/254647\n",
      "done 2nd loop 218600/254647\n",
      "done 2nd loop 218700/254647\n",
      "done 2nd loop 218800/254647\n",
      "done 2nd loop 218900/254647\n",
      "done 2nd loop 219000/254647\n",
      "done 2nd loop 219100/254647\n",
      "done 2nd loop 219200/254647\n",
      "done 2nd loop 219300/254647\n",
      "done 2nd loop 219400/254647\n",
      "done 2nd loop 219500/254647\n",
      "done 2nd loop 219600/254647\n",
      "done 2nd loop 219700/254647\n",
      "done 2nd loop 219800/254647\n",
      "done 2nd loop 219900/254647\n",
      "done 2nd loop 220000/254647\n",
      "done 2nd loop 220100/254647\n",
      "done 2nd loop 220200/254647\n",
      "done 2nd loop 220300/254647\n",
      "done 2nd loop 220400/254647\n",
      "done 2nd loop 220500/254647\n",
      "done 2nd loop 220600/254647\n",
      "done 2nd loop 220700/254647\n",
      "done 2nd loop 220800/254647\n",
      "done 2nd loop 220900/254647\n",
      "done 2nd loop 221000/254647\n",
      "done 2nd loop 221100/254647\n",
      "done 2nd loop 221200/254647\n",
      "done 2nd loop 221300/254647\n",
      "done 2nd loop 221400/254647\n",
      "done 2nd loop 221500/254647\n",
      "done 2nd loop 221600/254647\n",
      "done 2nd loop 221700/254647\n",
      "done 2nd loop 221800/254647\n",
      "done 2nd loop 221900/254647\n",
      "done 2nd loop 222000/254647\n",
      "done 2nd loop 222100/254647\n",
      "done 2nd loop 222200/254647\n",
      "done 2nd loop 222300/254647\n",
      "done 2nd loop 222400/254647\n",
      "done 2nd loop 222500/254647\n",
      "done 2nd loop 222600/254647\n",
      "done 2nd loop 222700/254647\n",
      "done 2nd loop 222800/254647\n",
      "done 2nd loop 222900/254647\n",
      "done 2nd loop 223000/254647\n",
      "done 2nd loop 223100/254647\n",
      "done 2nd loop 223200/254647\n",
      "done 2nd loop 223300/254647\n",
      "done 2nd loop 223400/254647\n",
      "done 2nd loop 223500/254647\n",
      "done 2nd loop 223600/254647\n",
      "done 2nd loop 223700/254647\n",
      "done 2nd loop 223800/254647\n",
      "done 2nd loop 223900/254647\n",
      "done 2nd loop 224000/254647\n",
      "done 2nd loop 224100/254647\n",
      "done 2nd loop 224200/254647\n",
      "done 2nd loop 224300/254647\n",
      "done 2nd loop 224400/254647\n",
      "done 2nd loop 224500/254647\n",
      "done 2nd loop 224600/254647\n",
      "done 2nd loop 224700/254647\n",
      "done 2nd loop 224800/254647\n",
      "done 2nd loop 224900/254647\n",
      "done 2nd loop 225000/254647\n",
      "done 2nd loop 225100/254647\n",
      "done 2nd loop 225200/254647\n",
      "done 2nd loop 225300/254647\n",
      "done 2nd loop 225400/254647\n",
      "done 2nd loop 225500/254647\n",
      "done 2nd loop 225600/254647\n",
      "done 2nd loop 225700/254647\n",
      "done 2nd loop 225800/254647\n",
      "done 2nd loop 225900/254647\n",
      "done 2nd loop 226000/254647\n",
      "done 2nd loop 226100/254647\n",
      "done 2nd loop 226200/254647\n",
      "done 2nd loop 226300/254647\n",
      "done 2nd loop 226400/254647\n",
      "done 2nd loop 226500/254647\n",
      "done 2nd loop 226600/254647\n",
      "done 2nd loop 226700/254647\n",
      "done 2nd loop 226800/254647\n",
      "done 2nd loop 226900/254647\n",
      "done 2nd loop 227000/254647\n",
      "done 2nd loop 227100/254647\n",
      "done 2nd loop 227200/254647\n",
      "done 2nd loop 227300/254647\n",
      "done 2nd loop 227400/254647\n",
      "done 2nd loop 227500/254647\n",
      "done 2nd loop 227600/254647\n",
      "done 2nd loop 227700/254647\n",
      "done 2nd loop 227800/254647\n",
      "done 2nd loop 227900/254647\n",
      "done 2nd loop 228000/254647\n",
      "done 2nd loop 228100/254647\n",
      "done 2nd loop 228200/254647\n",
      "done 2nd loop 228300/254647\n",
      "done 2nd loop 228400/254647\n",
      "done 2nd loop 228500/254647\n",
      "done 2nd loop 228600/254647\n",
      "done 2nd loop 228700/254647\n",
      "done 2nd loop 228800/254647\n",
      "done 2nd loop 228900/254647\n",
      "done 2nd loop 229000/254647\n",
      "done 2nd loop 229100/254647\n",
      "done 2nd loop 229200/254647\n",
      "done 2nd loop 229300/254647\n",
      "done 2nd loop 229400/254647\n",
      "done 2nd loop 229500/254647\n",
      "done 2nd loop 229600/254647\n",
      "done 2nd loop 229700/254647\n",
      "done 2nd loop 229800/254647\n",
      "done 2nd loop 229900/254647\n",
      "done 2nd loop 230000/254647\n",
      "done 2nd loop 230100/254647\n",
      "done 2nd loop 230200/254647\n",
      "done 2nd loop 230300/254647\n",
      "done 2nd loop 230400/254647\n",
      "done 2nd loop 230500/254647\n",
      "done 2nd loop 230600/254647\n",
      "done 2nd loop 230700/254647\n",
      "done 2nd loop 230800/254647\n",
      "done 2nd loop 230900/254647\n",
      "done 2nd loop 231000/254647\n",
      "done 2nd loop 231100/254647\n",
      "done 2nd loop 231200/254647\n",
      "done 2nd loop 231300/254647\n",
      "done 2nd loop 231400/254647\n",
      "done 2nd loop 231500/254647\n",
      "done 2nd loop 231600/254647\n",
      "done 2nd loop 231700/254647\n",
      "done 2nd loop 231800/254647\n",
      "done 2nd loop 231900/254647\n",
      "done 2nd loop 232000/254647\n",
      "done 2nd loop 232100/254647\n",
      "done 2nd loop 232200/254647\n",
      "done 2nd loop 232300/254647\n",
      "done 2nd loop 232400/254647\n",
      "done 2nd loop 232500/254647\n",
      "done 2nd loop 232600/254647\n",
      "done 2nd loop 232700/254647\n",
      "done 2nd loop 232800/254647\n",
      "done 2nd loop 232900/254647\n",
      "done 2nd loop 233000/254647\n",
      "done 2nd loop 233100/254647\n",
      "done 2nd loop 233200/254647\n",
      "done 2nd loop 233300/254647\n",
      "done 2nd loop 233400/254647\n",
      "done 2nd loop 233500/254647\n",
      "done 2nd loop 233600/254647\n",
      "done 2nd loop 233700/254647\n",
      "done 2nd loop 233800/254647\n",
      "done 2nd loop 233900/254647\n",
      "done 2nd loop 234000/254647\n",
      "done 2nd loop 234100/254647\n",
      "done 2nd loop 234200/254647\n",
      "done 2nd loop 234300/254647\n",
      "done 2nd loop 234400/254647\n",
      "done 2nd loop 234500/254647\n",
      "done 2nd loop 234600/254647\n",
      "done 2nd loop 234700/254647\n",
      "done 2nd loop 234800/254647\n",
      "done 2nd loop 234900/254647\n",
      "done 2nd loop 235000/254647\n",
      "done 2nd loop 235100/254647\n",
      "done 2nd loop 235200/254647\n",
      "done 2nd loop 235300/254647\n",
      "done 2nd loop 235400/254647\n",
      "done 2nd loop 235500/254647\n",
      "done 2nd loop 235600/254647\n",
      "done 2nd loop 235700/254647\n",
      "done 2nd loop 235800/254647\n",
      "done 2nd loop 235900/254647\n",
      "done 2nd loop 236000/254647\n",
      "done 2nd loop 236100/254647\n",
      "done 2nd loop 236200/254647\n",
      "done 2nd loop 236300/254647\n",
      "done 2nd loop 236400/254647\n",
      "done 2nd loop 236500/254647\n",
      "done 2nd loop 236600/254647\n",
      "done 2nd loop 236700/254647\n",
      "done 2nd loop 236800/254647\n",
      "done 2nd loop 236900/254647\n",
      "done 2nd loop 237000/254647\n",
      "done 2nd loop 237100/254647\n",
      "done 2nd loop 237200/254647\n",
      "done 2nd loop 237300/254647\n",
      "done 2nd loop 237400/254647\n",
      "done 2nd loop 237500/254647\n",
      "done 2nd loop 237600/254647\n",
      "done 2nd loop 237700/254647\n",
      "done 2nd loop 237800/254647\n",
      "done 2nd loop 237900/254647\n",
      "done 2nd loop 238000/254647\n",
      "done 2nd loop 238100/254647\n",
      "done 2nd loop 238200/254647\n",
      "done 2nd loop 238300/254647\n",
      "done 2nd loop 238400/254647\n",
      "done 2nd loop 238500/254647\n",
      "done 2nd loop 238600/254647\n",
      "done 2nd loop 238700/254647\n",
      "done 2nd loop 238800/254647\n",
      "done 2nd loop 238900/254647\n",
      "done 2nd loop 239000/254647\n",
      "done 2nd loop 239100/254647\n",
      "done 2nd loop 239200/254647\n",
      "done 2nd loop 239300/254647\n",
      "done 2nd loop 239400/254647\n",
      "done 2nd loop 239500/254647\n",
      "done 2nd loop 239600/254647\n",
      "done 2nd loop 239700/254647\n",
      "done 2nd loop 239800/254647\n",
      "done 2nd loop 239900/254647\n",
      "done 2nd loop 240000/254647\n",
      "done 2nd loop 240100/254647\n",
      "done 2nd loop 240200/254647\n",
      "done 2nd loop 240300/254647\n",
      "done 2nd loop 240400/254647\n",
      "done 2nd loop 240500/254647\n",
      "done 2nd loop 240600/254647\n",
      "done 2nd loop 240700/254647\n",
      "done 2nd loop 240800/254647\n",
      "done 2nd loop 240900/254647\n",
      "done 2nd loop 241000/254647\n",
      "done 2nd loop 241100/254647\n",
      "done 2nd loop 241200/254647\n",
      "done 2nd loop 241300/254647\n",
      "done 2nd loop 241400/254647\n",
      "done 2nd loop 241500/254647\n",
      "done 2nd loop 241600/254647\n",
      "done 2nd loop 241700/254647\n",
      "done 2nd loop 241800/254647\n",
      "done 2nd loop 241900/254647\n",
      "done 2nd loop 242000/254647\n",
      "done 2nd loop 242100/254647\n",
      "done 2nd loop 242200/254647\n",
      "done 2nd loop 242300/254647\n",
      "done 2nd loop 242400/254647\n",
      "done 2nd loop 242500/254647\n",
      "done 2nd loop 242600/254647\n",
      "done 2nd loop 242700/254647\n",
      "done 2nd loop 242800/254647\n",
      "done 2nd loop 242900/254647\n",
      "done 2nd loop 243000/254647\n",
      "done 2nd loop 243100/254647\n",
      "done 2nd loop 243200/254647\n",
      "done 2nd loop 243300/254647\n",
      "done 2nd loop 243400/254647\n",
      "done 2nd loop 243500/254647\n",
      "done 2nd loop 243600/254647\n",
      "done 2nd loop 243700/254647\n",
      "done 2nd loop 243800/254647\n",
      "done 2nd loop 243900/254647\n",
      "done 2nd loop 244000/254647\n",
      "done 2nd loop 244100/254647\n",
      "done 2nd loop 244200/254647\n",
      "done 2nd loop 244300/254647\n",
      "done 2nd loop 244400/254647\n",
      "done 2nd loop 244500/254647\n",
      "done 2nd loop 244600/254647\n",
      "done 2nd loop 244700/254647\n",
      "done 2nd loop 244800/254647\n",
      "done 2nd loop 244900/254647\n",
      "done 2nd loop 245000/254647\n",
      "done 2nd loop 245100/254647\n",
      "done 2nd loop 245200/254647\n",
      "done 2nd loop 245300/254647\n",
      "done 2nd loop 245400/254647\n",
      "done 2nd loop 245500/254647\n",
      "done 2nd loop 245600/254647\n",
      "done 2nd loop 245700/254647\n",
      "done 2nd loop 245800/254647\n",
      "done 2nd loop 245900/254647\n",
      "done 2nd loop 246000/254647\n",
      "done 2nd loop 246100/254647\n",
      "done 2nd loop 246200/254647\n",
      "done 2nd loop 246300/254647\n",
      "done 2nd loop 246400/254647\n",
      "done 2nd loop 246500/254647\n",
      "done 2nd loop 246600/254647\n",
      "done 2nd loop 246700/254647\n",
      "done 2nd loop 246800/254647\n",
      "done 2nd loop 246900/254647\n",
      "done 2nd loop 247000/254647\n",
      "done 2nd loop 247100/254647\n",
      "done 2nd loop 247200/254647\n",
      "done 2nd loop 247300/254647\n",
      "done 2nd loop 247400/254647\n",
      "done 2nd loop 247500/254647\n",
      "done 2nd loop 247600/254647\n",
      "done 2nd loop 247700/254647\n",
      "done 2nd loop 247800/254647\n",
      "done 2nd loop 247900/254647\n",
      "done 2nd loop 248000/254647\n",
      "done 2nd loop 248100/254647\n",
      "done 2nd loop 248200/254647\n",
      "done 2nd loop 248300/254647\n",
      "done 2nd loop 248400/254647\n",
      "done 2nd loop 248500/254647\n",
      "done 2nd loop 248600/254647\n",
      "done 2nd loop 248700/254647\n",
      "done 2nd loop 248800/254647\n",
      "done 2nd loop 248900/254647\n",
      "done 2nd loop 249000/254647\n",
      "done 2nd loop 249100/254647\n",
      "done 2nd loop 249200/254647\n",
      "done 2nd loop 249300/254647\n",
      "done 2nd loop 249400/254647\n",
      "done 2nd loop 249500/254647\n",
      "done 2nd loop 249600/254647\n",
      "done 2nd loop 249700/254647\n",
      "done 2nd loop 249800/254647\n",
      "done 2nd loop 249900/254647\n",
      "done 2nd loop 250000/254647\n",
      "done 2nd loop 250100/254647\n",
      "done 2nd loop 250200/254647\n",
      "done 2nd loop 250300/254647\n",
      "done 2nd loop 250400/254647\n",
      "done 2nd loop 250500/254647\n",
      "done 2nd loop 250600/254647\n",
      "done 2nd loop 250700/254647\n",
      "done 2nd loop 250800/254647\n",
      "done 2nd loop 250900/254647\n",
      "done 2nd loop 251000/254647\n",
      "done 2nd loop 251100/254647\n",
      "done 2nd loop 251200/254647\n",
      "done 2nd loop 251300/254647\n",
      "done 2nd loop 251400/254647\n",
      "done 2nd loop 251500/254647\n",
      "done 2nd loop 251600/254647\n",
      "done 2nd loop 251700/254647\n",
      "done 2nd loop 251800/254647\n",
      "done 2nd loop 251900/254647\n",
      "done 2nd loop 252000/254647\n",
      "done 2nd loop 252100/254647\n",
      "done 2nd loop 252200/254647\n",
      "done 2nd loop 252300/254647\n",
      "done 2nd loop 252400/254647\n",
      "done 2nd loop 252500/254647\n",
      "done 2nd loop 252600/254647\n",
      "done 2nd loop 252700/254647\n",
      "done 2nd loop 252800/254647\n",
      "done 2nd loop 252900/254647\n",
      "done 2nd loop 253000/254647\n",
      "done 2nd loop 253100/254647\n",
      "done 2nd loop 253200/254647\n",
      "done 2nd loop 253300/254647\n",
      "done 2nd loop 253400/254647\n",
      "done 2nd loop 253500/254647\n",
      "done 2nd loop 253600/254647\n",
      "done 2nd loop 253700/254647\n",
      "done 2nd loop 253800/254647\n",
      "done 2nd loop 253900/254647\n",
      "done 2nd loop 254000/254647\n",
      "done 2nd loop 254100/254647\n",
      "done 2nd loop 254200/254647\n",
      "done 2nd loop 254300/254647\n",
      "done 2nd loop 254400/254647\n",
      "done 2nd loop 254500/254647\n",
      "done 2nd loop 254600/254647\n"
     ]
    }
   ],
   "source": [
    "vocab = generate_vocab(good_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1568466461989,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "-0X_bn_8oefZ",
    "outputId": "f91dc00f-aaa1-4019-9bff-0319006411c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101814"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htelwNv6seWm"
   },
   "outputs": [],
   "source": [
    "with open(base_path/'my_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lq8sChEdB7gh"
   },
   "source": [
    "Train pretrained model with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WY3VaCrzB8V-"
   },
   "outputs": [],
   "source": [
    "class LM_PreLoader():\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle\n",
    "        total_len = sum([len(t) for t in data.data])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.data\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3goN8NU6h-w"
   },
   "outputs": [],
   "source": [
    "def split_df(df, vocab, pct = 0.1):\n",
    "    index = int(len(df) * pct)\n",
    "    train_df = df.iloc[0:(len(df) - index), :]\n",
    "    valid_df = df.iloc[(len(df) - index):, :]\n",
    "    \n",
    "    train_txt_ds = TextDataset(train_df, vocab)\n",
    "    valid_txt_ds = TextDataset(valid_df, vocab)\n",
    "    \n",
    "    return train_txt_ds, valid_txt_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_OS0FSE9w41"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, vocab):\n",
    "        self.data = []\n",
    "        for index, row in df.iterrows():\n",
    "            title_nums = []\n",
    "            body_nums = []\n",
    "            for x in row[-2]:\n",
    "                if(x in vocab):\n",
    "                    title_nums.append(vocab[x])\n",
    "                else:\n",
    "                    title_nums.append(vocab['xxxunk'])\n",
    "                    \n",
    "            for x in row[-1]:\n",
    "                if(x in vocab):\n",
    "                    body_nums.append(vocab[x])\n",
    "                else:\n",
    "                    body_nums.append(vocab['xxxunk'])\n",
    "            self.data.append(title_nums + body_nums)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18198,
     "status": "error",
     "timestamp": 1568468995510,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "7n35t6o4-hTx",
    "outputId": "6e478e0d-a741-4b10-89cb-74bf69f39279"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-a94f81beb7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-0b0c69aaa09f>\u001b[0m in \u001b[0;36msplit_df\u001b[0;34m(df, vocab, pct)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_txt_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_txt_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-9c423a196005>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, vocab)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 106 at dim 1 (got 126)"
     ]
    }
   ],
   "source": [
    "train, valid = split_df(good_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1568468209381,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "XeJRiBRg_L52",
    "outputId": "63855bf1-afc5-47fe-883c-353f206506ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85798, 9533)"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cvjcf_w_YsM"
   },
   "outputs": [],
   "source": [
    "with open(base_path/'train_txt_ds.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "    \n",
    "with open(base_path/'valid_txt_ds.pkl', 'wb') as f:\n",
    "    pickle.dump(valid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0OHBNk7_lzA"
   },
   "outputs": [],
   "source": [
    "lm_preloader = LM_PreLoader(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNZUukizASrE"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_PreLoader(valid, shuffle=False), batch_size=64)\n",
    "iter_dl = iter(dl)\n",
    "x1,y1 = next(iter_dl)\n",
    "x2,y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzahLxj0BGCU"
   },
   "outputs": [],
   "source": [
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=False), batch_size=bs, **kwargs), #TODO: try to get shuffle = True to work\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d9e92178a470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'train_txt_ds.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_path' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXiOUgJ6BnZZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_lm_dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9aaa76c1de84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_valid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lm_dls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_lm_dls' is not defined"
     ]
    }
   ],
   "source": [
    "lm_train_dl, lm_valid_dl = get_lm_dls(train, valid, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eQZKEwjCnAu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynX9r9hwEKJW"
   },
   "source": [
    "Get pretrained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxDpx9hxEflU"
   },
   "outputs": [],
   "source": [
    "base_path = Path('storage/htn2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbfxmAbVEM8Q"
   },
   "outputs": [],
   "source": [
    "old_wgts  = torch.load(base_path/'pretrained.pth')\n",
    "old_vocab = pickle.load(open(base_path/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jr4IGaOmElrT"
   },
   "outputs": [],
   "source": [
    "vocab = None\n",
    "with open(base_path/'my_vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\n",
    "emb_sz, nh, nl = 300, 300, 2\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, vocab['xxxpad'], *dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzTG3JSpKBCQ"
   },
   "outputs": [],
   "source": [
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    wgts = old_wgts['0.emb.weight']\n",
    "    bias = old_wgts['1.decoder.bias']\n",
    "    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()\n",
    "    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))\n",
    "    new_bias = bias.new_zeros(len(new_vocab))\n",
    "    otoi = {v:k for k,v in enumerate(old_vocab)}\n",
    "    for i,w in enumerate(new_vocab): \n",
    "        if w in otoi:\n",
    "            idx = otoi[w]\n",
    "            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]\n",
    "        else: new_wgts[i],new_bias[i] = wgts_m,bias_m\n",
    "    old_wgts['0.emb.weight']    = new_wgts\n",
    "    old_wgts['0.emb_dp.emb.weight'] = new_wgts\n",
    "    old_wgts['1.decoder.weight']    = new_wgts\n",
    "    old_wgts['1.decoder.bias']      = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDDKsHmtKmz0"
   },
   "outputs": [],
   "source": [
    "wgts = match_embeds(old_wgts, old_vocab, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1568474769432,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "wPDUJWnOKxSf",
    "outputId": "56d08b06-b807-49b6-ac57-5dc957c9df39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1568474771061,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "CoX_-NwVLCGP",
    "outputId": "350981d7-051d-407d-8abe-49ab54317dc0"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF, expected 23633776 more bytes. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1fe47d2ca7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'lm_preunfreeze.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 23633776 more bytes. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(base_path/'lm_preunfreeze.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00XB_NAnLFYM"
   },
   "outputs": [],
   "source": [
    "def lm_splitter(m):\n",
    "    groups = []\n",
    "    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n",
    "    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]\n",
    "    return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNXDPI0PLp1H"
   },
   "outputs": [],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters(): p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGUXsdj9OXC4"
   },
   "outputs": [],
   "source": [
    "#learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(),\n",
    "               # cb_funcs=cbs, splitter=lm_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvPqoxNdMCm-"
   },
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)\n",
    "\n",
    "def fit(model, train_dl, valid_dl, loss_func, opt, num_epochs, acc_func):\n",
    "    model = model.cuda()\n",
    "    a = 2.\n",
    "    b = 1.\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        for xb, yb in train_dl:\n",
    "            model.train()\n",
    "            pred = model(xb.cuda())\n",
    "            loss = loss_func(pred[0], yb.cuda())\n",
    "           \n",
    "            raw_out,out = pred[1], pred[2]\n",
    "            if a != 0.:  \n",
    "                loss += a * out[-1].float().pow(2).mean()\n",
    "            \n",
    "            if b != 0.:\n",
    "                h = raw_out[-1]\n",
    "                if len(h)>1: \n",
    "                    loss += b * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "            \n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            #gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb.cuda())\n",
    "                loss = loss_func(pred[0], yb.cuda())\n",
    "                valid_loss += loss\n",
    "                acc = acc_func(pred[0], yb.cuda())\n",
    "                valid_acc += acc\n",
    "                \n",
    "        print(\"Epoch {0} complete. Train loss: {1}. Valid loss {2}. Valid Accuracy {3}\".format(epoch, train_loss / len(train_dl), valid_loss / len(valid_dl), valid_acc / len(valid_dl)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12416,
     "status": "error",
     "timestamp": 1568476735124,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "RZJad7wCR7CA",
    "outputId": "c11ed88d-3da2-4930-c535-34320714f9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Train loss: 5.264625549316406. Valid loss 5.260253429412842. Valid Accuracy 0.19097912311553955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-08097ae5ca33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_valid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-6312fd54d9e6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dl, valid_dl, loss_func, opt, num_epochs, acc_func)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\n",
    "emb_sz, nh, nl = 300, 300, 2\n",
    "\n",
    "base_path = Path('storage/htn2019')\n",
    "vocab = None\n",
    "with open(base_path/'my_vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, vocab['xxxpad'], *dps)\n",
    "model.load_state_dict(wgts)\n",
    "\n",
    "\n",
    "train = None\n",
    "with open(base_path/'train_txt_ds.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "valid = None\n",
    "with open(base_path/'valid_txt_ds.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "\n",
    "lm_train_dl, lm_valid_dl = get_lm_dls(train, valid, 64, 70)\n",
    "torch.cuda.empty_cache()\n",
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters(): p.requires_grad_(False)\n",
    "\n",
    "        \n",
    "fit(model, lm_train_dl, lm_valid_dl, cross_entropy_flat, torch.optim.Adam(model.parameters(), lr = 2e-2), 1, accuracy_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_path/'lm_before_unfreeze.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1568474718306,
     "user": {
      "displayName": "Andrew Pochapsky",
      "photoUrl": "",
      "userId": "05133336566585067591"
     },
     "user_tz": 240
    },
    "id": "MQKSNCj9UlGL",
    "outputId": "2ed6cdb4-1222-4f53-9f64-b102b7d7eb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Train loss: 4.9907379150390625. Valid loss 5.034150123596191. Valid Accuracy 0.20948079228401184\n",
      "Epoch 1 complete. Train loss: 21.26679801940918. Valid loss 41.60452651977539. Valid Accuracy 0.0016559930518269539\n",
      "Epoch 2 complete. Train loss: 32.195579528808594. Valid loss 28.752336502075195. Valid Accuracy 0.008455846458673477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2f5527d898b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_valid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-6312fd54d9e6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dl, valid_dl, loss_func, opt, num_epochs, acc_func)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters(): \n",
    "        p.requires_grad_(True)\n",
    "       \n",
    "fit(model, lm_train_dl, lm_valid_dl, cross_entropy_flat, torch.optim.Adam(model.parameters(), lr = 2e-2), 10, accuracy_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMPax7-wYNxE"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_path/'lm_fine_tuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbcPxmDYeTKd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59b704aae005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'lm_before_unfreeze.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(base_path/'lm_before_unfreeze.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(learn.model[0].state_dict(), path/'pre_enc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, vocab, category_encoder):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for index, row in df.iterrows():\n",
    "            title_nums = []\n",
    "            body_nums = []\n",
    "            for x in row[-2]:\n",
    "                if(x in vocab):\n",
    "                    title_nums.append(vocab[x])\n",
    "                else:\n",
    "                    title_nums.append(vocab['xxxunk'])\n",
    "                    \n",
    "            for x in row[-1]:\n",
    "                if(x in vocab):\n",
    "                    body_nums.append(vocab[x])\n",
    "                else:\n",
    "                    body_nums.append(vocab['xxxunk'])\n",
    "            self.x.append(title_nums + body_nums)\n",
    "            self.y.append(category_encoder[row[1]])\n",
    "        self.y = LongTensor(self.y)\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "    def pad_collate(self, pad_idx, pad_first=False):\n",
    "        max_len = max([len(s) for s in self.x])\n",
    "        res = torch.zeros(len(self.x), max_len).long() + pad_idx\n",
    "        for i,s in enumerate(self.x):\n",
    "            if pad_first: res[i, -len(s):] = LongTensor(s)\n",
    "            else:         res[i, :len(s) ] = LongTensor(s)\n",
    "        self.x = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('storage/htn2019')\n",
    "\n",
    "def create_category_encoder(categories):\n",
    "    encoder = dict()\n",
    "    i = 0\n",
    "    for c in categories:\n",
    "        encoder[c] = i\n",
    "        i += 1\n",
    "    return encoder\n",
    "\n",
    "def get_categories(df):\n",
    "    cats = set()\n",
    "    for i, row in df.iterrows():\n",
    "        cats.add(row[1])\n",
    "    return cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_encoder = None\n",
    "with open(base_path/'category_encoder.pkl', 'rb') as f:\n",
    "    category_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = None\n",
    "valid_ds = None\n",
    "with open(base_path/'train_class_ds.pkl', 'rb') as f:\n",
    "    train_ds = pickle.load(f)\n",
    "    \n",
    "with open(base_path/'valid_class_ds.pkl', 'rb') as f:\n",
    "    valid_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = None\n",
    "with open(base_path/'my_vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_ds.pad_collate(vocab['xxxpad'])\n",
    "#train_ds.pad_collate(vocab['xxxpad'])\n",
    "def pad_collate(samples, pad_idx=0, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, torch.Tensor)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds.x), type(valid_ds.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = split_df_classification(df, vocab, category_encoder)\n",
    "\n",
    "train_sampler = SortSampler(train_ds.x, key=lambda t: len(train_ds.x[t]))\n",
    "valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 64, sampler = train_sampler, collate_fn = pad_collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 128, sampler = valid_sampler, collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM1(nn.Module):\n",
    "    \"AWD-LSTM from fastai.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        mask = (input == self.pad_token)\n",
    "        lengths = sl - mask.long().sum(1)\n",
    "        n_empty = (lengths == 0).sum()\n",
    "        if n_empty > 0:\n",
    "            input = input[:-n_empty]\n",
    "            lengths = lengths[:-n_empty]\n",
    "            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "            new_hidden.append(new_h)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs, mask\n",
    "    \n",
    "    \n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader._SingleProcessDataLoaderIter"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        return output,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "       \n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(t, bs, val=0.):\n",
    "    if t.size(0) < bs:\n",
    "        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, module, bptt, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.bptt,self.module,self.pad_idx = bptt,module,pad_idx\n",
    "\n",
    "    def concat(self, arrs, bs):\n",
    "        return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.module.bs = bs\n",
    "        self.module.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            masks.append(pad_tensor(m, bs, 1))\n",
    "            raw_outputs.append(r)\n",
    "            outputs.append(o)\n",
    "        return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, \n",
    "                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = SentenceEncoder(rnn_enc, bptt)\n",
    "    if layers is None: \n",
    "        layers = [50]\n",
    "    if drops is None:  \n",
    "        drops = [0.1] * len(layers)\n",
    "    layers = [3 * emb_sz] + layers + [n_out] \n",
    "    drops = [output_p] + drops\n",
    "    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Dropout(p=0.10000000149011612, inplace=False)\n",
      "  (2): Linear(in_features=900, out_features=50, bias=True)\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Dropout(p=0.1, inplace=False)\n",
      "  (6): Linear(in_features=50, out_features=1013, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "emb_sz = 300\n",
    "nh = 300 \n",
    "nl = 2\n",
    "bptt = 70\n",
    "dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n",
    "model = get_text_classifier(len(vocab), emb_sz, nh, nl, len(category_encoder), vocab['xxxpad'], bptt, *dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].module.load_state_dict(torch.load(base_path/'pre_enc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_classification(model, train_dl, valid_dl, loss_func, opt, num_epochs, acc_func):\n",
    "    model = model.cuda()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        for xb, yb in train_dl:\n",
    "            model.train()\n",
    "           \n",
    "            pred = model(xb.cuda())\n",
    "            loss = loss_func(pred, yb.cuda())\n",
    "            \n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            #gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb.cuda())\n",
    "                loss = loss_func(pred, yb.cuda())\n",
    "                valid_loss += loss\n",
    "                acc = acc_func(pred, yb.cuda())\n",
    "                valid_acc += acc\n",
    "               \n",
    "               \n",
    "                \n",
    "        print(\"Epoch {0} complete. Train loss: {1}. Valid loss {2}. Valid Accuracy {3}\".format(epoch, train_loss / len(train_dl), valid_loss / len(valid_dl), valid_acc / len(valid_dl)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Train loss: 4.723647594451904. Valid loss 4.128415584564209. Valid Accuracy 0.16313353180885315\n",
      "Epoch 1 complete. Train loss: 4.105393409729004. Valid loss 3.885662078857422. Valid Accuracy 0.20418545603752136\n",
      "Epoch 2 complete. Train loss: 3.9674201011657715. Valid loss 3.8172800540924072. Valid Accuracy 0.2184767723083496\n"
     ]
    }
   ],
   "source": [
    "emb_sz = 300\n",
    "nh = 300 \n",
    "nl = 2\n",
    "bptt = 70\n",
    "dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n",
    "model = get_text_classifier(len(vocab), emb_sz, nh, nl, len(category_encoder), vocab['xxxpad'], bptt, *dps)\n",
    "model[0].module.load_state_dict(torch.load(base_path/'pre_enc.pth'))\n",
    "\n",
    "for p in model[0].parameters(): \n",
    "    p.requires_grad_(False)\n",
    "fit_classification(model, train_dl, valid_dl, F.cross_entropy, torch.optim.Adam(model.parameters(), lr = 2e-2), 3, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_path/'classifier_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(base_path/'classifier_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Train loss: 3.8677234649658203. Valid loss 3.708519458770752. Valid Accuracy 0.24500855803489685\n",
      "Epoch 1 complete. Train loss: 3.5215775966644287. Valid loss 3.542172431945801. Valid Accuracy 0.2768203616142273\n"
     ]
    }
   ],
   "source": [
    "for p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True)\n",
    "fit_classification(model, train_dl, valid_dl, F.cross_entropy, torch.optim.Adam(model.parameters(), lr = 2e-2), 2, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_path/'classifier_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Train loss: 4.2987470626831055. Valid loss 3.8495402336120605. Valid Accuracy 0.26598700881004333\n",
      "Epoch 1 complete. Train loss: 3.706559896469116. Valid loss 3.43371844291687. Valid Accuracy 0.34745559096336365\n"
     ]
    }
   ],
   "source": [
    "for p in model[0].parameters(): p.requires_grad_(True)\n",
    "fit_classification(model, train_dl, valid_dl, F.cross_entropy, torch.optim.Adam(model.parameters(), lr = 1e-2), 2, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_path/'classifier_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2319e-03,  4.5630e-41,  1.2211e-03,  4.5630e-41,  1.2049e-03],\n",
       "        [ 4.5630e-41, -6.7574e-16,  4.5629e-41, -6.8373e-16,  4.5629e-41],\n",
       "        [ 2.3710e-04,  4.5630e-41,  2.1992e-04,  4.5630e-41, -6.8557e-16],\n",
       "        [ 4.5629e-41, -6.7883e-16,  4.5629e-41, -6.7886e-16,  4.5629e-41],\n",
       "        [ 2.0539e-04,  4.5630e-41,  1.4515e-05,  4.5630e-41,  1.2048e-03]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HackTheNorth.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
